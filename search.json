[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dbreg",
    "section": "",
    "text": "Fast regressions on database backends.\n\n\ndbreg is an R package that leverages the power of databases to run regressions on very large datasets, which may not fit into R’s memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics. Our default DuckDB backend provides a powerful, embedded analytics engine to get users up and running with minimal effort. Users can also specify alternative database backends, depending on their computing needs and setup.\nThe dbreg R package is inspired by, and has similar aims to, the duckreg Python package. This implementation offers some idiomatic, R-focused features like a formula interface and “pretty” print methods. But our long-term goal is that these two packages should be aligned in terms of core feature parity.\n\n\n\ndbreg can be installed from R-universe.\ninstall.packages(\n   \"dbreg\",\n   repos = c(\"https://grantmcdermott.r-universe.dev\", getOption(\"repos\"))\n)\n\n\n\n\n\nTo get ourselves situated, we’ll first demonstrate by using an in-memory R dataset.\nlibrary(dbreg)\nlibrary(fixest) # for data and comparison\n\ndata(\"trade\", package = \"fixest\")\n\ndbreg(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; Compressed OLS estimation, Dep. Var.: Euros \n#&gt; Observations.: 38,325 (original) | 210 (compressed) \n#&gt; Standard-errors: Heteroskedasticity-robust\n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3         Adj. R2: 0.215289\nBehind the scenes, dbreg has compressed the original dataset down from nearly 40,000 observations to only 210, before running the final (weighted) regression on this much smaller data object. This compression procedure trick follows Wang _et. al. (2021) and effectively allows us to compute on a much lighter object, saving time and memory. We can confirm that it still gives the same result as running fixest::feols on the full dataset:\nfeols(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; OLS estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325\n#&gt; Fixed-effects: Destination: 15,  Origin: 15\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3     Adj. R2: 0.215289\n#&gt;                       Within R2: 0.025914\n\n\n\nFor a more appropriate dbreg use-case, let’s run a regression on some NYC taxi data. (Download instructions here.) The dataset that we’re working with here is about 180 million rows deep and takes up 8.5 GB on disk.1 dbreg offers two basic ways to analyse and interact with data of this size.\n\n\nUse the path argument to read the data directly from disk and perform the compression computation in an ephemeral DuckDB connection. This requires that the data are small enough to fit into RAM… but please note that “small enough” is a relative concept. Thanks to DuckDB’s incredible efficiency, your RAM should be able to handle very large datasets that would otherwise crash your R session, and require only a fraction of the computation time. Note that we also invoke the (optional) verbose  = TRUE argument to print additional information about the estimation strategy.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\", ## path to hive-partitioned dataset\n   vcov = \"hc1\",\n   verbose = TRUE ## optional (print info about the estimation strategy)\n)\n#&gt; [dbreg] Auto strategy:\n#&gt;         - data has 178,544,324 rows with 2 FE (24 unique groups)\n#&gt;         - compression ratio (0.00) satisfies threshold (0.6)\n#&gt;         - decision: compress\n#&gt; [dbreg] Executing compress strategy SQL\n#&gt; \n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Heteroskedasticity-robust\n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000068 1564.742 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.000106 -273.866 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nNote the size of the original dataset, which is nearly 180 million rows, versus the compressed dataset, which is down to only 70k. On my laptop (M4 MacBook Pro) this regression completes in under 3 seconds… and that includes the time it took to determine an optimal estimation strategy, as well as read the data from disk!2\nIn case you were wondering, obtaining clustered standard errors is just as easy; simply pass the relevant cluster variable as a formula to the vcov argument. Since we know that the optimal acceleration strategy is \"compress\", we’ll also go ahead a specify this explicitly to skip the auto strategy overhead.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path     = \"read_parquet('nyc-taxi/**/*.parquet')\",\n   vcov     = ~month,    # clustered SEs\n   strategy = \"compress\" # skip auto strategy overhead\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\n\n\n\nWhile querying on-the-fly with our default DuckDB backend is both convenient and extremely performant, you can also run regressions against existing tables in a persistent database connection. This could be DuckDB, but it could also be any other supported backend. All you need to do is specify the appropriate conn and table arguments.\n# load the DBI package to connect to a persistent database\nlibrary(DBI)\n\n# create connection to persistent DuckDB database (could be any supported backend)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"nyc.db\")\n\n# create a 'taxi' table in our new nyc.db database from our parquet dataset\ndbExecute(\n   con,\n   \"\n   CREATE TABLE taxi AS\n      FROM read_parquet('nyc-taxi/**/*.parquet')\n      SELECT tip_amount, fare_amount, passenger_count, month, vendor_name\n   \"\n)\n\n# now run our regression against this conn+table combo\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   conn = con,     # database connection,\n   table = \"taxi\", # table name\n   vcov = ~month,\n   strategy = \"compress\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nResult: we get the same coefficient and standard error estimates as earlier.\nWe’ll close by doing some (optional) clean up.\ndbRemoveTable(con, \"taxi\")\ndbDisconnect(con)\nunlink(\"nyc.db\") # remove from disk\n\n[!TIP] If you don’t want to create a persistent database (and materialize data), a nice alternative is CREATE VIEW. This lets you define subsets or computed columns on-the-fly. For example, to regress on Q1 2012 data with a day-of-week fixed effect:\ndbExecute(con, \"\n   CREATE VIEW nyc_subset AS\n   SELECT\n      tip_amount, trip_distance, passenger_count,\n      vendor_name, month,\n      dayofweek(dropoff_datetime) AS dofw\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   WHERE year = 2012 AND month &lt;= 3\n\")\n\ndbreg(\n   tip_amount ~ trip_distance + passenger_count | month + dofw + vendor_name,\n   conn = con,\n   table = \"nyc_subset\",\n   vcov = ~dofw\n)\n\n\n\n\n\n\nAll of the examples in this README have made use of the \"compress\" strategy. But the compression trick is not the only game in town and dbreg supports several other acceleration strategies: \"moments\", \"demean\", and \"mundlak\". Depending on your data and regression requirements, one of these other strategies may better suit your problem. The good news is that (the default) strategy = \"auto\" option uses some intelligent heuristics to determine which strategy is (probably) optimal for each case. You can set the verbose = TRUE argument to get real-time feedback about the decision criteria being used. The Acceleration Strategies section of the ?dbreg helpfile contains a lot detail about the different options and tradeoffs involved, so please do consult the documentation.\n\n\n\ndbreg is a maturing package and there are a number of features that we still plan to add before submitting it to CRAN. (See our TO-DO list.) We also don’t yet support some standard R operations like interaction terms in the formula. At the same time, the core dbreg() routine has been tested pretty thoroughly and should work in standard cases. Please help us by kicking the tyres and creating GitHub issues for both bug reports and feature requests."
  },
  {
    "objectID": "index.html#what",
    "href": "index.html#what",
    "title": "dbreg",
    "section": "",
    "text": "dbreg is an R package that leverages the power of databases to run regressions on very large datasets, which may not fit into R’s memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics. Our default DuckDB backend provides a powerful, embedded analytics engine to get users up and running with minimal effort. Users can also specify alternative database backends, depending on their computing needs and setup.\nThe dbreg R package is inspired by, and has similar aims to, the duckreg Python package. This implementation offers some idiomatic, R-focused features like a formula interface and “pretty” print methods. But our long-term goal is that these two packages should be aligned in terms of core feature parity."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "dbreg",
    "section": "",
    "text": "dbreg can be installed from R-universe.\ninstall.packages(\n   \"dbreg\",\n   repos = c(\"https://grantmcdermott.r-universe.dev\", getOption(\"repos\"))\n)"
  },
  {
    "objectID": "index.html#quickstart",
    "href": "index.html#quickstart",
    "title": "dbreg",
    "section": "",
    "text": "To get ourselves situated, we’ll first demonstrate by using an in-memory R dataset.\nlibrary(dbreg)\nlibrary(fixest) # for data and comparison\n\ndata(\"trade\", package = \"fixest\")\n\ndbreg(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; Compressed OLS estimation, Dep. Var.: Euros \n#&gt; Observations.: 38,325 (original) | 210 (compressed) \n#&gt; Standard-errors: Heteroskedasticity-robust\n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3         Adj. R2: 0.215289\nBehind the scenes, dbreg has compressed the original dataset down from nearly 40,000 observations to only 210, before running the final (weighted) regression on this much smaller data object. This compression procedure trick follows Wang _et. al. (2021) and effectively allows us to compute on a much lighter object, saving time and memory. We can confirm that it still gives the same result as running fixest::feols on the full dataset:\nfeols(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; OLS estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325\n#&gt; Fixed-effects: Destination: 15,  Origin: 15\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3     Adj. R2: 0.215289\n#&gt;                       Within R2: 0.025914\n\n\n\nFor a more appropriate dbreg use-case, let’s run a regression on some NYC taxi data. (Download instructions here.) The dataset that we’re working with here is about 180 million rows deep and takes up 8.5 GB on disk.1 dbreg offers two basic ways to analyse and interact with data of this size.\n\n\nUse the path argument to read the data directly from disk and perform the compression computation in an ephemeral DuckDB connection. This requires that the data are small enough to fit into RAM… but please note that “small enough” is a relative concept. Thanks to DuckDB’s incredible efficiency, your RAM should be able to handle very large datasets that would otherwise crash your R session, and require only a fraction of the computation time. Note that we also invoke the (optional) verbose  = TRUE argument to print additional information about the estimation strategy.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\", ## path to hive-partitioned dataset\n   vcov = \"hc1\",\n   verbose = TRUE ## optional (print info about the estimation strategy)\n)\n#&gt; [dbreg] Auto strategy:\n#&gt;         - data has 178,544,324 rows with 2 FE (24 unique groups)\n#&gt;         - compression ratio (0.00) satisfies threshold (0.6)\n#&gt;         - decision: compress\n#&gt; [dbreg] Executing compress strategy SQL\n#&gt; \n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Heteroskedasticity-robust\n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000068 1564.742 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.000106 -273.866 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nNote the size of the original dataset, which is nearly 180 million rows, versus the compressed dataset, which is down to only 70k. On my laptop (M4 MacBook Pro) this regression completes in under 3 seconds… and that includes the time it took to determine an optimal estimation strategy, as well as read the data from disk!2\nIn case you were wondering, obtaining clustered standard errors is just as easy; simply pass the relevant cluster variable as a formula to the vcov argument. Since we know that the optimal acceleration strategy is \"compress\", we’ll also go ahead a specify this explicitly to skip the auto strategy overhead.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path     = \"read_parquet('nyc-taxi/**/*.parquet')\",\n   vcov     = ~month,    # clustered SEs\n   strategy = \"compress\" # skip auto strategy overhead\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\n\n\n\nWhile querying on-the-fly with our default DuckDB backend is both convenient and extremely performant, you can also run regressions against existing tables in a persistent database connection. This could be DuckDB, but it could also be any other supported backend. All you need to do is specify the appropriate conn and table arguments.\n# load the DBI package to connect to a persistent database\nlibrary(DBI)\n\n# create connection to persistent DuckDB database (could be any supported backend)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"nyc.db\")\n\n# create a 'taxi' table in our new nyc.db database from our parquet dataset\ndbExecute(\n   con,\n   \"\n   CREATE TABLE taxi AS\n      FROM read_parquet('nyc-taxi/**/*.parquet')\n      SELECT tip_amount, fare_amount, passenger_count, month, vendor_name\n   \"\n)\n\n# now run our regression against this conn+table combo\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   conn = con,     # database connection,\n   table = \"taxi\", # table name\n   vcov = ~month,\n   strategy = \"compress\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nResult: we get the same coefficient and standard error estimates as earlier.\nWe’ll close by doing some (optional) clean up.\ndbRemoveTable(con, \"taxi\")\ndbDisconnect(con)\nunlink(\"nyc.db\") # remove from disk\n\n[!TIP] If you don’t want to create a persistent database (and materialize data), a nice alternative is CREATE VIEW. This lets you define subsets or computed columns on-the-fly. For example, to regress on Q1 2012 data with a day-of-week fixed effect:\ndbExecute(con, \"\n   CREATE VIEW nyc_subset AS\n   SELECT\n      tip_amount, trip_distance, passenger_count,\n      vendor_name, month,\n      dayofweek(dropoff_datetime) AS dofw\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   WHERE year = 2012 AND month &lt;= 3\n\")\n\ndbreg(\n   tip_amount ~ trip_distance + passenger_count | month + dofw + vendor_name,\n   conn = con,\n   table = \"nyc_subset\",\n   vcov = ~dofw\n)"
  },
  {
    "objectID": "index.html#acceleration-strategies",
    "href": "index.html#acceleration-strategies",
    "title": "dbreg",
    "section": "",
    "text": "All of the examples in this README have made use of the \"compress\" strategy. But the compression trick is not the only game in town and dbreg supports several other acceleration strategies: \"moments\", \"demean\", and \"mundlak\". Depending on your data and regression requirements, one of these other strategies may better suit your problem. The good news is that (the default) strategy = \"auto\" option uses some intelligent heuristics to determine which strategy is (probably) optimal for each case. You can set the verbose = TRUE argument to get real-time feedback about the decision criteria being used. The Acceleration Strategies section of the ?dbreg helpfile contains a lot detail about the different options and tradeoffs involved, so please do consult the documentation."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "dbreg",
    "section": "",
    "text": "dbreg is a maturing package and there are a number of features that we still plan to add before submitting it to CRAN. (See our TO-DO list.) We also don’t yet support some standard R operations like interaction terms in the formula. At the same time, the core dbreg() routine has been tested pretty thoroughly and should work in standard cases. Please help us by kicking the tyres and creating GitHub issues for both bug reports and feature requests."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "dbreg",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this dataset would occupy significantly more RAM than 8.5 GB if we loaded it into R’s memory, due to data serialization and the switch to richer representation formats (e.g., ordered factors require more memory). So there’s a good chance that just trying to load this raw dataset into R would cause your whole system to crash… never mind doing any statistical analysis on it.↩︎\nIf we provided an explicit dbreg(..., strategy = \"compress\") argument (thus skipping the automatic strategy determination), then the total computation time drops to less than 1 second…↩︎"
  },
  {
    "objectID": "man/tidiers.html",
    "href": "man/tidiers.html",
    "title": "dbreg",
    "section": "",
    "text": "Provides broom::tidy and broom::glance methods for \"dbreg\" objects.\n\n\n\n## S3 method for class 'dbreg'\ntidy(x, conf.int = FALSE, conf.level = 0.95, fe = FALSE, ...)\n\n## S3 method for class 'dbreg'\nglance(x, ...)\n\n\n\n\n\n\n\nx\n\n\na model of class ‘dbreg’ produced by the dbreg function.\n\n\n\n\nconf.int\n\n\nLogical indicating whether to include confidence intervals. Default is ‘FALSE’.\n\n\n\n\nconf.level\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be tidied too? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nAdditional arguments to tidying method. Currently unused except to handle superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ntidy(mod, conf.int = TRUE)\n\n  term  estimate std.error statistic     p.values  conf.low  conf.high\n1 Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n\ntidy(mod, conf.int = TRUE, fe = TRUE)\n\n         term  estimate std.error statistic     p.values  conf.low  conf.high\n1 (Intercept) 74.188474 2.0544007 36.111978 5.675829e-75 70.128499 78.2484490\n2        Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n3      Month6 12.543643 1.5942530  7.868038 7.184769e-13  9.393027 15.6942587\n4      Month7 16.362079 1.6183409 10.110404 1.431916e-18 13.163860 19.5602984\n5      Month8 16.316286 1.6239233 10.047449 2.091354e-18 13.107035 19.5255376\n6      Month9 10.279216 1.5959361  6.440869 1.590257e-09  7.125274 13.4331579\n\nglance(mod)\n\n  r.squared adj.r.squared     rmse nobs df.residual\n1 0.5884105     0.5744109 6.052589  153         147",
    "crumbs": [
      "Reference",
      "Other methods",
      "glance.dbreg"
    ]
  },
  {
    "objectID": "man/tidiers.html#tidiers-for-dbreg-objects",
    "href": "man/tidiers.html#tidiers-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Provides broom::tidy and broom::glance methods for \"dbreg\" objects.\n\n\n\n## S3 method for class 'dbreg'\ntidy(x, conf.int = FALSE, conf.level = 0.95, fe = FALSE, ...)\n\n## S3 method for class 'dbreg'\nglance(x, ...)\n\n\n\n\n\n\n\nx\n\n\na model of class ‘dbreg’ produced by the dbreg function.\n\n\n\n\nconf.int\n\n\nLogical indicating whether to include confidence intervals. Default is ‘FALSE’.\n\n\n\n\nconf.level\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be tidied too? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nAdditional arguments to tidying method. Currently unused except to handle superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ntidy(mod, conf.int = TRUE)\n\n  term  estimate std.error statistic     p.values  conf.low  conf.high\n1 Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n\ntidy(mod, conf.int = TRUE, fe = TRUE)\n\n         term  estimate std.error statistic     p.values  conf.low  conf.high\n1 (Intercept) 74.188474 2.0544007 36.111978 5.675829e-75 70.128499 78.2484490\n2        Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n3      Month6 12.543643 1.5942530  7.868038 7.184769e-13  9.393027 15.6942587\n4      Month7 16.362079 1.6183409 10.110404 1.431916e-18 13.163860 19.5602984\n5      Month8 16.316286 1.6239233 10.047449 2.091354e-18 13.107035 19.5255376\n6      Month9 10.279216 1.5959361  6.440869 1.590257e-09  7.125274 13.4331579\n\nglance(mod)\n\n  r.squared adj.r.squared     rmse nobs df.residual\n1 0.5884105     0.5744109 6.052589  153         147",
    "crumbs": [
      "Reference",
      "Other methods",
      "glance.dbreg"
    ]
  },
  {
    "objectID": "man/confint.dbreg.html",
    "href": "man/confint.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Confidence intervals for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nconfint(object, parm, level = 0.95, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\nparm\n\n\na specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.\n\n\n\n\nlevel\n\n\nthe confidence level required. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "confint.dbreg"
    ]
  },
  {
    "objectID": "man/confint.dbreg.html#confidence-intervals-for-dbreg-objects",
    "href": "man/confint.dbreg.html#confidence-intervals-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Confidence intervals for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nconfint(object, parm, level = 0.95, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\nparm\n\n\na specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.\n\n\n\n\nlevel\n\n\nthe confidence level required. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "confint.dbreg"
    ]
  },
  {
    "objectID": "man/predict.dbreg.html",
    "href": "man/predict.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Predict method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\npredict(\n  object,\n  newdata = NULL,\n  interval = c(\"none\", \"confidence\", \"prediction\"),\n  level = 0.95,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\nnewdata\n\n\nData frame for predictions. Required for objects that were estimated using the ‘\"mundlak\"’ and ‘\"moments\"’ strategies, since ‘dbreg’ does not retain any data for these estimations.\n\n\n\n\ninterval\n\n\nType of interval to compute: ‘\"none\"’ (default), ‘\"confidence\"’, or ‘\"prediction\"’. Note that ‘\"confidence\"’ intervals reflect uncertainty in the estimated mean, while ‘\"prediction\"’ intervals additionally account for residual variance. See predict.lm for details.\n\n\n\n\nlevel\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\n…\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\nPredicting on ‘dbreg’ objects should generally work as expected. However, predictions from ‘\"demean\"’ strategy models carry two important caveats:\n\nPredictions require group means to transform back to the original scale. If ‘newdata’ contains the outcome variable, group means are computed from ‘newdata’ and used to return level predictions. If the outcome is absent, within-group predictions (deviations from group means) are returned instead, with a message.\nConfidence/prediction intervals are not supported. A demeaned model cannot account for uncertainty in the fixed-effects (since these were absorbed at estimation time), which in turn would yield intervals that are too narrow. Requesting intervals for ‘\"demean\"’ strategy models will return point predictions with a message. Users should re-estimate with a different strategy if intervals are needed.\n\n\n\n\n[dbreg()] for examples.\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "predict.dbreg"
    ]
  },
  {
    "objectID": "man/predict.dbreg.html#predict-method-for-dbreg-objects",
    "href": "man/predict.dbreg.html#predict-method-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Predict method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\npredict(\n  object,\n  newdata = NULL,\n  interval = c(\"none\", \"confidence\", \"prediction\"),\n  level = 0.95,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\nnewdata\n\n\nData frame for predictions. Required for objects that were estimated using the ‘\"mundlak\"’ and ‘\"moments\"’ strategies, since ‘dbreg’ does not retain any data for these estimations.\n\n\n\n\ninterval\n\n\nType of interval to compute: ‘\"none\"’ (default), ‘\"confidence\"’, or ‘\"prediction\"’. Note that ‘\"confidence\"’ intervals reflect uncertainty in the estimated mean, while ‘\"prediction\"’ intervals additionally account for residual variance. See predict.lm for details.\n\n\n\n\nlevel\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\n…\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\nPredicting on ‘dbreg’ objects should generally work as expected. However, predictions from ‘\"demean\"’ strategy models carry two important caveats:\n\nPredictions require group means to transform back to the original scale. If ‘newdata’ contains the outcome variable, group means are computed from ‘newdata’ and used to return level predictions. If the outcome is absent, within-group predictions (deviations from group means) are returned instead, with a message.\nConfidence/prediction intervals are not supported. A demeaned model cannot account for uncertainty in the fixed-effects (since these were absorbed at estimation time), which in turn would yield intervals that are too narrow. Requesting intervals for ‘\"demean\"’ strategy models will return point predictions with a message. Users should re-estimate with a different strategy if intervals are needed.\n\n\n\n\n[dbreg()] for examples.\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "predict.dbreg"
    ]
  },
  {
    "objectID": "man/vcov.dbreg.html",
    "href": "man/vcov.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Variance-covariance matrix for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nvcov(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\n…\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "vcov.dbreg"
    ]
  },
  {
    "objectID": "man/vcov.dbreg.html#variance-covariance-matrix-for-dbreg-objects",
    "href": "man/vcov.dbreg.html#variance-covariance-matrix-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Variance-covariance matrix for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nvcov(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\n…\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "vcov.dbreg"
    ]
  },
  {
    "objectID": "man/coef.dbreg.html",
    "href": "man/coef.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Extract coefficients from dbreg objects\n\n\n\n## S3 method for class 'dbreg'\ncoef(object, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "coef.dbreg"
    ]
  },
  {
    "objectID": "man/coef.dbreg.html#extract-coefficients-from-dbreg-objects",
    "href": "man/coef.dbreg.html#extract-coefficients-from-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Extract coefficients from dbreg objects\n\n\n\n## S3 method for class 'dbreg'\ncoef(object, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "coef.dbreg"
    ]
  },
  {
    "objectID": "man/print.dbreg.html",
    "href": "man/print.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nprint(x, fe = FALSE, ...)\n\n\n\n\n\n\n\nx\n\n\n‘dbreg’ object.\n\n\n\n\nfe\n\n\nShould the fixed effects be displayed? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nOther arguments passed to print. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n# mod # same as below\nprint(mod)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\nprint(mod, fe = TRUE)  # include fixed effects\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411",
    "crumbs": [
      "Reference",
      "Base methods",
      "print.dbreg"
    ]
  },
  {
    "objectID": "man/print.dbreg.html#print-method-for-dbreg-objects",
    "href": "man/print.dbreg.html#print-method-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nprint(x, fe = FALSE, ...)\n\n\n\n\n\n\n\nx\n\n\n‘dbreg’ object.\n\n\n\n\nfe\n\n\nShould the fixed effects be displayed? Default is ‘FALSE’.\n\n\n\n\n…\n\n\nOther arguments passed to print. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n# mod # same as below\nprint(mod)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\nprint(mod, fe = TRUE)  # include fixed effects\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411",
    "crumbs": [
      "Reference",
      "Base methods",
      "print.dbreg"
    ]
  },
  {
    "objectID": "vignettes/intro.html",
    "href": "vignettes/intro.html",
    "title": "Introduction to dbreg",
    "section": "",
    "text": "COMING SOON.\nIn the meantime, please take a look at the README examples. The ?dbreg helpfile also contains a lot of useful information, especially concerning the various acceleration strategies."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 dbreg authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "News",
    "section": "",
    "text": "This NEWS file is best viewed on our website.\n\n\nWebsite\n\nWe finally have a package website: grantmcdermott.com/dbreg 🎉\n\nBreaking changes\n\nThe behaviour of the \"mundlak\" strategy has been changed. (#24)\n\nThe old \"mundlak\" strategy is remapped to the new \"demean\" (alias \"within\") strategy, to better reflect the fact that this strategy invokes a (double) demeaning transformation. Users who want the old behaviour should thus use \"demean\" instead of \"mundlak\".\nSimultaneously, we now provide a revised \"mundlak\" strategy that implements a “true” Mundlak/CRE estimator; see New features below.\n\nFor estimations with two fixed effects on unbalanced panels, strategy=\"auto\" now errors when the compression limits are exceeded. It does this to avoid silently selecting a different estimand (i.e., Mundlak/CRE instead of TWFE). For these ambiguous cases, users will now be prompted to explicitly choose \"compress\" (with higher limits) or \"mundlak\" (different model and thus potentially different coefficients). (#24)\nThe default verbose behaviour is changed to FALSE. Users can revert to the old behaviour for a single call (i.e., dbreg(..., verbose = TRUE)), or set it globally (i.e., options(dbreg.verbose = TRUE)). (#33)\nTechnically not a breaking change, since we currently support backwards compatibility, but several minor arguments have been renamed/superseded. (#34)\n\nquery_only -&gt; sql_only (in dbreg)\nfes -&gt; fe (in print.dbreg, coef.dbreg, confint.dbreg, etc.)\n\n\nMajor new features\n\nWe have added new/revised acceleration strategies. (#24)\n\nThe \"demean\" (alias \"within\") strategy implements a (double) demeaning transformation and is particularly suited to (balanced) panels with one or two fixed effects. Note the underlying query is the same as the old \"mundlak\" strategy, which was somewhat erroneously named. Speaking of which…\nThe revised \"mundlak\" strategy now implements a “true” Mundlak/CRE (correlated random effects) estimator by regressing Y on X plus group means of X. Unlike the \"demean\" strategy (above), this revised \"mundlak\" model obtains consistent coefficients regardless of panel structure (incl. unbalanced panels) and supports any number of fixed effects. However, users should note that Mundlak/CRE is a different model from “vanilla” fixed effects—albeit asymptotically equivalent under certain assumptions—and may obtain different coefficients as a result.\nPlease consult the expanded Acceleration Strategies section in the ?dbreg helpfile for technical details.\n\nAdd support for clustered standard errors. Follows the fixest API:\ndbreg(..., vcov = ~cluster_var)\nPlease note that these clustered SEs are computed analytically (not bootstrapped) and should thus add minimal overhead to your regressions. See the updated README for examples. (#29)\nAdded support for binscatter regressions on database backends via the dbbinsreg() function. This function attempts to mimic the main API of the binsreg package (modulo some simplifications), but should be much faster on big datasets. (#32)\n\nOther new features\n\nThe \"auto\" strategy logic now considers a compress_nmax threshold, which governs the maximum allowable size of the compressed data object (default threshold = 1 million rows). This additional guardrail is intended to avoid cases where the \"compress\" strategy satisfies the compress_ratio threshold, but could still return a prohibitively large dataset. The most common example would be querying a massive dataset on a remote database, where network latency makes data I/O transfer expensive, even though we’ve achieved good compression relative to the original data size. (#10)\n\nAside: Improved documentation and messaging (when verbose = TRUE) should also help users understand the \"auto\" strategy decision tree.\n\nEnabled weights for double demean (within) specification. (#13)\nEsimations now report some goodness-of-fit statistics like R2 and RMSE, powered by the (user-facing) gof() function. (#21)\nAdded support for various *.dbreg methods (#21, #30):\n\nFrom stats: coef(), confint(), predict(), and vcov().\nFrom broom/generics: tidy() and glance(). These also enable post-processing operations like exporting results to coefficient tables via modelsummary::msummary(). Thanks to @HariharanJayashankar for the request in #20.\n\nBetter documentation. (#28, #36, #43, and various other PRs)\n\nBug fixes\n\nAdded QR decomposition fallback for regression calculations, for cases where the default Cholesky solver fails. (#7)\nImproved integration for running regressions on AWS Athena datasets via the noctua package/driver. (#8)\nAutomatically drop incomplete cases (i.e., missing values) prior to any aggregation steps, avoiding mismatched matrices during estimation. (#19)\nUser-specified compress_ratio values should now bind in all cases. Previously, these could sometimes be silently ignored due to internal overrides. Also, clarify in the argument documentation that the default (automatic) compress_ratio threshold can vary based on heuristics related to model structure. (#25)\nCorrectly estimate HC1 standard errors for the \"moments\", \"demean\", and \"mundlak\" strategies. While the old analytic HC1 approach worked (and still does) for the \"compress\" case, it led to misleading SEs for these other strategies. The fix does impose some additional computational overhead, since it requires a second pass over the data to calculate the individual errors and “meat” of the sandwich matrix. But testing suggests that this leads to a &lt;2 increase in total estimation time, which seems a reasonable tradeoff for heteroskedastic-robust SEs. (#27)\nFixed a bug for in-memory dbreg(..., data = &lt;data&gt;) cases, where factor variables in X could cause the \"compress\" strategy to fail. (#41)\n\nInternals\n\nAdded unit testing framework using tinytest. (#16)\nAdded GitHub Actions CI. (#18)\n\n\n\n\nIMPORTANT BREAKING CHANGE:\nThe package has been renamed to dbreg to better reflect the fact that it supports multiple database backends. (#4)\nOther breaking changes\n\nThe default vcov is now “iid”. (#2 @grantmcdermott)\n\nNew features\n\nThe new dbreg(..., strategy = &lt;strategy&gt;) argument allows users to choose between different acceleration strategies for efficient computation of the regression coefficients and standard errors. This includes \"compress\" (the old default), as well as \"mundlak\" or \"moments\". The latter two strategies are newly introduced in dbreg v0.0.2 and offer important advantages in the case of true panel data. If an explicit strategy is not provided by the user, then dbreg() will invoke some internal heuristics to determine the optimal strategy based on the size and structure of the data. (#2 @jamesbrandecon and @grantmcdermott)\n\nProject\n\n@jamesbrandecon has joined the project as a core contributor.\n\n\n\n\n\nInitial GitHub release."
  },
  {
    "objectID": "NEWS.html#dev-version",
    "href": "NEWS.html#dev-version",
    "title": "News",
    "section": "",
    "text": "Website\n\nWe finally have a package website: grantmcdermott.com/dbreg 🎉\n\nBreaking changes\n\nThe behaviour of the \"mundlak\" strategy has been changed. (#24)\n\nThe old \"mundlak\" strategy is remapped to the new \"demean\" (alias \"within\") strategy, to better reflect the fact that this strategy invokes a (double) demeaning transformation. Users who want the old behaviour should thus use \"demean\" instead of \"mundlak\".\nSimultaneously, we now provide a revised \"mundlak\" strategy that implements a “true” Mundlak/CRE estimator; see New features below.\n\nFor estimations with two fixed effects on unbalanced panels, strategy=\"auto\" now errors when the compression limits are exceeded. It does this to avoid silently selecting a different estimand (i.e., Mundlak/CRE instead of TWFE). For these ambiguous cases, users will now be prompted to explicitly choose \"compress\" (with higher limits) or \"mundlak\" (different model and thus potentially different coefficients). (#24)\nThe default verbose behaviour is changed to FALSE. Users can revert to the old behaviour for a single call (i.e., dbreg(..., verbose = TRUE)), or set it globally (i.e., options(dbreg.verbose = TRUE)). (#33)\nTechnically not a breaking change, since we currently support backwards compatibility, but several minor arguments have been renamed/superseded. (#34)\n\nquery_only -&gt; sql_only (in dbreg)\nfes -&gt; fe (in print.dbreg, coef.dbreg, confint.dbreg, etc.)\n\n\nMajor new features\n\nWe have added new/revised acceleration strategies. (#24)\n\nThe \"demean\" (alias \"within\") strategy implements a (double) demeaning transformation and is particularly suited to (balanced) panels with one or two fixed effects. Note the underlying query is the same as the old \"mundlak\" strategy, which was somewhat erroneously named. Speaking of which…\nThe revised \"mundlak\" strategy now implements a “true” Mundlak/CRE (correlated random effects) estimator by regressing Y on X plus group means of X. Unlike the \"demean\" strategy (above), this revised \"mundlak\" model obtains consistent coefficients regardless of panel structure (incl. unbalanced panels) and supports any number of fixed effects. However, users should note that Mundlak/CRE is a different model from “vanilla” fixed effects—albeit asymptotically equivalent under certain assumptions—and may obtain different coefficients as a result.\nPlease consult the expanded Acceleration Strategies section in the ?dbreg helpfile for technical details.\n\nAdd support for clustered standard errors. Follows the fixest API:\ndbreg(..., vcov = ~cluster_var)\nPlease note that these clustered SEs are computed analytically (not bootstrapped) and should thus add minimal overhead to your regressions. See the updated README for examples. (#29)\nAdded support for binscatter regressions on database backends via the dbbinsreg() function. This function attempts to mimic the main API of the binsreg package (modulo some simplifications), but should be much faster on big datasets. (#32)\n\nOther new features\n\nThe \"auto\" strategy logic now considers a compress_nmax threshold, which governs the maximum allowable size of the compressed data object (default threshold = 1 million rows). This additional guardrail is intended to avoid cases where the \"compress\" strategy satisfies the compress_ratio threshold, but could still return a prohibitively large dataset. The most common example would be querying a massive dataset on a remote database, where network latency makes data I/O transfer expensive, even though we’ve achieved good compression relative to the original data size. (#10)\n\nAside: Improved documentation and messaging (when verbose = TRUE) should also help users understand the \"auto\" strategy decision tree.\n\nEnabled weights for double demean (within) specification. (#13)\nEsimations now report some goodness-of-fit statistics like R2 and RMSE, powered by the (user-facing) gof() function. (#21)\nAdded support for various *.dbreg methods (#21, #30):\n\nFrom stats: coef(), confint(), predict(), and vcov().\nFrom broom/generics: tidy() and glance(). These also enable post-processing operations like exporting results to coefficient tables via modelsummary::msummary(). Thanks to @HariharanJayashankar for the request in #20.\n\nBetter documentation. (#28, #36, #43, and various other PRs)\n\nBug fixes\n\nAdded QR decomposition fallback for regression calculations, for cases where the default Cholesky solver fails. (#7)\nImproved integration for running regressions on AWS Athena datasets via the noctua package/driver. (#8)\nAutomatically drop incomplete cases (i.e., missing values) prior to any aggregation steps, avoiding mismatched matrices during estimation. (#19)\nUser-specified compress_ratio values should now bind in all cases. Previously, these could sometimes be silently ignored due to internal overrides. Also, clarify in the argument documentation that the default (automatic) compress_ratio threshold can vary based on heuristics related to model structure. (#25)\nCorrectly estimate HC1 standard errors for the \"moments\", \"demean\", and \"mundlak\" strategies. While the old analytic HC1 approach worked (and still does) for the \"compress\" case, it led to misleading SEs for these other strategies. The fix does impose some additional computational overhead, since it requires a second pass over the data to calculate the individual errors and “meat” of the sandwich matrix. But testing suggests that this leads to a &lt;2 increase in total estimation time, which seems a reasonable tradeoff for heteroskedastic-robust SEs. (#27)\nFixed a bug for in-memory dbreg(..., data = &lt;data&gt;) cases, where factor variables in X could cause the \"compress\" strategy to fail. (#41)\n\nInternals\n\nAdded unit testing framework using tinytest. (#16)\nAdded GitHub Actions CI. (#18)"
  },
  {
    "objectID": "NEWS.html#dbreg-0.0.2",
    "href": "NEWS.html#dbreg-0.0.2",
    "title": "News",
    "section": "",
    "text": "IMPORTANT BREAKING CHANGE:\nThe package has been renamed to dbreg to better reflect the fact that it supports multiple database backends. (#4)\nOther breaking changes\n\nThe default vcov is now “iid”. (#2 @grantmcdermott)\n\nNew features\n\nThe new dbreg(..., strategy = &lt;strategy&gt;) argument allows users to choose between different acceleration strategies for efficient computation of the regression coefficients and standard errors. This includes \"compress\" (the old default), as well as \"mundlak\" or \"moments\". The latter two strategies are newly introduced in dbreg v0.0.2 and offer important advantages in the case of true panel data. If an explicit strategy is not provided by the user, then dbreg() will invoke some internal heuristics to determine the optimal strategy based on the size and structure of the data. (#2 @jamesbrandecon and @grantmcdermott)\n\nProject\n\n@jamesbrandecon has joined the project as a core contributor."
  },
  {
    "objectID": "NEWS.html#duckreg-0.0.1",
    "href": "NEWS.html#duckreg-0.0.1",
    "title": "News",
    "section": "",
    "text": "Initial GitHub release."
  },
  {
    "objectID": "man/dbbinsreg.html",
    "href": "man/dbbinsreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Performs binned regression entirely in SQL, returning plot-ready data with estimated bin means or piecewise polynomial fits. The API is designed to be compatible with the binsreg package by Cattaneo, Crump, Farrell, and Feng (2024). Supports unconditional and conditional models (with controls and/or fixed effects).\n\n\n\ndbbinsreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  points = c(0, 0),\n  line = NULL,\n  linegrid = 20,\n  nbins = 20,\n  binspos = \"qs\",\n  randcut = NULL,\n  ci = TRUE,\n  cb = FALSE,\n  vcov = NULL,\n  level = 0.95,\n  nsims = 500,\n  strategy = \"auto\",\n  plot = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  dots = NULL\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the binscatter relation. The first variable on the RHS is the running variable; additional variables are controls. Fixed effects go after |. Examples:\n\n\ny ~ x: simple binscatter\n\n\ny ~ x + w1 + w2: binscatter with controls\n\n\ny ~ x | fe: binscatter with fixed effects\n\n\ny ~ x + w1 + w2 | fe: binscatter with controls and fixed effects\n\n\n\n\n\n\nconn\n\n\nDatabase connection, e.g. created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g. “mydata/**/.*parquet”. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g. “read_parquet(’mydata/**/*.parquet’)“ for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\npoints\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the points (point estimates at bin means). Default is c(0, 0) for canonical binscatter (bin means). Set to NULL or FALSE to suppress points. The smoothness s must satisfy s &lt;= p.\n\n\n\n\nline\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the line (evaluated on a grid within bins). Default is NULL (no line). Set to TRUE for c(0, 0) or a vector like c(1, 1) for piecewise linear with continuity constraints. The smoothness \\(s\\) must satisfy s &lt;= p.\n\n\n\n\nlinegrid\n\n\nNumber of evaluation points per bin for the line. Default is 20.\n\n\n\n\nnbins\n\n\nInteger number of bins. Default is 20.\n\n\n\n\nbinspos\n\n\nBin positioning method. One of either “qs” (quantile-spaced, equal-count bins, the default), “es” (evenly-spaced, equal-width bins), or a numeric vector of knot positions for manual specification.\n\n\n\n\nrandcut\n\n\nNumeric in the range (0,1]. Controls the random sampling fraction for bin boundary computation on large datasets. If NULL (the default), then sampling is automatic: 0.01 (1%) for datasets exceeding 1 million rows and 1 (100%) otherwise. Note that sampling is only used for computing the bin boundaries, since this requires an expensive ranking operation. The subsequent, primary regression operations use all of the data.\n\n\n\n\nci\n\n\nLogical. Calculate standard errors and confidence intervals for points? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Calculate simultaneous confidence bands using simulation? Default is FALSE.\n\n\n\n\nvcov\n\n\nCharacter string or formula for standard errors. Options are “HC1” (default, heteroskedasticity-robust, matches binsreg), “iid”, or a one-sided formula for clustered standard errors (e.g., ~cluster_var).\n\n\n\n\nlevel\n\n\nNumeric in the range [0,1], giving the confidence level for the confidence levels and/or bands. Default is 0.95.\n\n\n\n\nnsims\n\n\nNumber of simulation draws for confidence band computation. Default is 500. Only used when cb = TRUE.\n\n\n\n\nstrategy\n\n\nAcceleration strategy passed to dbreg when smoothness is zero. Options are “auto” (default), “compress”, or “scan”. This parameter is ignored when s (smoothness parameter in points or lines) &gt; 0. See dbreg for details.\n\n\n\n\nplot\n\n\nLogical. If TRUE (default), a plot is produced as a side effect. Set to FALSE to suppress plotting.\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\ndots\n\n\nAlias for points for binsreg compatibility. If not NULL, overrides the points argument.\n\n\n\n\n\n\nA list of class \"dbbinsreg\" containing:\n\n\npoints\n\n\nData frame of the point estimates (one row per bin): x (bin mean), bin, and fit (fitted value). If ci=TRUE in the original call, then also includes the columns: se, lwr, and upr. Similarly, if cb=TRUE, then includes the columns: cb_lwr and cb_upr.\n\n\nline\n\n\nData frame of the line estimates (multiple rows per bin): x, bin, fit. Only present if line is specified.\n\n\nbins\n\n\nData frame with bin boundaries: id (bin number), left (left endpoint), right (right endpoint).\n\n\nmodel\n\n\nThe fitted dbreg model object (for points).\n\n\nopt\n\n\nList of options used: points, line, nbins, binspos, etc.\n\n\nIf plot = TRUE (the default), a binscatter plot is also produced as a side effect. See plot.dbbinsreg for plot customization.\n\n\n\nThe dbbinsreg function is deeply inspired by the binsreg package (Cattaneo et. al., 2024). The main difference is that dbbinsreg performs most of its computation on a database backend, employing various acceration strategies, which makes it particularly suitable for large datasets (which may not fit in memory). At the same time, the database backend introduces its own set of tradeoffs. We cover the most important points of similarity and difference below.\n\nWe aim to mimic the binsreg API as much as possible. Key parameter mappings include:\n\n\npoints (alias dots): Point estimates at bin means\n\n\nc(0,0): Canonical binscatter (bin means)\n\n\nc(p,0): Piecewise polynomial of degree \\(p\\), no smoothness\n\n\nc(p,s): Piecewise polynomial with \\(s\\) smoothness constraints\n\n\n\n\nline: Same as points but evaluated on a finer grid for smooth visualization\n\n\nbinspos: Bin positioning\n\n\n“qs”: Quantile-spaced (equal count)\n\n\n“es”: Evenly-spaced (equal width)\n\n\n\n\nImportant: Unlike binsreg, dbbinsreg does not automatically select the IMSE-optimal number of bins. Users must specify nbins manually. For guidance on bin selection, see binsregselect or Cattaneo et al. (2024).\n\nWhen ci = TRUE (default), pointwise confidence intervals (CIs) are computed at each bin mean using standard asymptotic theory. When cb = TRUE, simultaneous confidence bands (CBs) are computed using a simulation-based sup-\\(t\\) procedure:\n\n\nDraw nsims samples from the asymptotic distribution of the estimator\n\n\nCompute the supremum of the \\(t\\)-statistics across all bins for each draw\n\n\nUse the (\\(1-\\alpha\\)) quantile of these suprema as the critical value\n\n\nThe confidence band is wider than pointwise CIs and provides simultaneous coverage: with (\\(1-\\alpha\\)) probability, the entire true function lies within the band. This is useful for making statements about the overall shape of the relationship rather than individual point estimates.\nThere are two important caveats, regarding dbbinsreg’s CB support:\n\n\nUnlike binsreg, which evaluates CB on a fine grid within each bin, dbbinsreg computes CB only at bin means (same points as CI). This is much simpler for our backend SQL implementation and should be sufficient for most applications.\n\n\nCBs are currently only supported for unconstrained estimation (smoothness s = 0). When cb = TRUE with s &gt; 0, a warning is issued and CB is skipped.\n\n\n\nWhen using quantile-spaced bins (binspos = “qs”), dbbinsreg uses SQL’s NTILE() window function, while binsreg uses R’s quantile with type = 2. These algorithms have slightly different tie-breaking behavior, which can cause small differences in bin assignments at boundaries. In practice, differences are typically &lt;1% and become negligible with larger datasets. To match binsreg exactly, compute quantile breaks on a subset of data in R and pass them via the binspos argument as a numeric vector.\n\n\n\nCattaneo, M. D., R. K. Crump, M. H. Farrell, and Y. Feng (2024). On Binscatter. American Economic Review, 114(5): 1488-1514.\n\n\n\nplot.dbbinsreg for plot customization, dbreg for the underlying regression engine, binsreg for the original implementation.\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# For plot customization, save the model object so you can pass additional args\n# to (tiny)plot.dbbinsreg\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\nplot(bs, theme = \"clean\", main = \"A simple binscatter example\")\n\n\n\n\n\n\n\n# Alternatively: you can also set a global (tiny)plot theme\ntinyplot::tinytheme(\"classic\")\n\n# Piecewise linear, no smoothness\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.2059102  41.73348  42.54238\n2   2.5517241   2  52.31034 0.5770938  51.17680  53.44389\n3   4.8620690   3  66.08621 0.8504127  64.41581  67.75661\n4   7.1724138   4  83.86207 1.9453576  80.04095  87.68319\n5   9.4827586   5 103.18966 2.9650856  97.36556 109.01375\n6  11.8965517   6 126.68966 4.3493139 118.14663 135.23268\n7  14.4482759   7 149.31034 5.7331130 138.04922 160.57147\n8  16.8275862   8 176.46552 6.8938017 162.92454 190.00649\n9  19.1929825   9 205.84211 8.3187460 189.50222 222.18199\n10 20.7894737  10 215.35088 9.0642870 197.54658 233.15517\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Piecewise linear with continuity\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr   cb_lwr    cb_upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554 33.23757  51.03829\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965 33.58293  71.03776\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643 38.53218  93.64024\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302 36.13235 131.59178\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700 37.09528 169.28403\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966 33.34510 220.03421\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691 25.14593 273.47476\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028 27.41446 325.51657\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137 25.99386 385.69035\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843 19.98252 410.71923\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr     cb_lwr    cb_upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436   9.804767  40.77557\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409  22.027418  50.64445\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807  34.597558  64.16596\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354  51.384677  83.37397\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063  67.512911 104.06643\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043  87.108149 133.87893\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390 101.885603 161.31390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803 123.465626 195.50429\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526 143.600200 231.66043\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180 152.340134 244.54754\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)\ntinyplot::tinytheme() # reset plot theme",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbbinsreg"
    ]
  },
  {
    "objectID": "man/dbbinsreg.html#run-a-binscatter-regression-on-a-database-backend-and-plot-the-result",
    "href": "man/dbbinsreg.html#run-a-binscatter-regression-on-a-database-backend-and-plot-the-result",
    "title": "dbreg",
    "section": "",
    "text": "Performs binned regression entirely in SQL, returning plot-ready data with estimated bin means or piecewise polynomial fits. The API is designed to be compatible with the binsreg package by Cattaneo, Crump, Farrell, and Feng (2024). Supports unconditional and conditional models (with controls and/or fixed effects).\n\n\n\ndbbinsreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  points = c(0, 0),\n  line = NULL,\n  linegrid = 20,\n  nbins = 20,\n  binspos = \"qs\",\n  randcut = NULL,\n  ci = TRUE,\n  cb = FALSE,\n  vcov = NULL,\n  level = 0.95,\n  nsims = 500,\n  strategy = \"auto\",\n  plot = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  dots = NULL\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the binscatter relation. The first variable on the RHS is the running variable; additional variables are controls. Fixed effects go after |. Examples:\n\n\ny ~ x: simple binscatter\n\n\ny ~ x + w1 + w2: binscatter with controls\n\n\ny ~ x | fe: binscatter with fixed effects\n\n\ny ~ x + w1 + w2 | fe: binscatter with controls and fixed effects\n\n\n\n\n\n\nconn\n\n\nDatabase connection, e.g. created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g. “mydata/**/.*parquet”. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g. “read_parquet(’mydata/**/*.parquet’)“ for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\npoints\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the points (point estimates at bin means). Default is c(0, 0) for canonical binscatter (bin means). Set to NULL or FALSE to suppress points. The smoothness s must satisfy s &lt;= p.\n\n\n\n\nline\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the line (evaluated on a grid within bins). Default is NULL (no line). Set to TRUE for c(0, 0) or a vector like c(1, 1) for piecewise linear with continuity constraints. The smoothness \\(s\\) must satisfy s &lt;= p.\n\n\n\n\nlinegrid\n\n\nNumber of evaluation points per bin for the line. Default is 20.\n\n\n\n\nnbins\n\n\nInteger number of bins. Default is 20.\n\n\n\n\nbinspos\n\n\nBin positioning method. One of either “qs” (quantile-spaced, equal-count bins, the default), “es” (evenly-spaced, equal-width bins), or a numeric vector of knot positions for manual specification.\n\n\n\n\nrandcut\n\n\nNumeric in the range (0,1]. Controls the random sampling fraction for bin boundary computation on large datasets. If NULL (the default), then sampling is automatic: 0.01 (1%) for datasets exceeding 1 million rows and 1 (100%) otherwise. Note that sampling is only used for computing the bin boundaries, since this requires an expensive ranking operation. The subsequent, primary regression operations use all of the data.\n\n\n\n\nci\n\n\nLogical. Calculate standard errors and confidence intervals for points? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Calculate simultaneous confidence bands using simulation? Default is FALSE.\n\n\n\n\nvcov\n\n\nCharacter string or formula for standard errors. Options are “HC1” (default, heteroskedasticity-robust, matches binsreg), “iid”, or a one-sided formula for clustered standard errors (e.g., ~cluster_var).\n\n\n\n\nlevel\n\n\nNumeric in the range [0,1], giving the confidence level for the confidence levels and/or bands. Default is 0.95.\n\n\n\n\nnsims\n\n\nNumber of simulation draws for confidence band computation. Default is 500. Only used when cb = TRUE.\n\n\n\n\nstrategy\n\n\nAcceleration strategy passed to dbreg when smoothness is zero. Options are “auto” (default), “compress”, or “scan”. This parameter is ignored when s (smoothness parameter in points or lines) &gt; 0. See dbreg for details.\n\n\n\n\nplot\n\n\nLogical. If TRUE (default), a plot is produced as a side effect. Set to FALSE to suppress plotting.\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\ndots\n\n\nAlias for points for binsreg compatibility. If not NULL, overrides the points argument.\n\n\n\n\n\n\nA list of class \"dbbinsreg\" containing:\n\n\npoints\n\n\nData frame of the point estimates (one row per bin): x (bin mean), bin, and fit (fitted value). If ci=TRUE in the original call, then also includes the columns: se, lwr, and upr. Similarly, if cb=TRUE, then includes the columns: cb_lwr and cb_upr.\n\n\nline\n\n\nData frame of the line estimates (multiple rows per bin): x, bin, fit. Only present if line is specified.\n\n\nbins\n\n\nData frame with bin boundaries: id (bin number), left (left endpoint), right (right endpoint).\n\n\nmodel\n\n\nThe fitted dbreg model object (for points).\n\n\nopt\n\n\nList of options used: points, line, nbins, binspos, etc.\n\n\nIf plot = TRUE (the default), a binscatter plot is also produced as a side effect. See plot.dbbinsreg for plot customization.\n\n\n\nThe dbbinsreg function is deeply inspired by the binsreg package (Cattaneo et. al., 2024). The main difference is that dbbinsreg performs most of its computation on a database backend, employing various acceration strategies, which makes it particularly suitable for large datasets (which may not fit in memory). At the same time, the database backend introduces its own set of tradeoffs. We cover the most important points of similarity and difference below.\n\nWe aim to mimic the binsreg API as much as possible. Key parameter mappings include:\n\n\npoints (alias dots): Point estimates at bin means\n\n\nc(0,0): Canonical binscatter (bin means)\n\n\nc(p,0): Piecewise polynomial of degree \\(p\\), no smoothness\n\n\nc(p,s): Piecewise polynomial with \\(s\\) smoothness constraints\n\n\n\n\nline: Same as points but evaluated on a finer grid for smooth visualization\n\n\nbinspos: Bin positioning\n\n\n“qs”: Quantile-spaced (equal count)\n\n\n“es”: Evenly-spaced (equal width)\n\n\n\n\nImportant: Unlike binsreg, dbbinsreg does not automatically select the IMSE-optimal number of bins. Users must specify nbins manually. For guidance on bin selection, see binsregselect or Cattaneo et al. (2024).\n\nWhen ci = TRUE (default), pointwise confidence intervals (CIs) are computed at each bin mean using standard asymptotic theory. When cb = TRUE, simultaneous confidence bands (CBs) are computed using a simulation-based sup-\\(t\\) procedure:\n\n\nDraw nsims samples from the asymptotic distribution of the estimator\n\n\nCompute the supremum of the \\(t\\)-statistics across all bins for each draw\n\n\nUse the (\\(1-\\alpha\\)) quantile of these suprema as the critical value\n\n\nThe confidence band is wider than pointwise CIs and provides simultaneous coverage: with (\\(1-\\alpha\\)) probability, the entire true function lies within the band. This is useful for making statements about the overall shape of the relationship rather than individual point estimates.\nThere are two important caveats, regarding dbbinsreg’s CB support:\n\n\nUnlike binsreg, which evaluates CB on a fine grid within each bin, dbbinsreg computes CB only at bin means (same points as CI). This is much simpler for our backend SQL implementation and should be sufficient for most applications.\n\n\nCBs are currently only supported for unconstrained estimation (smoothness s = 0). When cb = TRUE with s &gt; 0, a warning is issued and CB is skipped.\n\n\n\nWhen using quantile-spaced bins (binspos = “qs”), dbbinsreg uses SQL’s NTILE() window function, while binsreg uses R’s quantile with type = 2. These algorithms have slightly different tie-breaking behavior, which can cause small differences in bin assignments at boundaries. In practice, differences are typically &lt;1% and become negligible with larger datasets. To match binsreg exactly, compute quantile breaks on a subset of data in R and pass them via the binspos argument as a numeric vector.\n\n\n\nCattaneo, M. D., R. K. Crump, M. H. Farrell, and Y. Feng (2024). On Binscatter. American Economic Review, 114(5): 1488-1514.\n\n\n\nplot.dbbinsreg for plot customization, dbreg for the underlying regression engine, binsreg for the original implementation.\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# For plot customization, save the model object so you can pass additional args\n# to (tiny)plot.dbbinsreg\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\nplot(bs, theme = \"clean\", main = \"A simple binscatter example\")\n\n\n\n\n\n\n\n# Alternatively: you can also set a global (tiny)plot theme\ntinyplot::tinytheme(\"classic\")\n\n# Piecewise linear, no smoothness\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.2059102  41.73348  42.54238\n2   2.5517241   2  52.31034 0.5770938  51.17680  53.44389\n3   4.8620690   3  66.08621 0.8504127  64.41581  67.75661\n4   7.1724138   4  83.86207 1.9453576  80.04095  87.68319\n5   9.4827586   5 103.18966 2.9650856  97.36556 109.01375\n6  11.8965517   6 126.68966 4.3493139 118.14663 135.23268\n7  14.4482759   7 149.31034 5.7331130 138.04922 160.57147\n8  16.8275862   8 176.46552 6.8938017 162.92454 190.00649\n9  19.1929825   9 205.84211 8.3187460 189.50222 222.18199\n10 20.7894737  10 215.35088 9.0642870 197.54658 233.15517\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Piecewise linear with continuity\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr   cb_lwr    cb_upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554 33.23757  51.03829\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965 33.58293  71.03776\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643 38.53218  93.64024\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302 36.13235 131.59178\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700 37.09528 169.28403\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966 33.34510 220.03421\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691 25.14593 273.47476\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028 27.41446 325.51657\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137 25.99386 385.69035\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843 19.98252 410.71923\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr     cb_lwr    cb_upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436   9.804767  40.77557\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409  22.027418  50.64445\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807  34.597558  64.16596\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354  51.384677  83.37397\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063  67.512911 104.06643\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043  87.108149 133.87893\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390 101.885603 161.31390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803 123.465626 195.50429\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526 143.600200 231.66043\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180 152.340134 244.54754\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)\ntinyplot::tinytheme() # reset plot theme",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbbinsreg"
    ]
  },
  {
    "objectID": "man/plot.dbbinsreg.html",
    "href": "man/plot.dbbinsreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Visualizes binned regression results from dbbinsreg. Plots dots at bin means with optional confidence intervals and/or confidence bands, and optionally overlays a smooth line if computed. Uses tinyplot for rendering but works with both plot() and tinyplot() generics.\n\n\n\n## S3 method for class 'dbbinsreg'\nplot(x, type = NULL, ci = TRUE, cb = TRUE, line = TRUE, ...)\n\n## S3 method for class 'dbbinsreg'\ntinyplot(x, type = NULL, ci = TRUE, cb = TRUE, line = TRUE, ...)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\ntype\n\n\nThe type of plot. If NULL (the default), then the type will be inferred based on the underlying object (e.g, “pointrange” for points with confidence intervals).\n\n\n\n\nci\n\n\nLogical. Show confidence intervals for dots? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Show confidence bands as a ribbon? Default is TRUE if available in the object.\n\n\n\n\nline\n\n\nLogical. Show the line overlay if available? Default is TRUE.\n\n\n\n\n…\n\n\nAdditional arguments passed to , e.g. theme, main, file’, etc.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# For plot customization, save the model object so you can pass additional args\n# to (tiny)plot.dbbinsreg\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\nplot(bs, theme = \"clean\", main = \"A simple binscatter example\")\n\n\n\n\n\n\n\n# Alternatively: you can also set a global (tiny)plot theme\ntinyplot::tinytheme(\"classic\")\n\n# Piecewise linear, no smoothness\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.2059102  41.73348  42.54238\n2   2.5517241   2  52.31034 0.5770938  51.17680  53.44389\n3   4.8620690   3  66.08621 0.8504127  64.41581  67.75661\n4   7.1724138   4  83.86207 1.9453576  80.04095  87.68319\n5   9.4827586   5 103.18966 2.9650856  97.36556 109.01375\n6  11.8965517   6 126.68966 4.3493139 118.14663 135.23268\n7  14.4482759   7 149.31034 5.7331130 138.04922 160.57147\n8  16.8275862   8 176.46552 6.8938017 162.92454 190.00649\n9  19.1929825   9 205.84211 8.3187460 189.50222 222.18199\n10 20.7894737  10 215.35088 9.0642870 197.54658 233.15517\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Piecewise linear with continuity\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr   cb_lwr    cb_upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554 33.23757  51.03829\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965 33.58293  71.03776\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643 38.53218  93.64024\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302 36.13235 131.59178\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700 37.09528 169.28403\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966 33.34510 220.03421\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691 25.14593 273.47476\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028 27.41446 325.51657\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137 25.99386 385.69035\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843 19.98252 410.71923\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr     cb_lwr    cb_upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436   9.804767  40.77557\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409  22.027418  50.64445\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807  34.597558  64.16596\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354  51.384677  83.37397\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063  67.512911 104.06643\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043  87.108149 133.87893\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390 101.885603 161.31390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803 123.465626 195.50429\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526 143.600200 231.66043\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180 152.340134 244.54754\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)\ntinyplot::tinytheme() # reset plot theme",
    "crumbs": [
      "Reference",
      "Base methods",
      "plot.dbbinsreg"
    ]
  },
  {
    "objectID": "man/plot.dbbinsreg.html#plot-method-for-dbbinsreg-objects",
    "href": "man/plot.dbbinsreg.html#plot-method-for-dbbinsreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Visualizes binned regression results from dbbinsreg. Plots dots at bin means with optional confidence intervals and/or confidence bands, and optionally overlays a smooth line if computed. Uses tinyplot for rendering but works with both plot() and tinyplot() generics.\n\n\n\n## S3 method for class 'dbbinsreg'\nplot(x, type = NULL, ci = TRUE, cb = TRUE, line = TRUE, ...)\n\n## S3 method for class 'dbbinsreg'\ntinyplot(x, type = NULL, ci = TRUE, cb = TRUE, line = TRUE, ...)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\ntype\n\n\nThe type of plot. If NULL (the default), then the type will be inferred based on the underlying object (e.g, “pointrange” for points with confidence intervals).\n\n\n\n\nci\n\n\nLogical. Show confidence intervals for dots? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Show confidence bands as a ribbon? Default is TRUE if available in the object.\n\n\n\n\nline\n\n\nLogical. Show the line overlay if available? Default is TRUE.\n\n\n\n\n…\n\n\nAdditional arguments passed to , e.g. theme, main, file’, etc.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# For plot customization, save the model object so you can pass additional args\n# to (tiny)plot.dbbinsreg\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\nplot(bs, theme = \"clean\", main = \"A simple binscatter example\")\n\n\n\n\n\n\n\n# Alternatively: you can also set a global (tiny)plot theme\ntinyplot::tinytheme(\"classic\")\n\n# Piecewise linear, no smoothness\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.2059102  41.73348  42.54238\n2   2.5517241   2  52.31034 0.5770938  51.17680  53.44389\n3   4.8620690   3  66.08621 0.8504127  64.41581  67.75661\n4   7.1724138   4  83.86207 1.9453576  80.04095  87.68319\n5   9.4827586   5 103.18966 2.9650856  97.36556 109.01375\n6  11.8965517   6 126.68966 4.3493139 118.14663 135.23268\n7  14.4482759   7 149.31034 5.7331130 138.04922 160.57147\n8  16.8275862   8 176.46552 6.8938017 162.92454 190.00649\n9  19.1929825   9 205.84211 8.3187460 189.50222 222.18199\n10 20.7894737  10 215.35088 9.0642870 197.54658 233.15517\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Piecewise linear with continuity\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.18552 0.1557102  41.87968  42.49136\n2   2.5517241   2  52.18253 0.4170204  51.36344  53.00163\n3   4.8620690   3  66.14321 0.6639281  64.83915  67.44727\n4   7.1724138   4  84.23575 1.4595820  81.36890  87.10260\n5   9.4827586   5 104.38806 2.6000253  99.28120 109.49492\n6  11.8965517   6 126.04158 2.6563138 120.82416 131.25900\n7  14.4482759   7 150.15277 4.1078954 142.08422 158.22132\n8  16.8275862   8 177.23258 5.2935573 166.83520 187.62995\n9  19.1929825   9 201.83852 6.7460015 188.58831 215.08872\n10 20.7894737  10 216.80015 8.6514357 199.80738 233.79293\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843\n\n$line:\n           x bin      fit\n1  0.0000000   1 41.06000\n2  0.1052632   1 41.48947\n3  0.2105263   1 41.91895\n4  0.3157895   1 42.34842\n5  0.4210526   1 42.77789\n6  0.5263158   1 43.20737\n7  0.6315789   1 43.63684\n8  0.7368421   1 44.06632\n9  0.8421053   1 44.49579\n10 0.9473684   1 44.92526\n... 190 more rows\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit        se       lwr       upr   cb_lwr    cb_upr\n1   0.2758621   1  42.13793 0.4111757  41.33032  42.94554 33.23757  51.03829\n2   2.5517241   2  52.31034 0.8651630  50.61104  54.00965 33.58293  71.03776\n3   4.8620690   3  66.08621 1.2729317  63.58598  68.58643 38.53218  93.64024\n4   7.1724138   4  83.86207 2.2050012  79.53112  88.19302 36.13235 131.59178\n5   9.4827586   5 103.18966 3.0534057  97.19231 109.18700 37.09528 169.28403\n6  11.8965517   6 126.68966 4.3122999 118.21965 135.15966 33.34510 220.03421\n7  14.4482759   7 149.31034 5.7361057 138.04378 160.57691 25.14593 273.47476\n8  16.8275862   8 176.46552 6.8858102 162.94076 189.99028 27.41446 325.51657\n9  19.1929825   9 205.84211 8.3085682 189.52284 222.16137 25.99386 385.69035\n10 20.7894737  10 215.35088 9.0255610 197.62333 233.07843 19.98252 410.71923\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr     cb_lwr    cb_upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436   9.804767  40.77557\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409  22.027418  50.64445\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807  34.597558  64.16596\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354  51.384677  83.37397\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063  67.512911 104.06643\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043  87.108149 133.87893\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390 101.885603 161.31390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803 123.465626 195.50429\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526 143.600200 231.66043\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180 152.340134 244.54754\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10)\n\n\n\n\n\n\n\n\nDatabase binned regression (binsreg-compatible)\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 | binspos = 'qs'\nN = 578 | level = 95%\n\n$points:\n            x bin       fit       se       lwr       upr\n1   0.2758621   1  25.29017 2.787023  19.81597  30.76436\n2   2.5517241   2  36.33593 2.575210  31.27778  41.39409\n3   4.8620690   3  49.38176 2.660823  44.15545  54.60807\n4   7.1724138   4  67.37932 2.878676  61.72511  73.03354\n5   9.4827586   5  85.78967 3.289406  79.32872  92.25063\n6  11.8965517   6 110.49354 4.208844 102.22665 118.76043\n7  14.4482759   7 131.59975 5.347878 121.09560 142.10390\n8  16.8275862   8 159.48496 6.482669 146.75189 172.21803\n9  19.1929825   9 187.63032 7.924430 172.06538 203.19526\n10 20.7894737  10 198.44384 8.297629 182.14587 214.74180\n\n$bins:\n   id left right\n1   1    0     2\n2   2    2     4\n3   3    4     6\n4   4    6     8\n5   5    8    10\n6   6   10    14\n7   7   14    16\n8   8   16    18\n9   9   18    20\n10 10   20    21\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)\ntinyplot::tinytheme() # reset plot theme",
    "crumbs": [
      "Reference",
      "Base methods",
      "plot.dbbinsreg"
    ]
  },
  {
    "objectID": "man/print.dbbinsreg.html",
    "href": "man/print.dbbinsreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbbinsreg objects (binsreg-compatible format)\n\n\n\n## S3 method for class 'dbbinsreg'\nprint(x, ...)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\n…\n\n\nAdditional arguments passed to print"
  },
  {
    "objectID": "man/print.dbbinsreg.html#print-method-for-dbbinsreg-objects-binsreg-compatible-format",
    "href": "man/print.dbbinsreg.html#print-method-for-dbbinsreg-objects-binsreg-compatible-format",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbbinsreg objects (binsreg-compatible format)\n\n\n\n## S3 method for class 'dbbinsreg'\nprint(x, ...)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\n…\n\n\nAdditional arguments passed to print"
  },
  {
    "objectID": "man/dbreg.html",
    "href": "man/dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Leverages the power of databases to run regressions on very large datasets, which may not fit into R’s memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics.\n\n\n\ndbreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  vcov = c(\"iid\", \"hc1\"),\n  strategy = c(\"auto\", \"compress\", \"moments\", \"demean\", \"within\", \"mundlak\"),\n  compress_ratio = NULL,\n  compress_nmax = 1e+06,\n  cluster = NULL,\n  ssc = c(\"full\", \"nested\"),\n  sql_only = FALSE,\n  data_only = FALSE,\n  drop_missings = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the relation to be estimated. Fixed effects should be included after a pipe, e.g fml = y ~ x1 + x2 | fe1 + f2. Currently, only simple additive terms are supported (i.e., no interaction terms, transformations or literals).\n\n\n\n\nconn\n\n\nDatabase connection, e.g. created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g. “mydata/**/.*parquet”. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g. “read_parquet(’mydata/**/*.parquet’)“ for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\nvcov\n\n\nCharacter string or formula denoting the desired type of variance- covariance correction / standard errors. Options are “iid” (default), “hc1” (heteroskedasticity-consistent), or a one-sided formula like ~cluster_var for cluster-robust standard errors. Note that “hc1” and clustered SEs require a second pass over the data unless strategy = “compress” to construct the residuals.\n\n\n\n\nstrategy\n\n\nCharacter string indicating the preferred acceleration strategy. The default “auto” will pick an optimal strategy based on internal heuristics. Users can also override with one of the following explicit strategies: “compress”, “demean” (alias: “within”), “mundlak”, or “moments”. See the Acceleration Strategies section below for details.\n\n\n\n\ncompress_ratio, compress_nmax\n\n\nNumeric(s). Parameters that help to determine the acceleration strategy under the default “auto” option.\n\n\ncompress_ratio defines the compression ratio threshold, i.e. numeric in the range [0,1] defining the minimum acceptable compressed versus the original data size. Default value of NULL means that the threshold will be automatically determined based on some internal heuristic (e.g., 0.01 for models without fixed effects).\n\n\ncompress_nmax defines the maximum allowable size (in rows) of the compressed dataset that can be serialized into R. Pays heed to the idea that big data serialization can be costly (esp. for remote databases), even if we have achieved good compression on top of the original dataset. Default value is 1e6 (i.e., a million rows).\n\n\nSee the Acceleration Strategies section below for further details.\n\n\n\n\ncluster\n\n\nOptional. Provides an alternative way to specify cluster-robust standard errors (i.e., instead of vcov = ~cluster_var). Either a one-sided formula (e.g., ~firm) or character string giving the variable name. Only single-variable clustering is currently supported.\n\n\n\n\nssc\n\n\nCharacter string controlling the small-sample correction for clustered standard errors. Options are “full” (default) or “nested”. With “full”, all parameters (including fixed effect dummies) are counted in K for the CR1 correction. With “nested”, fixed effects that are nested within the cluster variable are excluded from K, matching the default behavior of fixest::feols. Only applies to “compress” and “demean” strategies (Mundlak uses explicit group mean regressors, not FE dummies). This distinction only matters for small samples. For large datasets (dbreg’s target use case), the difference is negligible and hence we default to the simple “full” option.\n\n\n\n\nsql_only\n\n\nLogical indicating whether only the underlying compression SQL query should be returned (i.e., no computation will be performed). Default is FALSE.\n\n\n\n\ndata_only\n\n\nLogical indicating whether only the compressed dataset should be returned (i.e., no regression is run). Default is FALSE.\n\n\n\n\ndrop_missings\n\n\nLogical indicating whether incomplete cases (i.e., rows where any of the dependent, independent or FE variables are missing) should be dropped. The default is TRUE, according with standard regression software. It is strongly recommended not to change this value unless you are absolutely sure that your data have no missings and you wish to skip some internal checks. (Even then, it probably isn’t worth it.)\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\n…\n\n\nAdditional arguments. Currently ignored, except to handle superseded arguments for backwards compatibility.\n\n\n\n\n\n\nA list of class \"dbreg\" containing various slots, including a table of coefficients (which the associated print method will display).\n\n\n\ndbreg offers four primary acceleration strategies for estimating regression results from simplified data representations. Below we use the shorthand Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition purposes:\n\n\n“compress”: compresses the data via a GROUP BY operation (using X and the FE as groups), before running weighted least squares on this much smaller dataset:\n\n\\(\\hat{\\beta} = (X_c' W X_c)^{-1} X_c' W Y_c\\)\nwhere \\(W = \\text{diag}(n_g)\\) are the group frequencies. This procedure follows Wang et al. (2021).\n\n\n“moments”: computes sufficient statistics (\\(X'X, X'y\\)) directly via SQL aggregation, returning a single-row result. This solves the standard OLS normal equations \\(\\hat{\\beta} = (X'X)^{-1}X'y\\). Limited to cases without FE.\n\n\n“demean” (alias “within”): subtracts group-level means from both Y and X before computing sufficient statistics (per the “moments” strategy). For example, given unit \\(i\\) and time \\(t\\) FE, we apply double demeaning:\n\n\\(\\ddot{Y}_{it} = \\beta \\ddot{X}_{it} + \\varepsilon_{it}\\)\nwhere \\(\\ddot{X} = X - \\bar{X}_i - \\bar{X}_t + \\bar{X}\\). This (single-pass) within transformation is algebraically equivalent to the fixed effects projection—i.e., Frisch-Waugh-Lovell partialling out—in the presence of a single FE. It is also identical for the two-way FE (TWFE) case if your panel is balanced. For unbalanced two-way panels, however, the double demeaning strategy is not algebraically equivalent to the fixed effects projection and therefore does not recover the exact TWFE coefficients. Moreover, note that this “demean” strategy permits at most two FE.\n\n\n“mundlak”: a generalized Mundlak (1978), or correlated random effects (CRE) estimator that regresses Y on X plus group means of X:\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_i + \\varepsilon_{it} \\quad \\text{(one-way)}\\)\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_{i} + \\delta \\bar{X}_{t} + \\varepsilon_{it} \\quad \\text{(two-way, etc.)}\\)\nUnlike “demean”, Y is not transformed, so predictions are on the original scale. Supports any number of FE and works correctly for any panel structure (balanced or unbalanced). However, note that CRE is a different model from FE: while coefficients are asymptotically equivalent under certain assumptions, they will generally differ in finite samples.\n\n\nThe relative efficiency of each of these strategies depends on the size and structure of the data, as well the number of unique regressors and FE. For (quote unquote) \"standard\" cases, the “compress” strategy can yield remarkable performance gains and should justifiably be viewed as a good default. However, the compression approach tends to be less efficient for true panels (repeated cross-sections over time), where N &gt;&gt; T. In such cases, it can be more efficient to use a demeaning strategy that first controls for (e.g. subtracts) group means, before computing sufficient statistics on the aggregated data. The reason for this is that time and unit FE are typically high dimensional, but covariate averages are not; see Arkhangelsky & Imbens (2024).\nHowever, the demeaning approaches invite tradeoffs of their own. For example, the double demeaning transformation of the “demean” strategy does not obtain exact TWFE results in unbalanced panels, and it is also limited to at most two FE. Conversely, the “mundlak” (CRE) strategy obtains consistent coefficients regardless of panel structure and FE count, but at the \"cost\" of recovering a different estimand. (It is a different model to TWFE, after all.) See Wooldridge (2025) for an extended discussion of these issues.\nUsers should weigh these tradeoffs when choosing their acceleration strategy. Summarising, we can provide a few guiding principles. “compress” is a good default that guarantees the \"exact\" FE estimates and is usually very efficient (barring data I/O costs and high FE dimensionality). “mundlak” is another efficient alternative provided that the CRE estimand is acceptable (don’t be alarmed if your coefficients are not identical). Finally, the “demean” and “moments” strategies are great for particular use cases (i.e., balanced panels and cases without FE, respectively).\nIf this all sounds like too much to think about, don’t fret. The good news is that dbreg can do a lot (all?) of the deciding for you. Specifically, it will invoke an “auto” heuristic behind the scenes if a user does not provide an explicit acceleration strategy. Working through the heuristic logic does impose some additional overhead, but this should be negligible in most cases (certainly compared to the overall time savings). The “auto” heuristic is as follows:\n\n\nIF no FE AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN “moments”.\n\n\nELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN “demean”.\n\n\nELSE IF 2 FE AND (poor compression ratio OR too big compressed data):\n\n\nIF balanced panel THEN “demean”.\n\n\nELSE error (exact TWFE infeasible; user must explicitly choose “compress” or “mundlak”).\n\n\n\n\nELSE THEN “compress”.\n\n\nTip: set dbreg(…, verbose = TRUE) to print information about the auto strategy decision criteria.\n\n\n\nArkhangelsky, D. & Imbens, G. (2024) Fixed Effects and the Generalized Mundlak Estimator. The Review of Economic Studies, 91(5), pp. 2545–2571. Available: https://doi.org/10.1093/restud/rdad089\nMundlak, Y. (1978) On the Pooling of Time Series and Cross Section Data. Econometrica, 46(1), pp. 69–85. Available: https://doi.org/10.2307/1913646\nWong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021). You Only Compress Once: Optimal Data Compression for Estimating Linear Models. arXiv preprint arXiv:2102.11297. Available: https://doi.org/10.48550/arXiv.2102.11297\nWooldridge, J.M. (2025) Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators. Empirical Economics, 69, pp. 2545–2587. Available: https://doi.org/10.1007/s00181-025-02807-z\n\n\n\ndbConnect for creating database connections, duckdb for DuckDB-specific connections\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# We can pass in-memory R data frames to an ephemeral DuckDB connection via\n# the `data` argument. This is convenient for small(er) datasets and demos.\n\n# Default \"compress\" strategy reduces the data to 4 rows before running OLS\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Compare with lm\nsummary(lm(weight ~ Diet, data = ChickWeight))$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 102.64545   4.673954 21.961161 4.712762e-78\nDiet2        19.97121   7.867437  2.538465 1.139708e-02\nDiet3        40.30455   7.867437  5.122958 4.113938e-07\nDiet4        32.61726   7.910461  4.123307 4.286352e-05\n\n# Add \"fixed effects\" after a `|` \ndbreg(weight ~ Time | Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# \"robust\" SEs can also be computed using a sufficient statistics approach\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = \"hc1\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Heteroskedasticity-robust \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.261676 33.4402 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = ~Chick)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Clustered (50 clusters) \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.527463 16.5898 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Different acceleration strategies + specifications\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"demean\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.729032\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"mundlak\")\n\nOne-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049    0.22765 38.4383 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729827\n\ndbreg(weight ~ Time | Diet + Chick, data = ChickWeight, strategy = \"mundlak\") # two-way Mundlak\n\nTwo-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.71519   0.229744 37.9344 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729953\n\ndbreg(weight ~ Time, data = ChickWeight, strategy = \"moments\") # no FEs\n\nMoments-based OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 27.46743   3.036464  9.04586 &lt; 2.2e-16 ***\nTime         8.80304   0.239700 36.72524 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 38.8                Adj. R2: 0.700220\n\n# etc.\n\n#\n## DBI connection ----\n\n# For persistent databases or more control, use the `conn` + `table` args.\n# Again, we use DuckDB below but any other DBI-supported backend should work\n# too (e.g., odbc, bigrquery, noctua (AWS Athena),  etc.) See:\n# https://r-dbi.org/backends/\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbreg(weight ~ Time | Diet, conn = con, table = \"cw\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Tip: Rather than creating or writing (temp) tables, use CREATE VIEW to\n# define subsets or computed columns without materializing data. This is more\n# efficient and especially useful for filtering or adding variables.\ndbExecute(\n  con,\n  \"\n  CREATE VIEW cw1 AS\n  SELECT *\n  FROM cw\n  WHERE Diet = 1\n  \"\n)\n\n[1] 0\n\ndbreg(weight ~ Time | Chick, conn = con, table = \"cw1\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 220 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  6.69062   0.246299 27.1646 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 22.9                Adj. R2: 0.766255\n\n#\n## Path to file ----\n#\n# For file-based data (e.g., parquet), use the path argument.\n\ntmp = tempfile(fileext = \".parquet\")\ndbExecute(con, sprintf(\"COPY cw TO '%s' (FORMAT PARQUET)\", tmp))\n\n[1] 578\n\ndbreg(weight ~ Time | Diet, path = tmp)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Cleanup\ndbDisconnect(con)\nunlink(tmp)\n\n#\n## Big dataset ----\n\n# For a more compelling and appropriate dbreg use-case, i.e. regression on a\n# big (~180 million row) dataset of Hive-partioned parquet files, see the\n# package website:\n# https://grantmcdermott.com/dbreg/",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbreg"
    ]
  },
  {
    "objectID": "man/dbreg.html#run-a-regression-on-a-database-backend",
    "href": "man/dbreg.html#run-a-regression-on-a-database-backend",
    "title": "dbreg",
    "section": "",
    "text": "Leverages the power of databases to run regressions on very large datasets, which may not fit into R’s memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics.\n\n\n\ndbreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  vcov = c(\"iid\", \"hc1\"),\n  strategy = c(\"auto\", \"compress\", \"moments\", \"demean\", \"within\", \"mundlak\"),\n  compress_ratio = NULL,\n  compress_nmax = 1e+06,\n  cluster = NULL,\n  ssc = c(\"full\", \"nested\"),\n  sql_only = FALSE,\n  data_only = FALSE,\n  drop_missings = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the relation to be estimated. Fixed effects should be included after a pipe, e.g fml = y ~ x1 + x2 | fe1 + f2. Currently, only simple additive terms are supported (i.e., no interaction terms, transformations or literals).\n\n\n\n\nconn\n\n\nDatabase connection, e.g. created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g. “mydata/**/.*parquet”. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g. “read_parquet(’mydata/**/*.parquet’)“ for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\nvcov\n\n\nCharacter string or formula denoting the desired type of variance- covariance correction / standard errors. Options are “iid” (default), “hc1” (heteroskedasticity-consistent), or a one-sided formula like ~cluster_var for cluster-robust standard errors. Note that “hc1” and clustered SEs require a second pass over the data unless strategy = “compress” to construct the residuals.\n\n\n\n\nstrategy\n\n\nCharacter string indicating the preferred acceleration strategy. The default “auto” will pick an optimal strategy based on internal heuristics. Users can also override with one of the following explicit strategies: “compress”, “demean” (alias: “within”), “mundlak”, or “moments”. See the Acceleration Strategies section below for details.\n\n\n\n\ncompress_ratio, compress_nmax\n\n\nNumeric(s). Parameters that help to determine the acceleration strategy under the default “auto” option.\n\n\ncompress_ratio defines the compression ratio threshold, i.e. numeric in the range [0,1] defining the minimum acceptable compressed versus the original data size. Default value of NULL means that the threshold will be automatically determined based on some internal heuristic (e.g., 0.01 for models without fixed effects).\n\n\ncompress_nmax defines the maximum allowable size (in rows) of the compressed dataset that can be serialized into R. Pays heed to the idea that big data serialization can be costly (esp. for remote databases), even if we have achieved good compression on top of the original dataset. Default value is 1e6 (i.e., a million rows).\n\n\nSee the Acceleration Strategies section below for further details.\n\n\n\n\ncluster\n\n\nOptional. Provides an alternative way to specify cluster-robust standard errors (i.e., instead of vcov = ~cluster_var). Either a one-sided formula (e.g., ~firm) or character string giving the variable name. Only single-variable clustering is currently supported.\n\n\n\n\nssc\n\n\nCharacter string controlling the small-sample correction for clustered standard errors. Options are “full” (default) or “nested”. With “full”, all parameters (including fixed effect dummies) are counted in K for the CR1 correction. With “nested”, fixed effects that are nested within the cluster variable are excluded from K, matching the default behavior of fixest::feols. Only applies to “compress” and “demean” strategies (Mundlak uses explicit group mean regressors, not FE dummies). This distinction only matters for small samples. For large datasets (dbreg’s target use case), the difference is negligible and hence we default to the simple “full” option.\n\n\n\n\nsql_only\n\n\nLogical indicating whether only the underlying compression SQL query should be returned (i.e., no computation will be performed). Default is FALSE.\n\n\n\n\ndata_only\n\n\nLogical indicating whether only the compressed dataset should be returned (i.e., no regression is run). Default is FALSE.\n\n\n\n\ndrop_missings\n\n\nLogical indicating whether incomplete cases (i.e., rows where any of the dependent, independent or FE variables are missing) should be dropped. The default is TRUE, according with standard regression software. It is strongly recommended not to change this value unless you are absolutely sure that your data have no missings and you wish to skip some internal checks. (Even then, it probably isn’t worth it.)\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\n…\n\n\nAdditional arguments. Currently ignored, except to handle superseded arguments for backwards compatibility.\n\n\n\n\n\n\nA list of class \"dbreg\" containing various slots, including a table of coefficients (which the associated print method will display).\n\n\n\ndbreg offers four primary acceleration strategies for estimating regression results from simplified data representations. Below we use the shorthand Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition purposes:\n\n\n“compress”: compresses the data via a GROUP BY operation (using X and the FE as groups), before running weighted least squares on this much smaller dataset:\n\n\\(\\hat{\\beta} = (X_c' W X_c)^{-1} X_c' W Y_c\\)\nwhere \\(W = \\text{diag}(n_g)\\) are the group frequencies. This procedure follows Wang et al. (2021).\n\n\n“moments”: computes sufficient statistics (\\(X'X, X'y\\)) directly via SQL aggregation, returning a single-row result. This solves the standard OLS normal equations \\(\\hat{\\beta} = (X'X)^{-1}X'y\\). Limited to cases without FE.\n\n\n“demean” (alias “within”): subtracts group-level means from both Y and X before computing sufficient statistics (per the “moments” strategy). For example, given unit \\(i\\) and time \\(t\\) FE, we apply double demeaning:\n\n\\(\\ddot{Y}_{it} = \\beta \\ddot{X}_{it} + \\varepsilon_{it}\\)\nwhere \\(\\ddot{X} = X - \\bar{X}_i - \\bar{X}_t + \\bar{X}\\). This (single-pass) within transformation is algebraically equivalent to the fixed effects projection—i.e., Frisch-Waugh-Lovell partialling out—in the presence of a single FE. It is also identical for the two-way FE (TWFE) case if your panel is balanced. For unbalanced two-way panels, however, the double demeaning strategy is not algebraically equivalent to the fixed effects projection and therefore does not recover the exact TWFE coefficients. Moreover, note that this “demean” strategy permits at most two FE.\n\n\n“mundlak”: a generalized Mundlak (1978), or correlated random effects (CRE) estimator that regresses Y on X plus group means of X:\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_i + \\varepsilon_{it} \\quad \\text{(one-way)}\\)\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_{i} + \\delta \\bar{X}_{t} + \\varepsilon_{it} \\quad \\text{(two-way, etc.)}\\)\nUnlike “demean”, Y is not transformed, so predictions are on the original scale. Supports any number of FE and works correctly for any panel structure (balanced or unbalanced). However, note that CRE is a different model from FE: while coefficients are asymptotically equivalent under certain assumptions, they will generally differ in finite samples.\n\n\nThe relative efficiency of each of these strategies depends on the size and structure of the data, as well the number of unique regressors and FE. For (quote unquote) \"standard\" cases, the “compress” strategy can yield remarkable performance gains and should justifiably be viewed as a good default. However, the compression approach tends to be less efficient for true panels (repeated cross-sections over time), where N &gt;&gt; T. In such cases, it can be more efficient to use a demeaning strategy that first controls for (e.g. subtracts) group means, before computing sufficient statistics on the aggregated data. The reason for this is that time and unit FE are typically high dimensional, but covariate averages are not; see Arkhangelsky & Imbens (2024).\nHowever, the demeaning approaches invite tradeoffs of their own. For example, the double demeaning transformation of the “demean” strategy does not obtain exact TWFE results in unbalanced panels, and it is also limited to at most two FE. Conversely, the “mundlak” (CRE) strategy obtains consistent coefficients regardless of panel structure and FE count, but at the \"cost\" of recovering a different estimand. (It is a different model to TWFE, after all.) See Wooldridge (2025) for an extended discussion of these issues.\nUsers should weigh these tradeoffs when choosing their acceleration strategy. Summarising, we can provide a few guiding principles. “compress” is a good default that guarantees the \"exact\" FE estimates and is usually very efficient (barring data I/O costs and high FE dimensionality). “mundlak” is another efficient alternative provided that the CRE estimand is acceptable (don’t be alarmed if your coefficients are not identical). Finally, the “demean” and “moments” strategies are great for particular use cases (i.e., balanced panels and cases without FE, respectively).\nIf this all sounds like too much to think about, don’t fret. The good news is that dbreg can do a lot (all?) of the deciding for you. Specifically, it will invoke an “auto” heuristic behind the scenes if a user does not provide an explicit acceleration strategy. Working through the heuristic logic does impose some additional overhead, but this should be negligible in most cases (certainly compared to the overall time savings). The “auto” heuristic is as follows:\n\n\nIF no FE AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN “moments”.\n\n\nELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN “demean”.\n\n\nELSE IF 2 FE AND (poor compression ratio OR too big compressed data):\n\n\nIF balanced panel THEN “demean”.\n\n\nELSE error (exact TWFE infeasible; user must explicitly choose “compress” or “mundlak”).\n\n\n\n\nELSE THEN “compress”.\n\n\nTip: set dbreg(…, verbose = TRUE) to print information about the auto strategy decision criteria.\n\n\n\nArkhangelsky, D. & Imbens, G. (2024) Fixed Effects and the Generalized Mundlak Estimator. The Review of Economic Studies, 91(5), pp. 2545–2571. Available: https://doi.org/10.1093/restud/rdad089\nMundlak, Y. (1978) On the Pooling of Time Series and Cross Section Data. Econometrica, 46(1), pp. 69–85. Available: https://doi.org/10.2307/1913646\nWong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021). You Only Compress Once: Optimal Data Compression for Estimating Linear Models. arXiv preprint arXiv:2102.11297. Available: https://doi.org/10.48550/arXiv.2102.11297\nWooldridge, J.M. (2025) Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators. Empirical Economics, 69, pp. 2545–2587. Available: https://doi.org/10.1007/s00181-025-02807-z\n\n\n\ndbConnect for creating database connections, duckdb for DuckDB-specific connections\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# We can pass in-memory R data frames to an ephemeral DuckDB connection via\n# the `data` argument. This is convenient for small(er) datasets and demos.\n\n# Default \"compress\" strategy reduces the data to 4 rows before running OLS\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Compare with lm\nsummary(lm(weight ~ Diet, data = ChickWeight))$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 102.64545   4.673954 21.961161 4.712762e-78\nDiet2        19.97121   7.867437  2.538465 1.139708e-02\nDiet3        40.30455   7.867437  5.122958 4.113938e-07\nDiet4        32.61726   7.910461  4.123307 4.286352e-05\n\n# Add \"fixed effects\" after a `|` \ndbreg(weight ~ Time | Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# \"robust\" SEs can also be computed using a sufficient statistics approach\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = \"hc1\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Heteroskedasticity-robust \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.261676 33.4402 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = ~Chick)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Clustered (50 clusters) \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.527463 16.5898 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Different acceleration strategies + specifications\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"demean\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.729032\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"mundlak\")\n\nOne-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049    0.22765 38.4383 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729827\n\ndbreg(weight ~ Time | Diet + Chick, data = ChickWeight, strategy = \"mundlak\") # two-way Mundlak\n\nTwo-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.71519   0.229744 37.9344 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729953\n\ndbreg(weight ~ Time, data = ChickWeight, strategy = \"moments\") # no FEs\n\nMoments-based OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 27.46743   3.036464  9.04586 &lt; 2.2e-16 ***\nTime         8.80304   0.239700 36.72524 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 38.8                Adj. R2: 0.700220\n\n# etc.\n\n#\n## DBI connection ----\n\n# For persistent databases or more control, use the `conn` + `table` args.\n# Again, we use DuckDB below but any other DBI-supported backend should work\n# too (e.g., odbc, bigrquery, noctua (AWS Athena),  etc.) See:\n# https://r-dbi.org/backends/\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbreg(weight ~ Time | Diet, conn = con, table = \"cw\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Tip: Rather than creating or writing (temp) tables, use CREATE VIEW to\n# define subsets or computed columns without materializing data. This is more\n# efficient and especially useful for filtering or adding variables.\ndbExecute(\n  con,\n  \"\n  CREATE VIEW cw1 AS\n  SELECT *\n  FROM cw\n  WHERE Diet = 1\n  \"\n)\n\n[1] 0\n\ndbreg(weight ~ Time | Chick, conn = con, table = \"cw1\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 220 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  6.69062   0.246299 27.1646 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 22.9                Adj. R2: 0.766255\n\n#\n## Path to file ----\n#\n# For file-based data (e.g., parquet), use the path argument.\n\ntmp = tempfile(fileext = \".parquet\")\ndbExecute(con, sprintf(\"COPY cw TO '%s' (FORMAT PARQUET)\", tmp))\n\n[1] 578\n\ndbreg(weight ~ Time | Diet, path = tmp)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Cleanup\ndbDisconnect(con)\nunlink(tmp)\n\n#\n## Big dataset ----\n\n# For a more compelling and appropriate dbreg use-case, i.e. regression on a\n# big (~180 million row) dataset of Hive-partioned parquet files, see the\n# package website:\n# https://grantmcdermott.com/dbreg/",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbreg"
    ]
  },
  {
    "objectID": "man/gof.html",
    "href": "man/gof.html",
    "title": "dbreg",
    "section": "",
    "text": "Calculate goodness-of-fit metrics for dbreg objects\n\n\n\ngof(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\n…\n\n\nAdditional arguments (currently unused)\n\n\n\n\n\n\nNamed vector with r2, adj_r2, and rmse\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ngof(mod)\n\n       r2    adj_r2      rmse \n0.5884105 0.5744109 6.0525892",
    "crumbs": [
      "Reference",
      "utilities",
      "gof"
    ]
  },
  {
    "objectID": "man/gof.html#calculate-goodness-of-fit-metrics-for-dbreg-objects",
    "href": "man/gof.html#calculate-goodness-of-fit-metrics-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Calculate goodness-of-fit metrics for dbreg objects\n\n\n\ngof(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‘dbreg’ object.\n\n\n\n\n…\n\n\nAdditional arguments (currently unused)\n\n\n\n\n\n\nNamed vector with r2, adj_r2, and rmse\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ngof(mod)\n\n       r2    adj_r2      rmse \n0.5884105 0.5744109 6.0525892",
    "crumbs": [
      "Reference",
      "utilities",
      "gof"
    ]
  },
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite package ‘dbreg’ in publications use:\n\nMcDermott G, Brand J (2025). dbreg: Fast Regressions on Database Backends. R package version 0.0.2.99, https://grantmcdermott.com/dbreg/."
  }
]
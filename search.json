[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dbreg",
    "section": "",
    "text": "Fast regressions on database backends.\n\n\ndbreg is an R package that leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics. Our default DuckDB backend provides a powerful, embedded analytics engine to get users up and running with minimal effort. Users can also specify alternative database backends, depending on their computing needs and setup.\nThe dbreg R package is inspired by, and has similar aims to, the duckreg Python package. This implementation offers some idiomatic, R-focused features like a formula interface and ‚Äúpretty‚Äù print methods. But our long-term goal is that these two packages should be aligned in terms of core feature parity.\n\n\n\ndbreg can be installed from R-universe.\ninstall.packages(\n   \"dbreg\",\n   repos = c(\"https://grantmcdermott.r-universe.dev\", getOption(\"repos\"))\n)\n\n\n\n\n\nTo get ourselves situated, we‚Äôll first demonstrate by using an in-memory R dataset.\nlibrary(dbreg)\nlibrary(fixest) # for data and comparison\n\ndata(\"trade\", package = \"fixest\")\n\ndbreg(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; Compressed OLS estimation, Dep. Var.: Euros \n#&gt; Observations.: 38,325 (original) | 210 (compressed) \n#&gt; Standard-errors: Heteroskedasticity-robust\n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3         Adj. R2: 0.215289\nBehind the scenes, dbreg has compressed the original dataset down from nearly 40,000 observations to only 210, before running the final (weighted) regression on this much smaller data object. This compression procedure trick follows Wang _et. al.¬†(2021) and effectively allows us to compute on a much lighter object, saving time and memory. We can confirm that it still gives the same result as running fixest::feols on the full dataset:\nfeols(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; OLS estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325\n#&gt; Fixed-effects: Destination: 15,  Origin: 15\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3     Adj. R2: 0.215289\n#&gt;                       Within R2: 0.025914\n\n\n\nFor a more appropriate dbreg use-case, let‚Äôs run a regression on some NYC taxi data. (Download instructions here.) The dataset that we‚Äôre working with here is about 180 million rows deep and takes up 8.5 GB on disk.1 dbreg offers two basic ways to analyse and interact with data of this size.\n\n\nUse the path argument to read the data directly from disk and perform the compression computation in an ephemeral DuckDB connection. This requires that the data are small enough to fit into RAM‚Ä¶ but please note that ‚Äúsmall enough‚Äù is a relative concept. Thanks to DuckDB‚Äôs incredible efficiency, your RAM should be able to handle very large datasets that would otherwise crash your R session, and require only a fraction of the computation time. Note that we also invoke the (optional) verbose  = TRUE argument to print additional information about the estimation strategy.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\", ## path to hive-partitioned dataset\n   vcov = \"hc1\",\n   verbose = TRUE ## optional (print info about the estimation strategy)\n)\n#&gt; [dbreg] Auto strategy:\n#&gt;         - data has 178,544,324 rows with 2 FE (24 unique groups)\n#&gt;         - compression ratio (0.00) satisfies threshold (0.6)\n#&gt;         - decision: compress\n#&gt; [dbreg] Executing compress strategy SQL\n#&gt; \n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Heteroskedasticity-robust\n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000068 1564.742 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.000106 -273.866 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nNote the size of the original dataset, which is nearly 180 million rows, versus the compressed dataset, which is down to only 70k. On my laptop (M4 MacBook Pro) this regression completes in under 3 seconds‚Ä¶ and that includes the time it took to determine an optimal estimation strategy, as well as read the data from disk!2\nIn case you were wondering, obtaining clustered standard errors is just as easy; simply pass the relevant cluster variable as a formula to the vcov argument. Since we know that the optimal acceleration strategy is \"compress\", we‚Äôll also go ahead a specify this explicitly to skip the auto strategy overhead.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path     = \"read_parquet('nyc-taxi/**/*.parquet')\",\n   vcov     = ~month,    # clustered SEs\n   strategy = \"compress\" # skip auto strategy overhead\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\n\n\n\nWhile querying on-the-fly with our default DuckDB backend is both convenient and extremely performant, you can also run regressions against existing tables in a persistent database connection. This could be DuckDB, but it could also be any other supported backend. All you need to do is specify the appropriate conn and table arguments.\n# load the DBI package to connect to a persistent database\nlibrary(DBI)\n\n# create connection to persistent DuckDB database (could be any supported backend)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"nyc.db\")\n\n# create a 'taxi' table in our new nyc.db database from our parquet dataset\ndbExecute(\n   con,\n   \"\n   CREATE TABLE taxi AS\n      FROM read_parquet('nyc-taxi/**/*.parquet')\n      SELECT tip_amount, fare_amount, passenger_count, month, vendor_name\n   \"\n)\n\n# now run our regression against this conn+table combo\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   conn = con,     # database connection,\n   table = \"taxi\", # table name\n   vcov = ~month,\n   strategy = \"compress\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nResult: we get the same coefficient and standard error estimates as earlier.\nWe‚Äôll close by doing some (optional) clean up.\ndbRemoveTable(con, \"taxi\")\ndbDisconnect(con)\nunlink(\"nyc.db\") # remove from disk\n\n[!TIP] If you don‚Äôt want to create a persistent database (and materialize data), a nice alternative is CREATE VIEW. This lets you define subsets or computed columns on-the-fly. For example, to regress on Q1 2012 data with a day-of-week fixed effect:\ndbExecute(con, \"\n   CREATE VIEW nyc_subset AS\n   SELECT\n      tip_amount, trip_distance, passenger_count,\n      vendor_name, month,\n      dayofweek(dropoff_datetime) AS dofw\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   WHERE year = 2012 AND month &lt;= 3\n\")\n\ndbreg(\n   tip_amount ~ trip_distance + passenger_count | month + dofw + vendor_name,\n   conn = con,\n   table = \"nyc_subset\",\n   vcov = ~dofw\n)\n\n\n\n\n\n\nAll of the examples in this README have made use of the \"compress\" strategy. But the compression trick is not the only game in town and dbreg supports several other acceleration strategies: \"moments\", \"demean\", and \"mundlak\". Depending on your data and regression requirements, one of these other strategies may better suit your problem. The good news is that (the default) strategy = \"auto\" option uses some intelligent heuristics to determine which strategy is (probably) optimal for each case. You can set the verbose = TRUE argument to get real-time feedback about the decision criteria being used.\nMoreover, the Acceleration Strategies section of the ?dbreg helpfile contains a lot detail about the different options and tradeoffs involved, so please do consult the documentation.\n\n\n\ndbreg is a maturing package and there are a number of features that we still plan to add before submitting it to CRAN. (See our TO-DO list.) We also don‚Äôt yet support some standard R operations like interaction terms in the formula. At the same time, the core dbreg() routine has been tested pretty thoroughly and should work in standard cases. Please help us by kicking the tyres and creating GitHub issues for both bug reports and feature requests."
  },
  {
    "objectID": "index.html#what",
    "href": "index.html#what",
    "title": "dbreg",
    "section": "",
    "text": "dbreg is an R package that leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics. Our default DuckDB backend provides a powerful, embedded analytics engine to get users up and running with minimal effort. Users can also specify alternative database backends, depending on their computing needs and setup.\nThe dbreg R package is inspired by, and has similar aims to, the duckreg Python package. This implementation offers some idiomatic, R-focused features like a formula interface and ‚Äúpretty‚Äù print methods. But our long-term goal is that these two packages should be aligned in terms of core feature parity."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "dbreg",
    "section": "",
    "text": "dbreg can be installed from R-universe.\ninstall.packages(\n   \"dbreg\",\n   repos = c(\"https://grantmcdermott.r-universe.dev\", getOption(\"repos\"))\n)"
  },
  {
    "objectID": "index.html#quickstart",
    "href": "index.html#quickstart",
    "title": "dbreg",
    "section": "",
    "text": "To get ourselves situated, we‚Äôll first demonstrate by using an in-memory R dataset.\nlibrary(dbreg)\nlibrary(fixest) # for data and comparison\n\ndata(\"trade\", package = \"fixest\")\n\ndbreg(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; Compressed OLS estimation, Dep. Var.: Euros \n#&gt; Observations.: 38,325 (original) | 210 (compressed) \n#&gt; Standard-errors: Heteroskedasticity-robust\n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3         Adj. R2: 0.215289\nBehind the scenes, dbreg has compressed the original dataset down from nearly 40,000 observations to only 210, before running the final (weighted) regression on this much smaller data object. This compression procedure trick follows Wang _et. al.¬†(2021) and effectively allows us to compute on a much lighter object, saving time and memory. We can confirm that it still gives the same result as running fixest::feols on the full dataset:\nfeols(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; OLS estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325\n#&gt; Fixed-effects: Destination: 15,  Origin: 15\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3     Adj. R2: 0.215289\n#&gt;                       Within R2: 0.025914\n\n\n\nFor a more appropriate dbreg use-case, let‚Äôs run a regression on some NYC taxi data. (Download instructions here.) The dataset that we‚Äôre working with here is about 180 million rows deep and takes up 8.5 GB on disk.1 dbreg offers two basic ways to analyse and interact with data of this size.\n\n\nUse the path argument to read the data directly from disk and perform the compression computation in an ephemeral DuckDB connection. This requires that the data are small enough to fit into RAM‚Ä¶ but please note that ‚Äúsmall enough‚Äù is a relative concept. Thanks to DuckDB‚Äôs incredible efficiency, your RAM should be able to handle very large datasets that would otherwise crash your R session, and require only a fraction of the computation time. Note that we also invoke the (optional) verbose  = TRUE argument to print additional information about the estimation strategy.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\", ## path to hive-partitioned dataset\n   vcov = \"hc1\",\n   verbose = TRUE ## optional (print info about the estimation strategy)\n)\n#&gt; [dbreg] Auto strategy:\n#&gt;         - data has 178,544,324 rows with 2 FE (24 unique groups)\n#&gt;         - compression ratio (0.00) satisfies threshold (0.6)\n#&gt;         - decision: compress\n#&gt; [dbreg] Executing compress strategy SQL\n#&gt; \n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Heteroskedasticity-robust\n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000068 1564.742 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.000106 -273.866 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nNote the size of the original dataset, which is nearly 180 million rows, versus the compressed dataset, which is down to only 70k. On my laptop (M4 MacBook Pro) this regression completes in under 3 seconds‚Ä¶ and that includes the time it took to determine an optimal estimation strategy, as well as read the data from disk!2\nIn case you were wondering, obtaining clustered standard errors is just as easy; simply pass the relevant cluster variable as a formula to the vcov argument. Since we know that the optimal acceleration strategy is \"compress\", we‚Äôll also go ahead a specify this explicitly to skip the auto strategy overhead.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path     = \"read_parquet('nyc-taxi/**/*.parquet')\",\n   vcov     = ~month,    # clustered SEs\n   strategy = \"compress\" # skip auto strategy overhead\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed)\n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\n\n\n\nWhile querying on-the-fly with our default DuckDB backend is both convenient and extremely performant, you can also run regressions against existing tables in a persistent database connection. This could be DuckDB, but it could also be any other supported backend. All you need to do is specify the appropriate conn and table arguments.\n# load the DBI package to connect to a persistent database\nlibrary(DBI)\n\n# create connection to persistent DuckDB database (could be any supported backend)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"nyc.db\")\n\n# create a 'taxi' table in our new nyc.db database from our parquet dataset\ndbExecute(\n   con,\n   \"\n   CREATE TABLE taxi AS\n      FROM read_parquet('nyc-taxi/**/*.parquet')\n      SELECT tip_amount, fare_amount, passenger_count, month, vendor_name\n   \"\n)\n\n# now run our regression against this conn+table combo\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   conn = con,     # database connection,\n   table = \"taxi\", # table name\n   vcov = ~month,\n   strategy = \"compress\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters)\n#&gt;                  Estimate Std. Error  t value   Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934  &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 1.2923e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nResult: we get the same coefficient and standard error estimates as earlier.\nWe‚Äôll close by doing some (optional) clean up.\ndbRemoveTable(con, \"taxi\")\ndbDisconnect(con)\nunlink(\"nyc.db\") # remove from disk\n\n[!TIP] If you don‚Äôt want to create a persistent database (and materialize data), a nice alternative is CREATE VIEW. This lets you define subsets or computed columns on-the-fly. For example, to regress on Q1 2012 data with a day-of-week fixed effect:\ndbExecute(con, \"\n   CREATE VIEW nyc_subset AS\n   SELECT\n      tip_amount, trip_distance, passenger_count,\n      vendor_name, month,\n      dayofweek(dropoff_datetime) AS dofw\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   WHERE year = 2012 AND month &lt;= 3\n\")\n\ndbreg(\n   tip_amount ~ trip_distance + passenger_count | month + dofw + vendor_name,\n   conn = con,\n   table = \"nyc_subset\",\n   vcov = ~dofw\n)"
  },
  {
    "objectID": "index.html#acceleration-strategies",
    "href": "index.html#acceleration-strategies",
    "title": "dbreg",
    "section": "",
    "text": "All of the examples in this README have made use of the \"compress\" strategy. But the compression trick is not the only game in town and dbreg supports several other acceleration strategies: \"moments\", \"demean\", and \"mundlak\". Depending on your data and regression requirements, one of these other strategies may better suit your problem. The good news is that (the default) strategy = \"auto\" option uses some intelligent heuristics to determine which strategy is (probably) optimal for each case. You can set the verbose = TRUE argument to get real-time feedback about the decision criteria being used.\nMoreover, the Acceleration Strategies section of the ?dbreg helpfile contains a lot detail about the different options and tradeoffs involved, so please do consult the documentation."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "dbreg",
    "section": "",
    "text": "dbreg is a maturing package and there are a number of features that we still plan to add before submitting it to CRAN. (See our TO-DO list.) We also don‚Äôt yet support some standard R operations like interaction terms in the formula. At the same time, the core dbreg() routine has been tested pretty thoroughly and should work in standard cases. Please help us by kicking the tyres and creating GitHub issues for both bug reports and feature requests."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "dbreg",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this dataset would occupy significantly more RAM than 8.5 GB if we loaded it into R‚Äôs memory, due to data serialization and the switch to richer representation formats (e.g., ordered factors require more memory). So there‚Äôs a good chance that just trying to load this raw dataset into R would cause your whole system to crash‚Ä¶ never mind doing any statistical analysis on it.‚Ü©Ô∏é\nIf we provided an explicit dbreg(..., strategy = \"compress\") argument (thus skipping the automatic strategy determination), then the total computation time drops to less than 1 second‚Ä¶‚Ü©Ô∏é"
  },
  {
    "objectID": "man/tidiers.html",
    "href": "man/tidiers.html",
    "title": "dbreg",
    "section": "",
    "text": "Provides broom::tidy and broom::glance methods for \"dbreg\" objects.\n\n\n\n## S3 method for class 'dbreg'\ntidy(x, conf.int = FALSE, conf.level = 0.95, fe = FALSE, ...)\n\n## S3 method for class 'dbreg'\nglance(x, ...)\n\n\n\n\n\n\n\nx\n\n\na model of class ‚Äòdbreg‚Äô produced by the dbreg function.\n\n\n\n\nconf.int\n\n\nLogical indicating whether to include confidence intervals. Default is ‚ÄòFALSE‚Äô.\n\n\n\n\nconf.level\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be tidied too? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments to tidying method. Currently unused except to handle superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ntidy(mod, conf.int = TRUE)\n\n  term  estimate std.error statistic     p.values  conf.low  conf.high\n1 Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n\ntidy(mod, conf.int = TRUE, fe = TRUE)\n\n         term  estimate std.error statistic     p.values  conf.low  conf.high\n1 (Intercept) 74.188474 2.0544007 36.111978 5.675829e-75 70.128499 78.2484490\n2        Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n3      Month6 12.543643 1.5942530  7.868038 7.184769e-13  9.393027 15.6942587\n4      Month7 16.362079 1.6183409 10.110404 1.431916e-18 13.163860 19.5602984\n5      Month8 16.316286 1.6239233 10.047449 2.091354e-18 13.107035 19.5255376\n6      Month9 10.279216 1.5959361  6.440869 1.590257e-09  7.125274 13.4331579\n\nglance(mod)\n\n  r.squared adj.r.squared     rmse nobs df.residual\n1 0.5884105     0.5744109 6.052589  153         147",
    "crumbs": [
      "Reference",
      "Other methods",
      "glance.dbreg"
    ]
  },
  {
    "objectID": "man/tidiers.html#tidiers-for-dbreg-objects",
    "href": "man/tidiers.html#tidiers-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Provides broom::tidy and broom::glance methods for \"dbreg\" objects.\n\n\n\n## S3 method for class 'dbreg'\ntidy(x, conf.int = FALSE, conf.level = 0.95, fe = FALSE, ...)\n\n## S3 method for class 'dbreg'\nglance(x, ...)\n\n\n\n\n\n\n\nx\n\n\na model of class ‚Äòdbreg‚Äô produced by the dbreg function.\n\n\n\n\nconf.int\n\n\nLogical indicating whether to include confidence intervals. Default is ‚ÄòFALSE‚Äô.\n\n\n\n\nconf.level\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be tidied too? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments to tidying method. Currently unused except to handle superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ntidy(mod, conf.int = TRUE)\n\n  term  estimate std.error statistic     p.values  conf.low  conf.high\n1 Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n\ntidy(mod, conf.int = TRUE, fe = TRUE)\n\n         term  estimate std.error statistic     p.values  conf.low  conf.high\n1 (Intercept) 74.188474 2.0544007 36.111978 5.675829e-75 70.128499 78.2484490\n2        Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n3      Month6 12.543643 1.5942530  7.868038 7.184769e-13  9.393027 15.6942587\n4      Month7 16.362079 1.6183409 10.110404 1.431916e-18 13.163860 19.5602984\n5      Month8 16.316286 1.6239233 10.047449 2.091354e-18 13.107035 19.5255376\n6      Month9 10.279216 1.5959361  6.440869 1.590257e-09  7.125274 13.4331579\n\nglance(mod)\n\n  r.squared adj.r.squared     rmse nobs df.residual\n1 0.5884105     0.5744109 6.052589  153         147",
    "crumbs": [
      "Reference",
      "Other methods",
      "glance.dbreg"
    ]
  },
  {
    "objectID": "man/confint.dbreg.html",
    "href": "man/confint.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Confidence intervals for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nconfint(object, parm, level = 0.95, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nparm\n\n\na specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.\n\n\n\n\nlevel\n\n\nthe confidence level required. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "confint.dbreg"
    ]
  },
  {
    "objectID": "man/confint.dbreg.html#confidence-intervals-for-dbreg-objects",
    "href": "man/confint.dbreg.html#confidence-intervals-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Confidence intervals for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nconfint(object, parm, level = 0.95, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nparm\n\n\na specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.\n\n\n\n\nlevel\n\n\nthe confidence level required. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "confint.dbreg"
    ]
  },
  {
    "objectID": "man/predict.dbreg.html",
    "href": "man/predict.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Predict method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\npredict(\n  object,\n  newdata = NULL,\n  interval = c(\"none\", \"confidence\", \"prediction\"),\n  level = 0.95,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nnewdata\n\n\nData frame for predictions. Required for objects that were estimated using the ‚Äò\"mundlak\"‚Äô and ‚Äò\"moments\"‚Äô strategies, since ‚Äòdbreg‚Äô does not retain any data for these estimations.\n\n\n\n\ninterval\n\n\nType of interval to compute: ‚Äò\"none\"‚Äô (default), ‚Äò\"confidence\"‚Äô, or ‚Äò\"prediction\"‚Äô. Note that ‚Äò\"confidence\"‚Äô intervals reflect uncertainty in the estimated mean, while ‚Äò\"prediction\"‚Äô intervals additionally account for residual variance. See predict.lm for details.\n\n\n\n\nlevel\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\nPredicting on ‚Äòdbreg‚Äô objects should generally work as expected. However, predictions from ‚Äò\"demean\"‚Äô strategy models carry two important caveats:\n\nPredictions require group means to transform back to the original scale. If ‚Äònewdata‚Äô contains the outcome variable, group means are computed from ‚Äònewdata‚Äô and used to return level predictions. If the outcome is absent, within-group predictions (deviations from group means) are returned instead, with a message.\nConfidence/prediction intervals are not supported. A demeaned model cannot account for uncertainty in the fixed-effects (since these were absorbed at estimation time), which in turn would yield intervals that are too narrow. Requesting intervals for ‚Äò\"demean\"‚Äô strategy models will return point predictions with a message. Users should re-estimate with a different strategy if intervals are needed.\n\n\n\n\n[dbreg()] for examples.\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "predict.dbreg"
    ]
  },
  {
    "objectID": "man/predict.dbreg.html#predict-method-for-dbreg-objects",
    "href": "man/predict.dbreg.html#predict-method-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Predict method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\npredict(\n  object,\n  newdata = NULL,\n  interval = c(\"none\", \"confidence\", \"prediction\"),\n  level = 0.95,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nnewdata\n\n\nData frame for predictions. Required for objects that were estimated using the ‚Äò\"mundlak\"‚Äô and ‚Äò\"moments\"‚Äô strategies, since ‚Äòdbreg‚Äô does not retain any data for these estimations.\n\n\n\n\ninterval\n\n\nType of interval to compute: ‚Äò\"none\"‚Äô (default), ‚Äò\"confidence\"‚Äô, or ‚Äò\"prediction\"‚Äô. Note that ‚Äò\"confidence\"‚Äô intervals reflect uncertainty in the estimated mean, while ‚Äò\"prediction\"‚Äô intervals additionally account for residual variance. See predict.lm for details.\n\n\n\n\nlevel\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\nPredicting on ‚Äòdbreg‚Äô objects should generally work as expected. However, predictions from ‚Äò\"demean\"‚Äô strategy models carry two important caveats:\n\nPredictions require group means to transform back to the original scale. If ‚Äònewdata‚Äô contains the outcome variable, group means are computed from ‚Äònewdata‚Äô and used to return level predictions. If the outcome is absent, within-group predictions (deviations from group means) are returned instead, with a message.\nConfidence/prediction intervals are not supported. A demeaned model cannot account for uncertainty in the fixed-effects (since these were absorbed at estimation time), which in turn would yield intervals that are too narrow. Requesting intervals for ‚Äò\"demean\"‚Äô strategy models will return point predictions with a message. Users should re-estimate with a different strategy if intervals are needed.\n\n\n\n\n[dbreg()] for examples.\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "predict.dbreg"
    ]
  },
  {
    "objectID": "man/coef.dbreg.html",
    "href": "man/coef.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Extract coefficients from dbreg objects\n\n\n\n## S3 method for class 'dbreg'\ncoef(object, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "coef.dbreg"
    ]
  },
  {
    "objectID": "man/coef.dbreg.html#extract-coefficients-from-dbreg-objects",
    "href": "man/coef.dbreg.html#extract-coefficients-from-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Extract coefficients from dbreg objects\n\n\n\n## S3 method for class 'dbreg'\ncoef(object, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "coef.dbreg"
    ]
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "News",
    "section": "",
    "text": "This NEWS file is best viewed on our website.\n\n\nWebsite\n\nWe finally have a package website: grantmcdermott.com/dbreg üéâ\n\nBreaking changes\n\nThe behaviour of the \"mundlak\" strategy has been changed. (#24)\n\nThe old \"mundlak\" strategy is remapped to the new \"demean\" (alias \"within\") strategy, to better reflect the fact that this strategy invokes a (double) demeaning transformation. Users who want the old behaviour should thus use \"demean\" instead of \"mundlak\".\nSimultaneously, we now provide a revised \"mundlak\" strategy that implements a ‚Äútrue‚Äù Mundlak/CRE estimator; see New features below.\n\nFor estimations with two fixed effects on unbalanced panels, strategy=\"auto\" now errors when the compression limits are exceeded. It does this to avoid silently selecting a different estimand (i.e., Mundlak/CRE instead of TWFE). For these ambiguous cases, users will now be prompted to explicitly choose \"compress\" (with higher limits) or \"mundlak\" (different model and thus potentially different coefficients). (#24)\nThe default verbose behaviour is changed to FALSE. Users can revert to the old behaviour for a single call (i.e., dbreg(..., verbose = TRUE)), or set it globally (i.e., options(dbreg.verbose = TRUE)). (#33)\nTechnically not a breaking change, since we currently support backwards compatibility, but several minor arguments have been renamed/superseded. (#34)\n\nquery_only -&gt; sql_only (in dbreg)\nfes -&gt; fe (in print.dbreg, coef.dbreg, confint.dbreg, etc.)\n\n\nNew features\n\nWe have added new/revised acceleration strategies. (#24)\n\nThe \"demean\" (alias \"within\") strategy implements a (double) demeaning transformation and is particularly suited to (balanced) panels with one or two fixed effects. Note the underlying query is the same as the old \"mundlak\" strategy, which was somewhat erroneously named. Speaking of which‚Ä¶\nThe revised \"mundlak\" strategy now implements a ‚Äútrue‚Äù Mundlak/CRE (correlated random effects) estimator by regressing Y on X plus group means of X. Unlike the \"demean\" strategy (above), this revised \"mundlak\" model obtains consistent coefficients regardless of panel structure (incl. unbalanced panels) and supports any number of fixed effects. However, users should note that Mundlak/CRE is a different model from ‚Äúvanilla‚Äù fixed effects‚Äîalbeit asymptotically equivalent under certain assumptions‚Äîand may obtain different coefficients as a result.\nPlease consult the expanded Acceleration Strategies section in the ?dbreg helpfile for technical details.\n\nAdd support for clustered standard errors. Follows the fixest API:\ndbreg(..., vcov = ~cluster_var)\nPlease note that these clustered SEs are computed analytically (not bootstrapped) and should thus add minimal overhead to your regressions. See the updated README for examples. (#29)\nThe \"auto\" strategy logic now considers a compress_nmax threshold, which governs the maximum allowable size of the compressed data object (default threshold = 1 million rows). This additional guardrail is intended to avoid cases where the \"compress\" strategy satisfies the compress_ratio threshold, but could still return a prohibitively large dataset. The most common example would be querying a massive dataset on a remote database, where network latency makes data I/O transfer expensive, even though we‚Äôve achieved good compression relative to the original data size. (#10)\n\nAside: Improved documentation and messaging (when verbose = TRUE) should also help users understand the \"auto\" strategy decision tree.\n\nEnabled weights for double demean (within) specification. (#13)\nEsimations now report some goodness-of-fit statistics like R2 and RMSE, powered by the (user-facing) gof() function. (#21)\nAdded support for various *.dbreg methods (#21, #30):\n\nFrom stats: coef(), confint(), predict(), and vcov().\nFrom broom/generics: tidy() and glance(). These also enable post-processing operations like exporting results to coefficient tables via modelsummary::msummary(). Thanks to @HariharanJayashankar for the request in #20.\n\n\nBug fixes\n\nAdded QR decomposition fallback for regression calculations, for cases where the default Cholesky solver fails. (#7)\nImproved integration for running regressions on AWS Athena datasets via the noctua package/driver. (#8)\nAutomatically drop incomplete cases (i.e., missing values) prior to any aggregation steps, avoiding mismatched matrices during estimation. (#19)\nUser-specified compress_ratio values should now bind in all cases. Previously, these could sometimes be silently ignored due to internal overrides. Also, clarify in the argument documentation that the default (automatic) compress_ratio threshold can vary based on heuristics related to model structure. (#25)\nCorrectly estimate HC1 standard errors for the \"moments\", \"demean\", and \"mundlak\" strategies. While the old analytic HC1 approach worked (and still does) for the \"compress\" case, it led to misleading SEs for these other strategies. The fix does impose some additional computational overhead, since it requires a second pass over the data to calculate the individual errors and ‚Äúmeat‚Äù of the sandwich matrix. But testing suggests that this leads to a &lt;2 increase in total estimation time, which seems a reasonable tradeoff for heteroskedastic-robust SEs. (#27)\n\nInternals\n\nAdded unit testing framework using tinytest. (#16)\nAdded GitHub Actions CI. (#18)\n\n\n\n\nIMPORTANT BREAKING CHANGE:\nThe package has been renamed to dbreg to better reflect the fact that it supports multiple database backends. (#4)\nOther breaking changes\n\nThe default vcov is now ‚Äúiid‚Äù. (#2 @grantmcdermott)\n\nNew features\n\nThe new dbreg(..., strategy = &lt;strategy&gt;) argument allows users to choose between different acceleration strategies for efficient computation of the regression coefficients and standard errors. This includes \"compress\" (the old default), as well as \"mundlak\" or \"moments\". The latter two strategies are newly introduced in dbreg v0.0.2 and offer important advantages in the case of true panel data. If an explicit strategy is not provided by the user, then dbreg() will invoke some internal heuristics to determine the optimal strategy based on the size and structure of the data. (#2 @jamesbrandecon and @grantmcdermott)\n\nProject\n\n@jamesbrandecon has joined the project as a core contributor.\n\n\n\n\n\nInitial GitHub release."
  },
  {
    "objectID": "NEWS.html#dev-version",
    "href": "NEWS.html#dev-version",
    "title": "News",
    "section": "",
    "text": "Website\n\nWe finally have a package website: grantmcdermott.com/dbreg üéâ\n\nBreaking changes\n\nThe behaviour of the \"mundlak\" strategy has been changed. (#24)\n\nThe old \"mundlak\" strategy is remapped to the new \"demean\" (alias \"within\") strategy, to better reflect the fact that this strategy invokes a (double) demeaning transformation. Users who want the old behaviour should thus use \"demean\" instead of \"mundlak\".\nSimultaneously, we now provide a revised \"mundlak\" strategy that implements a ‚Äútrue‚Äù Mundlak/CRE estimator; see New features below.\n\nFor estimations with two fixed effects on unbalanced panels, strategy=\"auto\" now errors when the compression limits are exceeded. It does this to avoid silently selecting a different estimand (i.e., Mundlak/CRE instead of TWFE). For these ambiguous cases, users will now be prompted to explicitly choose \"compress\" (with higher limits) or \"mundlak\" (different model and thus potentially different coefficients). (#24)\nThe default verbose behaviour is changed to FALSE. Users can revert to the old behaviour for a single call (i.e., dbreg(..., verbose = TRUE)), or set it globally (i.e., options(dbreg.verbose = TRUE)). (#33)\nTechnically not a breaking change, since we currently support backwards compatibility, but several minor arguments have been renamed/superseded. (#34)\n\nquery_only -&gt; sql_only (in dbreg)\nfes -&gt; fe (in print.dbreg, coef.dbreg, confint.dbreg, etc.)\n\n\nNew features\n\nWe have added new/revised acceleration strategies. (#24)\n\nThe \"demean\" (alias \"within\") strategy implements a (double) demeaning transformation and is particularly suited to (balanced) panels with one or two fixed effects. Note the underlying query is the same as the old \"mundlak\" strategy, which was somewhat erroneously named. Speaking of which‚Ä¶\nThe revised \"mundlak\" strategy now implements a ‚Äútrue‚Äù Mundlak/CRE (correlated random effects) estimator by regressing Y on X plus group means of X. Unlike the \"demean\" strategy (above), this revised \"mundlak\" model obtains consistent coefficients regardless of panel structure (incl. unbalanced panels) and supports any number of fixed effects. However, users should note that Mundlak/CRE is a different model from ‚Äúvanilla‚Äù fixed effects‚Äîalbeit asymptotically equivalent under certain assumptions‚Äîand may obtain different coefficients as a result.\nPlease consult the expanded Acceleration Strategies section in the ?dbreg helpfile for technical details.\n\nAdd support for clustered standard errors. Follows the fixest API:\ndbreg(..., vcov = ~cluster_var)\nPlease note that these clustered SEs are computed analytically (not bootstrapped) and should thus add minimal overhead to your regressions. See the updated README for examples. (#29)\nThe \"auto\" strategy logic now considers a compress_nmax threshold, which governs the maximum allowable size of the compressed data object (default threshold = 1 million rows). This additional guardrail is intended to avoid cases where the \"compress\" strategy satisfies the compress_ratio threshold, but could still return a prohibitively large dataset. The most common example would be querying a massive dataset on a remote database, where network latency makes data I/O transfer expensive, even though we‚Äôve achieved good compression relative to the original data size. (#10)\n\nAside: Improved documentation and messaging (when verbose = TRUE) should also help users understand the \"auto\" strategy decision tree.\n\nEnabled weights for double demean (within) specification. (#13)\nEsimations now report some goodness-of-fit statistics like R2 and RMSE, powered by the (user-facing) gof() function. (#21)\nAdded support for various *.dbreg methods (#21, #30):\n\nFrom stats: coef(), confint(), predict(), and vcov().\nFrom broom/generics: tidy() and glance(). These also enable post-processing operations like exporting results to coefficient tables via modelsummary::msummary(). Thanks to @HariharanJayashankar for the request in #20.\n\n\nBug fixes\n\nAdded QR decomposition fallback for regression calculations, for cases where the default Cholesky solver fails. (#7)\nImproved integration for running regressions on AWS Athena datasets via the noctua package/driver. (#8)\nAutomatically drop incomplete cases (i.e., missing values) prior to any aggregation steps, avoiding mismatched matrices during estimation. (#19)\nUser-specified compress_ratio values should now bind in all cases. Previously, these could sometimes be silently ignored due to internal overrides. Also, clarify in the argument documentation that the default (automatic) compress_ratio threshold can vary based on heuristics related to model structure. (#25)\nCorrectly estimate HC1 standard errors for the \"moments\", \"demean\", and \"mundlak\" strategies. While the old analytic HC1 approach worked (and still does) for the \"compress\" case, it led to misleading SEs for these other strategies. The fix does impose some additional computational overhead, since it requires a second pass over the data to calculate the individual errors and ‚Äúmeat‚Äù of the sandwich matrix. But testing suggests that this leads to a &lt;2 increase in total estimation time, which seems a reasonable tradeoff for heteroskedastic-robust SEs. (#27)\n\nInternals\n\nAdded unit testing framework using tinytest. (#16)\nAdded GitHub Actions CI. (#18)"
  },
  {
    "objectID": "NEWS.html#dbreg-0.0.2",
    "href": "NEWS.html#dbreg-0.0.2",
    "title": "News",
    "section": "",
    "text": "IMPORTANT BREAKING CHANGE:\nThe package has been renamed to dbreg to better reflect the fact that it supports multiple database backends. (#4)\nOther breaking changes\n\nThe default vcov is now ‚Äúiid‚Äù. (#2 @grantmcdermott)\n\nNew features\n\nThe new dbreg(..., strategy = &lt;strategy&gt;) argument allows users to choose between different acceleration strategies for efficient computation of the regression coefficients and standard errors. This includes \"compress\" (the old default), as well as \"mundlak\" or \"moments\". The latter two strategies are newly introduced in dbreg v0.0.2 and offer important advantages in the case of true panel data. If an explicit strategy is not provided by the user, then dbreg() will invoke some internal heuristics to determine the optimal strategy based on the size and structure of the data. (#2 @jamesbrandecon and @grantmcdermott)\n\nProject\n\n@jamesbrandecon has joined the project as a core contributor."
  },
  {
    "objectID": "NEWS.html#duckreg-0.0.1",
    "href": "NEWS.html#duckreg-0.0.1",
    "title": "News",
    "section": "",
    "text": "Initial GitHub release."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 dbreg authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "vignettes/intro.html",
    "href": "vignettes/intro.html",
    "title": "Introduction to dbreg",
    "section": "",
    "text": "COMING SOON.\nIn the meantime, please take a look at the README examples. The ?dbreg helpfile also contains a lot of useful information, especially concerning the various acceleration strategies."
  },
  {
    "objectID": "man/print.dbreg.html",
    "href": "man/print.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nprint(x, fe = FALSE, ...)\n\n\n\n\n\n\n\nx\n\n\n‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be displayed? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nOther arguments passed to print. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n# mod # same as below\nprint(mod)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\nprint(mod, fe = TRUE)  # include fixed effects\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411",
    "crumbs": [
      "Reference",
      "Base methods",
      "print.dbreg"
    ]
  },
  {
    "objectID": "man/print.dbreg.html#print-method-for-dbreg-objects",
    "href": "man/print.dbreg.html#print-method-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nprint(x, fe = FALSE, ...)\n\n\n\n\n\n\n\nx\n\n\n‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be displayed? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nOther arguments passed to print. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n# mod # same as below\nprint(mod)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\nprint(mod, fe = TRUE)  # include fixed effects\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411",
    "crumbs": [
      "Reference",
      "Base methods",
      "print.dbreg"
    ]
  },
  {
    "objectID": "man/vcov.dbreg.html",
    "href": "man/vcov.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Variance-covariance matrix for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nvcov(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "vcov.dbreg"
    ]
  },
  {
    "objectID": "man/vcov.dbreg.html#variance-covariance-matrix-for-dbreg-objects",
    "href": "man/vcov.dbreg.html#variance-covariance-matrix-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Variance-covariance matrix for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nvcov(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "vcov.dbreg"
    ]
  },
  {
    "objectID": "man/dbreg.html",
    "href": "man/dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics.\n\n\n\ndbreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  vcov = c(\"iid\", \"hc1\"),\n  strategy = c(\"auto\", \"compress\", \"moments\", \"demean\", \"within\", \"mundlak\"),\n  compress_ratio = NULL,\n  compress_nmax = 1e+06,\n  cluster = NULL,\n  ssc = c(\"full\", \"nested\"),\n  sql_only = FALSE,\n  data_only = FALSE,\n  drop_missings = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the relation to be estimated. Fixed effects should be included after a pipe, e.g fml = y ~ x1 + x2 | fe1 + f2. Currently, only simple additive terms are supported (i.e., no interaction terms, transformations or literals).\n\n\n\n\nconn\n\n\nDatabase connection, e.g.¬†created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g.¬†‚Äúmydata/**/.*parquet‚Äù. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g.¬†‚Äúread_parquet(‚Äômydata/**/*.parquet‚Äô)‚Äú for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\nvcov\n\n\nCharacter string or formula denoting the desired type of variance- covariance correction / standard errors. Options are ‚Äúiid‚Äù (default), ‚Äúhc1‚Äù (heteroskedasticity-consistent), or a one-sided formula like ~cluster_var for cluster-robust standard errors. Note that ‚Äúhc1‚Äù and clustered SEs require a second pass over the data unless strategy = ‚Äúcompress‚Äù to construct the residuals.\n\n\n\n\nstrategy\n\n\nCharacter string indicating the preferred acceleration strategy. The default ‚Äúauto‚Äù will pick an optimal strategy based on internal heuristics. Users can also override with one of the following explicit strategies: ‚Äúcompress‚Äù, ‚Äúdemean‚Äù (alias: ‚Äúwithin‚Äù), ‚Äúmundlak‚Äù, or ‚Äúmoments‚Äù. See the Acceleration Strategies section below for details.\n\n\n\n\ncompress_ratio, compress_nmax\n\n\nNumeric(s). Parameters that help to determine the acceleration strategy under the default ‚Äúauto‚Äù option.\n\n\ncompress_ratio defines the compression ratio threshold, i.e.¬†numeric in the range [0,1] defining the minimum acceptable compressed versus the original data size. Default value of NULL means that the threshold will be automatically determined based on some internal heuristic (e.g., 0.01 for models without fixed effects).\n\n\ncompress_nmax defines the maximum allowable size (in rows) of the compressed dataset that can be serialized into R. Pays heed to the idea that big data serialization can be costly (esp.¬†for remote databases), even if we have achieved good compression on top of the original dataset. Default value is 1e6 (i.e., a million rows).\n\n\nSee the Acceleration Strategies section below for further details.\n\n\n\n\ncluster\n\n\nOptional. Provides an alternative way to specify cluster-robust standard errors (i.e., instead of vcov = ~cluster_var). Either a one-sided formula (e.g., ~firm) or character string giving the variable name. Only single-variable clustering is currently supported.\n\n\n\n\nssc\n\n\nCharacter string controlling the small-sample correction for clustered standard errors. Options are ‚Äúfull‚Äù (default) or ‚Äúnested‚Äù. With ‚Äúfull‚Äù, all parameters (including fixed effect dummies) are counted in K for the CR1 correction. With ‚Äúnested‚Äù, fixed effects that are nested within the cluster variable are excluded from K, matching the default behavior of fixest::feols. Only applies to ‚Äúcompress‚Äù and ‚Äúdemean‚Äù strategies (Mundlak uses explicit group mean regressors, not FE dummies). This distinction only matters for small samples. For large datasets (dbreg‚Äôs target use case), the difference is negligible and hence we default to the simple ‚Äúfull‚Äù option.\n\n\n\n\nsql_only\n\n\nLogical indicating whether only the underlying compression SQL query should be returned (i.e., no computation will be performed). Default is FALSE.\n\n\n\n\ndata_only\n\n\nLogical indicating whether only the compressed dataset should be returned (i.e., no regression is run). Default is FALSE.\n\n\n\n\ndrop_missings\n\n\nLogical indicating whether incomplete cases (i.e., rows where any of the dependent, independent or FE variables are missing) should be dropped. The default is TRUE, according with standard regression software. It is strongly recommended not to change this value unless you are absolutely sure that your data have no missings and you wish to skip some internal checks. (Even then, it probably isn‚Äôt worth it.)\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently ignored, except to handle superseded arguments for backwards compatibility.\n\n\n\n\n\n\nA list of class \"dbreg\" containing various slots, including a table of coefficients (which the associated print method will display).\n\n\n\ndbreg offers four primary acceleration strategies for estimating regression results from simplified data representations. Below we use the shorthand Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition purposes:\n\n\n‚Äúcompress‚Äù: compresses the data via a GROUP BY operation (using X and the FE as groups), before running weighted least squares on this much smaller dataset:\n\n\\(\\hat{\\beta} = (X_c' W X_c)^{-1} X_c' W Y_c\\)\nwhere \\(W = \\text{diag}(n_g)\\) are the group frequencies. This procedure follows Wang et al.¬†(2021).\n\n\n‚Äúmoments‚Äù: computes sufficient statistics (\\(X'X, X'y\\)) directly via SQL aggregation, returning a single-row result. This solves the standard OLS normal equations \\(\\hat{\\beta} = (X'X)^{-1}X'y\\). Limited to cases without FE.\n\n\n‚Äúdemean‚Äù (alias ‚Äúwithin‚Äù): subtracts group-level means from both Y and X before computing sufficient statistics (per the ‚Äúmoments‚Äù strategy). For example, given unit \\(i\\) and time \\(t\\) FE, we apply double demeaning:\n\n\\(\\ddot{Y}_{it} = \\beta \\ddot{X}_{it} + \\varepsilon_{it}\\)\nwhere \\(\\ddot{X} = X - \\bar{X}_i - \\bar{X}_t + \\bar{X}\\). This (single-pass) within transformation is algebraically equivalent to the fixed effects projection‚Äîi.e., Frisch-Waugh-Lovell partialling out‚Äîin the presence of a single FE. It is also identical for the two-way FE (TWFE) case if your panel is balanced. For unbalanced two-way panels, however, the double demeaning strategy is not algebraically equivalent to the fixed effects projection and therefore does not recover the exact TWFE coefficients. Moreover, note that this ‚Äúdemean‚Äù strategy permits at most two FE.\n\n\n‚Äúmundlak‚Äù: a generalized Mundlak (1978), or correlated random effects (CRE) estimator that regresses Y on X plus group means of X:\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_i + \\varepsilon_{it} \\quad \\text{(one-way)}\\)\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_{i} + \\delta \\bar{X}_{t} + \\varepsilon_{it} \\quad \\text{(two-way, etc.)}\\)\nUnlike ‚Äúdemean‚Äù, Y is not transformed, so predictions are on the original scale. Supports any number of FE and works correctly for any panel structure (balanced or unbalanced). However, note that CRE is a different model from FE: while coefficients are asymptotically equivalent under certain assumptions, they will generally differ in finite samples.\n\n\nThe relative efficiency of each of these strategies depends on the size and structure of the data, as well the number of unique regressors and FE. For (quote unquote) \"standard\" cases, the ‚Äúcompress‚Äù strategy can yield remarkable performance gains and should justifiably be viewed as a good default. However, the compression approach tends to be less efficient for true panels (repeated cross-sections over time), where N &gt;&gt; T. In such cases, it can be more efficient to use a demeaning strategy that first controls for (e.g.¬†subtracts) group means, before computing sufficient statistics on the aggregated data. The reason for this is that time and unit FE are typically high dimensional, but covariate averages are not; see Arkhangelsky & Imbens (2024).\nHowever, the demeaning approaches invite tradeoffs of their own. For example, the double demeaning transformation of the ‚Äúdemean‚Äù strategy does not obtain exact TWFE results in unbalanced panels, and it is also limited to at most two FE. Conversely, the ‚Äúmundlak‚Äù (CRE) strategy obtains consistent coefficients regardless of panel structure and FE count, but at the \"cost\" of recovering a different estimand. (It is a different model to TWFE, after all.) See Wooldridge (2025) for an extended discussion of these issues.\nUsers should weigh these tradeoffs when choosing their acceleration strategy. Summarising, we can provide a few guiding principles. ‚Äúcompress‚Äù is a good default that guarantees the \"exact\" FE estimates and is usually very efficient (barring data I/O costs and high FE dimensionality). ‚Äúmundlak‚Äù is another efficient alternative provided that the CRE estimand is acceptable (don‚Äôt be alarmed if your coefficients are not identical). Finally, the ‚Äúdemean‚Äù and ‚Äúmoments‚Äù strategies are great for particular use cases (i.e., balanced panels and cases without FE, respectively).\nIf this all sounds like too much to think about, don‚Äôt fret. The good news is that dbreg can do a lot (all?) of the deciding for you. Specifically, it will invoke an ‚Äúauto‚Äù heuristic behind the scenes if a user does not provide an explicit acceleration strategy. Working through the heuristic logic does impose some additional overhead, but this should be negligible in most cases (certainly compared to the overall time savings). The ‚Äúauto‚Äù heuristic is as follows:\n\n\nIF no FE AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN ‚Äúmoments‚Äù.\n\n\nELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN ‚Äúdemean‚Äù.\n\n\nELSE IF 2 FE AND (poor compression ratio OR too big compressed data):\n\n\nIF balanced panel THEN ‚Äúdemean‚Äù.\n\n\nELSE error (exact TWFE infeasible; user must explicitly choose ‚Äúcompress‚Äù or ‚Äúmundlak‚Äù).\n\n\n\n\nELSE THEN ‚Äúcompress‚Äù.\n\n\nTip: set dbreg(‚Ä¶, verbose = TRUE) to print information about the auto strategy decision criteria.\n\n\n\nArkhangelsky, D. & Imbens, G. (2024) Fixed Effects and the Generalized Mundlak Estimator. The Review of Economic Studies, 91(5), pp.¬†2545‚Äì2571. Available: https://doi.org/10.1093/restud/rdad089\nMundlak, Y. (1978) On the Pooling of Time Series and Cross Section Data. Econometrica, 46(1), pp.¬†69‚Äì85. Available: https://doi.org/10.2307/1913646\nWong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021). You Only Compress Once: Optimal Data Compression for Estimating Linear Models. arXiv preprint arXiv:2102.11297. Available: https://doi.org/10.48550/arXiv.2102.11297\nWooldridge, J.M. (2025) Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators. Empirical Economics, 69, pp.¬†2545‚Äì2587. Available: https://doi.org/10.1007/s00181-025-02807-z\n\n\n\ndbConnect for creating database connections, duckdb for DuckDB-specific connections\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## Small dataset ----\n\n# dbreg is primarily intended for use against big datasets/databases. But it\n# also works with small in-memory datasets, which lets us demo the syntax...\n\n# auto strategy defaults to \"compress\" in this case\n(mod = dbreg(Temp ~ Wind | Month, data = airquality))\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\n# Same result as lm\ncoef(lm(Temp ~ Wind + factor(Month), data = airquality))\n\n   (Intercept)           Wind factor(Month)6 factor(Month)7 factor(Month)8 \n     74.188474      -0.743388      12.543643      16.362079      16.316286 \nfactor(Month)9 \n     10.279216 \n\n# aside: dbreg's default print method hides the \"nuisance\" coefficients\n# like the intercept and fixed effect(s). But we can grab them if we want.\nprint(mod, fe = TRUE)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\n# \"robust\" SEs can also be computed using a sufficient statistics approach\ndbreg(Temp ~ Wind | Month, data = airquality, vcov = \"hc1\")\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: Heteroskedasticity-robust \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nWind -0.743388    0.16158 -4.60073 9.0198e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\ndbreg(Temp ~ Wind | Month, data = airquality, vcov = ~Month)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: Clustered (5 clusters) \n      Estimate Std. Error  t value  Pr(&gt;|t|)    \nWind -0.743388   0.221804 -3.35155 0.0010217 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\n# other strategies\ndbreg(Temp ~ Wind | Month, data = airquality, strategy = \"demean\")\n\nDemeaned OLS estimation, Dep. Var.: Temp \nObservations.: 153 \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.116084\n\ndbreg(Temp ~ Wind | Month, data = airquality, strategy = \"mundlak\")\n\nOne-way Mundlak OLS estimation, Dep. Var.: Temp \nObservations.: 153 \nStandard Errors: IID \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nWind -0.743389   0.153267 -4.85028 3.0557e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.3                 Adj. R2: 0.548417\n\ndbreg(Temp ~ Wind, data = airquality, strategy = \"moments\") # no FEs\n\nMoments-based OLS estimation, Dep. Var.: Temp \nObservations.: 153 \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 90.13487   2.052185 43.92140  &lt; 2.2e-16 ***\nWind        -1.23048   0.194363 -6.33084 2.6416e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 8.4                 Adj. R2: 0.204519\n\n#\n## Big dataset ----\n\n# For a more compelling and appropriate dbreg use-case, i.e. regression on a\n# big (~180 million row) dataset of Hive-partioned parquet files, see the\n# package website:\n# https://github.com/grantmcdermott/dbreg?tab=readme-ov-file#quickstart",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbreg"
    ]
  },
  {
    "objectID": "man/dbreg.html#run-a-regression-on-a-database-backend.",
    "href": "man/dbreg.html#run-a-regression-on-a-database-backend.",
    "title": "dbreg",
    "section": "",
    "text": "Leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics.\n\n\n\ndbreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  vcov = c(\"iid\", \"hc1\"),\n  strategy = c(\"auto\", \"compress\", \"moments\", \"demean\", \"within\", \"mundlak\"),\n  compress_ratio = NULL,\n  compress_nmax = 1e+06,\n  cluster = NULL,\n  ssc = c(\"full\", \"nested\"),\n  sql_only = FALSE,\n  data_only = FALSE,\n  drop_missings = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the relation to be estimated. Fixed effects should be included after a pipe, e.g fml = y ~ x1 + x2 | fe1 + f2. Currently, only simple additive terms are supported (i.e., no interaction terms, transformations or literals).\n\n\n\n\nconn\n\n\nDatabase connection, e.g.¬†created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g.¬†‚Äúmydata/**/.*parquet‚Äù. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g.¬†‚Äúread_parquet(‚Äômydata/**/*.parquet‚Äô)‚Äú for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\nvcov\n\n\nCharacter string or formula denoting the desired type of variance- covariance correction / standard errors. Options are ‚Äúiid‚Äù (default), ‚Äúhc1‚Äù (heteroskedasticity-consistent), or a one-sided formula like ~cluster_var for cluster-robust standard errors. Note that ‚Äúhc1‚Äù and clustered SEs require a second pass over the data unless strategy = ‚Äúcompress‚Äù to construct the residuals.\n\n\n\n\nstrategy\n\n\nCharacter string indicating the preferred acceleration strategy. The default ‚Äúauto‚Äù will pick an optimal strategy based on internal heuristics. Users can also override with one of the following explicit strategies: ‚Äúcompress‚Äù, ‚Äúdemean‚Äù (alias: ‚Äúwithin‚Äù), ‚Äúmundlak‚Äù, or ‚Äúmoments‚Äù. See the Acceleration Strategies section below for details.\n\n\n\n\ncompress_ratio, compress_nmax\n\n\nNumeric(s). Parameters that help to determine the acceleration strategy under the default ‚Äúauto‚Äù option.\n\n\ncompress_ratio defines the compression ratio threshold, i.e.¬†numeric in the range [0,1] defining the minimum acceptable compressed versus the original data size. Default value of NULL means that the threshold will be automatically determined based on some internal heuristic (e.g., 0.01 for models without fixed effects).\n\n\ncompress_nmax defines the maximum allowable size (in rows) of the compressed dataset that can be serialized into R. Pays heed to the idea that big data serialization can be costly (esp.¬†for remote databases), even if we have achieved good compression on top of the original dataset. Default value is 1e6 (i.e., a million rows).\n\n\nSee the Acceleration Strategies section below for further details.\n\n\n\n\ncluster\n\n\nOptional. Provides an alternative way to specify cluster-robust standard errors (i.e., instead of vcov = ~cluster_var). Either a one-sided formula (e.g., ~firm) or character string giving the variable name. Only single-variable clustering is currently supported.\n\n\n\n\nssc\n\n\nCharacter string controlling the small-sample correction for clustered standard errors. Options are ‚Äúfull‚Äù (default) or ‚Äúnested‚Äù. With ‚Äúfull‚Äù, all parameters (including fixed effect dummies) are counted in K for the CR1 correction. With ‚Äúnested‚Äù, fixed effects that are nested within the cluster variable are excluded from K, matching the default behavior of fixest::feols. Only applies to ‚Äúcompress‚Äù and ‚Äúdemean‚Äù strategies (Mundlak uses explicit group mean regressors, not FE dummies). This distinction only matters for small samples. For large datasets (dbreg‚Äôs target use case), the difference is negligible and hence we default to the simple ‚Äúfull‚Äù option.\n\n\n\n\nsql_only\n\n\nLogical indicating whether only the underlying compression SQL query should be returned (i.e., no computation will be performed). Default is FALSE.\n\n\n\n\ndata_only\n\n\nLogical indicating whether only the compressed dataset should be returned (i.e., no regression is run). Default is FALSE.\n\n\n\n\ndrop_missings\n\n\nLogical indicating whether incomplete cases (i.e., rows where any of the dependent, independent or FE variables are missing) should be dropped. The default is TRUE, according with standard regression software. It is strongly recommended not to change this value unless you are absolutely sure that your data have no missings and you wish to skip some internal checks. (Even then, it probably isn‚Äôt worth it.)\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently ignored, except to handle superseded arguments for backwards compatibility.\n\n\n\n\n\n\nA list of class \"dbreg\" containing various slots, including a table of coefficients (which the associated print method will display).\n\n\n\ndbreg offers four primary acceleration strategies for estimating regression results from simplified data representations. Below we use the shorthand Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition purposes:\n\n\n‚Äúcompress‚Äù: compresses the data via a GROUP BY operation (using X and the FE as groups), before running weighted least squares on this much smaller dataset:\n\n\\(\\hat{\\beta} = (X_c' W X_c)^{-1} X_c' W Y_c\\)\nwhere \\(W = \\text{diag}(n_g)\\) are the group frequencies. This procedure follows Wang et al.¬†(2021).\n\n\n‚Äúmoments‚Äù: computes sufficient statistics (\\(X'X, X'y\\)) directly via SQL aggregation, returning a single-row result. This solves the standard OLS normal equations \\(\\hat{\\beta} = (X'X)^{-1}X'y\\). Limited to cases without FE.\n\n\n‚Äúdemean‚Äù (alias ‚Äúwithin‚Äù): subtracts group-level means from both Y and X before computing sufficient statistics (per the ‚Äúmoments‚Äù strategy). For example, given unit \\(i\\) and time \\(t\\) FE, we apply double demeaning:\n\n\\(\\ddot{Y}_{it} = \\beta \\ddot{X}_{it} + \\varepsilon_{it}\\)\nwhere \\(\\ddot{X} = X - \\bar{X}_i - \\bar{X}_t + \\bar{X}\\). This (single-pass) within transformation is algebraically equivalent to the fixed effects projection‚Äîi.e., Frisch-Waugh-Lovell partialling out‚Äîin the presence of a single FE. It is also identical for the two-way FE (TWFE) case if your panel is balanced. For unbalanced two-way panels, however, the double demeaning strategy is not algebraically equivalent to the fixed effects projection and therefore does not recover the exact TWFE coefficients. Moreover, note that this ‚Äúdemean‚Äù strategy permits at most two FE.\n\n\n‚Äúmundlak‚Äù: a generalized Mundlak (1978), or correlated random effects (CRE) estimator that regresses Y on X plus group means of X:\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_i + \\varepsilon_{it} \\quad \\text{(one-way)}\\)\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_{i} + \\delta \\bar{X}_{t} + \\varepsilon_{it} \\quad \\text{(two-way, etc.)}\\)\nUnlike ‚Äúdemean‚Äù, Y is not transformed, so predictions are on the original scale. Supports any number of FE and works correctly for any panel structure (balanced or unbalanced). However, note that CRE is a different model from FE: while coefficients are asymptotically equivalent under certain assumptions, they will generally differ in finite samples.\n\n\nThe relative efficiency of each of these strategies depends on the size and structure of the data, as well the number of unique regressors and FE. For (quote unquote) \"standard\" cases, the ‚Äúcompress‚Äù strategy can yield remarkable performance gains and should justifiably be viewed as a good default. However, the compression approach tends to be less efficient for true panels (repeated cross-sections over time), where N &gt;&gt; T. In such cases, it can be more efficient to use a demeaning strategy that first controls for (e.g.¬†subtracts) group means, before computing sufficient statistics on the aggregated data. The reason for this is that time and unit FE are typically high dimensional, but covariate averages are not; see Arkhangelsky & Imbens (2024).\nHowever, the demeaning approaches invite tradeoffs of their own. For example, the double demeaning transformation of the ‚Äúdemean‚Äù strategy does not obtain exact TWFE results in unbalanced panels, and it is also limited to at most two FE. Conversely, the ‚Äúmundlak‚Äù (CRE) strategy obtains consistent coefficients regardless of panel structure and FE count, but at the \"cost\" of recovering a different estimand. (It is a different model to TWFE, after all.) See Wooldridge (2025) for an extended discussion of these issues.\nUsers should weigh these tradeoffs when choosing their acceleration strategy. Summarising, we can provide a few guiding principles. ‚Äúcompress‚Äù is a good default that guarantees the \"exact\" FE estimates and is usually very efficient (barring data I/O costs and high FE dimensionality). ‚Äúmundlak‚Äù is another efficient alternative provided that the CRE estimand is acceptable (don‚Äôt be alarmed if your coefficients are not identical). Finally, the ‚Äúdemean‚Äù and ‚Äúmoments‚Äù strategies are great for particular use cases (i.e., balanced panels and cases without FE, respectively).\nIf this all sounds like too much to think about, don‚Äôt fret. The good news is that dbreg can do a lot (all?) of the deciding for you. Specifically, it will invoke an ‚Äúauto‚Äù heuristic behind the scenes if a user does not provide an explicit acceleration strategy. Working through the heuristic logic does impose some additional overhead, but this should be negligible in most cases (certainly compared to the overall time savings). The ‚Äúauto‚Äù heuristic is as follows:\n\n\nIF no FE AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN ‚Äúmoments‚Äù.\n\n\nELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN ‚Äúdemean‚Äù.\n\n\nELSE IF 2 FE AND (poor compression ratio OR too big compressed data):\n\n\nIF balanced panel THEN ‚Äúdemean‚Äù.\n\n\nELSE error (exact TWFE infeasible; user must explicitly choose ‚Äúcompress‚Äù or ‚Äúmundlak‚Äù).\n\n\n\n\nELSE THEN ‚Äúcompress‚Äù.\n\n\nTip: set dbreg(‚Ä¶, verbose = TRUE) to print information about the auto strategy decision criteria.\n\n\n\nArkhangelsky, D. & Imbens, G. (2024) Fixed Effects and the Generalized Mundlak Estimator. The Review of Economic Studies, 91(5), pp.¬†2545‚Äì2571. Available: https://doi.org/10.1093/restud/rdad089\nMundlak, Y. (1978) On the Pooling of Time Series and Cross Section Data. Econometrica, 46(1), pp.¬†69‚Äì85. Available: https://doi.org/10.2307/1913646\nWong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021). You Only Compress Once: Optimal Data Compression for Estimating Linear Models. arXiv preprint arXiv:2102.11297. Available: https://doi.org/10.48550/arXiv.2102.11297\nWooldridge, J.M. (2025) Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators. Empirical Economics, 69, pp.¬†2545‚Äì2587. Available: https://doi.org/10.1007/s00181-025-02807-z\n\n\n\ndbConnect for creating database connections, duckdb for DuckDB-specific connections\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## Small dataset ----\n\n# dbreg is primarily intended for use against big datasets/databases. But it\n# also works with small in-memory datasets, which lets us demo the syntax...\n\n# auto strategy defaults to \"compress\" in this case\n(mod = dbreg(Temp ~ Wind | Month, data = airquality))\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\n# Same result as lm\ncoef(lm(Temp ~ Wind + factor(Month), data = airquality))\n\n   (Intercept)           Wind factor(Month)6 factor(Month)7 factor(Month)8 \n     74.188474      -0.743388      12.543643      16.362079      16.316286 \nfactor(Month)9 \n     10.279216 \n\n# aside: dbreg's default print method hides the \"nuisance\" coefficients\n# like the intercept and fixed effect(s). But we can grab them if we want.\nprint(mod, fe = TRUE)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\n# \"robust\" SEs can also be computed using a sufficient statistics approach\ndbreg(Temp ~ Wind | Month, data = airquality, vcov = \"hc1\")\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: Heteroskedasticity-robust \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nWind -0.743388    0.16158 -4.60073 9.0198e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\ndbreg(Temp ~ Wind | Month, data = airquality, vcov = ~Month)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: Clustered (5 clusters) \n      Estimate Std. Error  t value  Pr(&gt;|t|)    \nWind -0.743388   0.221804 -3.35155 0.0010217 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\n# other strategies\ndbreg(Temp ~ Wind | Month, data = airquality, strategy = \"demean\")\n\nDemeaned OLS estimation, Dep. Var.: Temp \nObservations.: 153 \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.116084\n\ndbreg(Temp ~ Wind | Month, data = airquality, strategy = \"mundlak\")\n\nOne-way Mundlak OLS estimation, Dep. Var.: Temp \nObservations.: 153 \nStandard Errors: IID \n      Estimate Std. Error  t value   Pr(&gt;|t|)    \nWind -0.743389   0.153267 -4.85028 3.0557e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.3                 Adj. R2: 0.548417\n\ndbreg(Temp ~ Wind, data = airquality, strategy = \"moments\") # no FEs\n\nMoments-based OLS estimation, Dep. Var.: Temp \nObservations.: 153 \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 90.13487   2.052185 43.92140  &lt; 2.2e-16 ***\nWind        -1.23048   0.194363 -6.33084 2.6416e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 8.4                 Adj. R2: 0.204519\n\n#\n## Big dataset ----\n\n# For a more compelling and appropriate dbreg use-case, i.e. regression on a\n# big (~180 million row) dataset of Hive-partioned parquet files, see the\n# package website:\n# https://github.com/grantmcdermott/dbreg?tab=readme-ov-file#quickstart",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbreg"
    ]
  },
  {
    "objectID": "man/gof.html",
    "href": "man/gof.html",
    "title": "dbreg",
    "section": "",
    "text": "Calculate goodness-of-fit metrics for dbreg objects\n\n\n\ngof(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused)\n\n\n\n\n\n\nNamed vector with r2, adj_r2, and rmse\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ngof(mod)\n\n       r2    adj_r2      rmse \n0.5884105 0.5744109 6.0525892",
    "crumbs": [
      "Reference",
      "utilities",
      "gof"
    ]
  },
  {
    "objectID": "man/gof.html#calculate-goodness-of-fit-metrics-for-dbreg-objects",
    "href": "man/gof.html#calculate-goodness-of-fit-metrics-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Calculate goodness-of-fit metrics for dbreg objects\n\n\n\ngof(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused)\n\n\n\n\n\n\nNamed vector with r2, adj_r2, and rmse\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ngof(mod)\n\n       r2    adj_r2      rmse \n0.5884105 0.5744109 6.0525892",
    "crumbs": [
      "Reference",
      "utilities",
      "gof"
    ]
  },
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite package ‚Äòdbreg‚Äô in publications use:\n\nMcDermott G (2025). dbreg: Fast Regressions on Database Backends. R package version 0.0.2.99, https://grantmcdermott.com/dbreg/."
  }
]
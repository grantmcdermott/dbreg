% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dbreg.R
\name{dbreg}
\alias{dbreg}
\title{Run a regression on a database backend}
\usage{
dbreg(
  fml,
  conn = NULL,
  table = NULL,
  data = NULL,
  path = NULL,
  vcov = c("iid", "hc1"),
  strategy = c("auto", "compress", "moments", "demean", "within", "mundlak"),
  compress_ratio = NULL,
  compress_nmax = 1e+06,
  cluster = NULL,
  ssc = c("full", "nested"),
  sql_only = FALSE,
  data_only = FALSE,
  drop_missings = TRUE,
  verbose = getOption("dbreg.verbose", FALSE),
  ...
)
}
\arguments{
\item{fml}{A \code{\link[stats]{formula}} representing the relation to be
estimated. Fixed effects should be included after a pipe, e.g
\code{fml = y ~ x1 + x2 | fe1 + f2}. Interaction terms are supported using
standard R syntax (\code{:} for interactions, \code{*} for main effects plus
interaction). Transformations and literals are not yet supported.}

\item{conn}{Database connection, e.g. created with
\code{\link[DBI]{dbConnect}}. Can be either persistent (disk-backed) or
ephemeral (in-memory). If no connection is provided, then an ephemeral
\code{\link[duckdb]{duckdb}} connection will be created automatically and
closed before the function exits. Note that a persistent (disk-backed)
database connection is required for larger-than-RAM datasets in order to take
advantage of out-of-core functionality like streaming (where supported).}

\item{table, data, path}{Mutually exclusive arguments for specifying the data
table (object) to be queried. In order of precedence:
\itemize{
\item \code{table}: Character string giving the name of the data table in an
existing (open) database connection.
\item \code{data}: R dataframe that can be copied over to \code{conn} as a temporary
table for querying via the DuckDB query engine. Ignored if \code{table} is
provided.
\item \code{path}: Character string giving a path to the data file(s) on disk, which
will be read into \code{conn}. Internally, this string is passed to the \code{FROM}
query statement, so could (should) include file globbing for
Hive-partitioned datasets, e.g. \code{"mydata/**/.*parquet"}. For more precision,
however, it is recommended to pass the desired database reader function as
part of this string, e.g. \code{"read_parquet('mydata/**/*.parquet')"} for DuckDB;
note the use of single quotes.
Ignored if either \code{table} or \code{data} is provided.
}}

\item{vcov}{Character string or formula denoting the desired type of variance-
covariance correction / standard errors. Options are \code{"iid"} (default),
\code{"hc1"} (heteroskedasticity-consistent), or a one-sided formula like
\code{~cluster_var} for cluster-robust standard errors. Note that \code{"hc1"} and
clustered SEs require a second pass over the data unless
\code{strategy = "compress"} to construct the residuals.}

\item{strategy}{Character string indicating the preferred acceleration
strategy. The default \code{"auto"} will pick an optimal strategy based on
internal heuristics. Users can also override with one of the following
explicit strategies: \code{"compress"}, \code{"demean"} (alias: \code{"within"}),
\code{"mundlak"}, or \code{"moments"}. See the Acceleration Strategies section below
for details.}

\item{compress_ratio, compress_nmax}{Numeric(s). Parameters that help to
determine the acceleration \code{strategy} under the default \code{"auto"} option.
\itemize{
\item \code{compress_ratio} defines the compression ratio threshold, i.e. numeric
in the range \verb{[0,1]} defining the minimum acceptable compressed versus
the original data size. Default value of \code{NULL} means that the threshold
will be automatically determined based on some internal heuristic
(e.g., 0.01 for models without fixed effects).
\item \code{compress_nmax} defines the maximum allowable size (in rows) of the
compressed dataset that can be serialized into R. Pays heed to the idea
that big data serialization can be costly (esp. for remote databases),
even if we have achieved good compression on top of the original dataset.
Default value is 1e6 (i.e., a million rows).
}

See the Acceleration Strategies section below for further details.}

\item{cluster}{Optional. Provides an alternative way to specify
cluster-robust standard errors (i.e., instead of \code{vcov = ~cluster_var}).
Either a one-sided formula (e.g., \code{~firm}) or character string giving the
variable name. Only single-variable clustering is currently supported.}

\item{ssc}{Character string controlling the small-sample correction for
clustered standard errors. Options are \code{"full"} (default) or \code{"nested"}.
With \code{"full"}, all parameters (including fixed effect dummies) are counted
in K for the CR1 correction. With \code{"nested"}, fixed effects that are nested
within the cluster variable are excluded from K, matching the default
behavior of \code{fixest::feols}. Only applies to \code{"compress"} and \code{"demean"}
strategies (Mundlak uses explicit group mean regressors, not FE dummies).
This distinction only matters for small samples. For large datasets
(\code{dbreg}'s target use case), the difference is negligible and hence we
default to the simple \code{"full"} option.}

\item{sql_only}{Logical indicating whether only the underlying compression
SQL query should be returned (i.e., no computation will be performed).
Default is \code{FALSE}.}

\item{data_only}{Logical indicating whether only the compressed dataset
should be returned (i.e., no regression is run). Default is \code{FALSE}.}

\item{drop_missings}{Logical indicating whether incomplete cases (i.e., rows
where any of the dependent, independent or FE variables are
missing) should be dropped. The default is \code{TRUE}, according with standard
regression software. It is \emph{strongly} recommended not to change this value
unless you are absolutely sure that your data have no missings and you wish
to skip some internal checks. (Even then, it probably isn't worth it.)}

\item{verbose}{Logical. Print auto strategy and progress messages to the
console? Defaults to \code{FALSE}. This can be overridden for a single call
by supplying \code{verbose = TRUE}, or set globally via
\code{options(dbreg.verbose = TRUE)}.}

\item{...}{Additional arguments. Currently ignored, except to handle
superseded arguments for backwards compatibility.}
}
\value{
A list of class "dbreg" containing various slots, including a table
of coefficients (which the associated \code{print} method will display).
}
\description{
Leverages the power of databases to run regressions on very large datasets,
which may not fit into R's memory. Various acceleration strategies allow for
highly efficient computation, while robust standard errors are computed from
sufficient statistics.
}
\section{Acceleration Strategies}{


\code{dbreg} offers four primary acceleration strategies for estimating regression
results from simplified data representations. Below we use the shorthand
Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition
purposes:
\enumerate{
\item \code{"compress"}: compresses the data via a \verb{GROUP BY} operation (using X and
the FE as groups), before running weighted least squares on this much
smaller dataset:
\deqn{\hat{\beta} = (X_c' W X_c)^{-1} X_c' W Y_c}
where \eqn{W = \text{diag}(n_g)} are the group frequencies. This procedure
follows Wang et al. (2021).
\item \code{"moments"}: computes sufficient statistics (\eqn{X'X, X'y}) directly via
SQL aggregation, returning a single-row result. This solves the standard
OLS normal equations \eqn{\hat{\beta} = (X'X)^{-1}X'y}. Limited to cases
without FE.
\item \code{"demean"} (alias \code{"within"}): subtracts group-level means from both Y and
X before computing sufficient statistics (per the \code{"moments"} strategy).
For example, given unit \eqn{i} and time \eqn{t} FE, we apply double
demeaning:
\deqn{\ddot{Y}_{it} = \beta \ddot{X}_{it} + \varepsilon_{it}}
where \eqn{\ddot{X} = X - \bar{X}_i - \bar{X}_t + \bar{X}}. This
(single-pass) within transformation is algebraically equivalent to the
fixed effects projection---i.e., Frisch-Waugh-Lovell partialling out---in
the presence of a single FE. It is also identical for the two-way FE
(TWFE) case if your panel is balanced. For unbalanced two-way panels,
however, the double demeaning strategy is not algebraically equivalent to
the fixed effects projection and therefore does not recover the exact TWFE
coefficients. Moreover, note that this \code{"demean"} strategy permits at most
two FE.
\item \code{"mundlak"}: a generalized Mundlak (1978), or correlated random effects
(CRE) estimator that regresses Y on X plus group means of X:
\deqn{Y_{it} = \alpha + \beta X_{it} + \gamma \bar{X}_i + \varepsilon_{it} \quad \text{(one-way)}}
\deqn{Y_{it} = \alpha + \beta X_{it} + \gamma \bar{X}_{i} + \delta \bar{X}_{t} + \varepsilon_{it} \quad \text{(two-way, etc.)}}
Unlike \code{"demean"}, Y is not transformed, so predictions are on the
original scale. Supports any number of FE and works correctly for any
panel structure (balanced or unbalanced). However, note that CRE is a
\emph{different model} from FE: while coefficients are asymptotically
equivalent under certain assumptions, they will generally differ in
finite samples.
}

The relative efficiency of each of these strategies depends on the size and
structure of the data, as well the number of unique regressors and FE. For
(quote unquote) "standard" cases, the \code{"compress"} strategy can yield
remarkable performance gains and should justifiably be viewed as a good
default. However, the compression approach tends to be less efficient for
true panels (repeated cross-sections over time), where N >> T. In such
cases, it can be more efficient to use a demeaning strategy that first
controls for (e.g. subtracts) group means, before computing sufficient
statistics on the aggregated data. The reason for this is that time and unit
FE are typically high dimensional, but covariate averages are not; see
Arkhangelsky & Imbens (2024).

However, the demeaning approaches invite tradeoffs of their own. For example,
the double demeaning transformation of the \code{"demean"} strategy does not
obtain exact TWFE results in unbalanced panels, and it is also limited to at
most two FE. Conversely, the \code{"mundlak"} (CRE) strategy obtains consistent
coefficients regardless of panel structure and FE count, but at the "cost" of
recovering a different estimand. (It is a different model to TWFE, after
all.) See Wooldridge (2025) for an extended discussion of these issues.

Users should weigh these tradeoffs when choosing their acceleration strategy.
Summarising, we can provide a few guiding principles. \code{"compress"} is a good
default that guarantees the "exact" FE estimates and is usually very
efficient (barring data I/O costs and high FE dimensionality). \code{"mundlak"} is
another efficient alternative provided that the CRE estimand is acceptable
(don't be alarmed if your coefficients are not identical). Finally, the
\code{"demean"} and \code{"moments"} strategies are great for particular use cases
(i.e., balanced panels and cases without FE, respectively).

If this all sounds like too much to think about, don't fret. The good news
is that \code{dbreg} can do a lot (all?) of the deciding for you. Specifically, it
will invoke an \code{"auto"} heuristic behind the scenes if a user does not
provide an explicit acceleration strategy. Working through the heuristic
logic does impose some additional overhead, but this should be negligible in
most cases (certainly compared to the overall time savings). The \code{"auto"}
heuristic is as follows:
\itemize{
\item IF no FE AND (any continuous regressor OR poor compression ratio OR too big
compressed data) THEN \code{"moments"}.
\item ELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN
\code{"demean"}.
\item ELSE IF 2 FE AND (poor compression ratio OR too big compressed data):
\itemize{
\item IF balanced panel THEN \code{"demean"}.
\item ELSE error (exact TWFE infeasible; user must explicitly choose
\code{"compress"} or \code{"mundlak"}).
}
\item ELSE THEN \code{"compress"}.
}

\emph{Tip: set \code{dbreg(..., verbose = TRUE)} to print information about the auto
strategy decision criteria.}
}

\examples{
#
## In-memory data ----

# We can pass in-memory R data frames to an ephemeral DuckDB connection via
# the `data` argument. This is convenient for small(er) datasets and demos.

# Default "compress" strategy reduces the data to 4 rows before running OLS
dbreg(weight ~ Diet, data = ChickWeight)

# Compare with lm
summary(lm(weight ~ Diet, data = ChickWeight))$coefficients

# Add "fixed effects" after a `|` 
dbreg(weight ~ Time | Diet, data = ChickWeight)

# "robust" SEs can also be computed using a sufficient statistics approach
dbreg(weight ~ Time | Diet, data = ChickWeight, vcov = "hc1")
dbreg(weight ~ Time | Diet, data = ChickWeight, vcov = ~Chick)

# Different acceleration strategies + specifications
dbreg(weight ~ Time | Diet, data = ChickWeight, strategy = "demean")
dbreg(weight ~ Time | Diet, data = ChickWeight, strategy = "mundlak")
dbreg(weight ~ Time | Diet + Chick, data = ChickWeight, strategy = "mundlak") # two-way Mundlak
dbreg(weight ~ Time, data = ChickWeight, strategy = "moments") # no FEs
# etc.

# Interactions: does the effect of Time vary by Diet?
# (Diet main effects are collinear with Chick FE, so these drop out)
dbreg(weight ~ Time * Diet | Chick, data = ChickWeight)

#
## DBI connection ----

# For persistent databases or more control, use the `conn` + `table` args.
# Again, we use DuckDB below but any other DBI-supported backend should work
# too (e.g., odbc, bigrquery, noctua (AWS Athena),  etc.) See:
# https://r-dbi.org/backends/

library(DBI)
con = dbConnect(duckdb::duckdb())
dbWriteTable(con, "cw", as.data.frame(ChickWeight))

dbreg(weight ~ Time | Diet, conn = con, table = "cw")

# Tip: Rather than creating or writing (temp) tables, use CREATE VIEW to
# define subsets or computed columns without materializing data. This is more
# efficient and especially useful for filtering or adding variables.
dbExecute(
  con,
  "
  CREATE VIEW cw1 AS
  SELECT *
  FROM cw
  WHERE Diet = 1
  "
)
dbreg(weight ~ Time | Chick, conn = con, table = "cw1")

#
## Path to file ----
#
# For file-based data (e.g., parquet), use the path argument.

tmp = tempfile(fileext = ".parquet")
dbExecute(con, sprintf("COPY cw TO '\%s' (FORMAT PARQUET)", tmp))

dbreg(weight ~ Time | Diet, path = tmp)

# Cleanup
dbDisconnect(con)
unlink(tmp)

#
## Big dataset ----

# For a more compelling and appropriate dbreg use-case, i.e. regression on a
# big (~180 million row) dataset of Hive-partioned parquet files, see the
# package website:
# https://grantmcdermott.com/dbreg/
}
\references{
Arkhangelsky, D. & Imbens, G. (2024)
\cite{Fixed Effects and the Generalized Mundlak Estimator}.
The Review of Economic Studies, 91(5), pp. 2545–2571.
Available: https://doi.org/10.1093/restud/rdad089

Mundlak, Y. (1978)
\cite{On the Pooling of Time Series and Cross Section Data}.
Econometrica, 46(1), pp. 69–85.
Available: https://doi.org/10.2307/1913646

Wong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021).
\cite{You Only Compress Once: Optimal Data Compression for Estimating Linear Models.}
arXiv preprint arXiv:2102.11297.
Available: https://doi.org/10.48550/arXiv.2102.11297

Wooldridge, J.M. (2025)
\cite{Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators}.
Empirical Economics, 69, pp. 2545–2587.
Available: https://doi.org/10.1007/s00181-025-02807-z
}
\seealso{
\code{\link[DBI]{dbConnect}} for creating database connections,
\code{\link[duckdb]{duckdb}} for DuckDB-specific connections
}

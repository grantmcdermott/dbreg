% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dbreg.R
\name{dbreg}
\alias{dbreg}
\title{Run a regression on a database backend.}
\usage{
dbreg(
  fml,
  conn = NULL,
  table = NULL,
  data = NULL,
  path = NULL,
  vcov = c("iid", "hc1"),
  strategy = c("auto", "compress", "moments", "mundlak"),
  compress_ratio = 0.001,
  compress_nmax = 1e+06,
  query_only = FALSE,
  data_only = FALSE,
  verbose = TRUE
)
}
\arguments{
\item{fml}{A \code{\link[stats]{formula}} representing the relation to be
estimated. Fixed-effects should be included after a pipe, e.g
\code{fml = y ~ x1 + x2 | fe1 + f2}. Currently, only simple additive terms
are supported (i.e., no interaction terms, transformations or literals).}

\item{conn}{Database connection, e.g. created with
\code{\link[DBI]{dbConnect}}. Can be either persistent (disk-backed) or
ephemeral (in-memory). If no connection is provided, then an ephemeral
\code{\link[duckdb]{duckdb}} connection will be created automatically and
closed before the function exits. Note that a persistent (disk-backed)
database connection is required for larger-than-RAM datasets in order to take
advantage of out-of-core functionality like streaming (where supported).}

\item{table, data, path}{Mutually exclusive arguments for specifying the data
table (object) to be queried. In order of precedence:
\itemize{
\item \code{table}: Character string giving the name of the data table in an
existing (open) database connection.
\item \code{data}: R dataframe that can be copied over to \code{conn} as a temporary
table for querying via the DuckDB query engine. Ignored if \code{table} is
provided.
\item \code{path}: Character string giving a path to the data file(s) on disk, which
will be read into \code{conn}. Internally, this string is passed to the \code{FROM}
query statement, so could (should) include file globbing for
Hive-partitioned datasets, e.g. \code{"mydata/**/.*parquet"}. For more precision,
however, it is recommended to pass the desired database reader function as
part of this string, e.g. \code{"read_parquet('mydata/**/*.parquet')"} for DuckDB;
note the use of single quotes.
Ignored if either \code{table} or \code{data} is provided.
}}

\item{vcov}{Character string denoting the desired type of variance-
covariance correction / standard errors. At present, only "iid" (default) or
"hc1" (heteroskedasticity-consistent) are supported.}

\item{strategy}{Character string indicating the preferred acceleration
strategy. The default \code{"auto"} will pick an optimal strategy based on
internal heuristics. Users can also override with one of the following
explicit strategies: \code{"compress"}, \code{"mundlak"}, or \code{"moments"}. See
the Acceleration Strategies section below for details.}

\item{compress_ratio, compress_nmax}{Numeric(s). Parameters that help to
determine the acceleration \code{strategy} under the default \code{"auto"} option.
\itemize{
\item \code{compress_ratio} defines the compression ratio threshold, i.e. compressed
data size vs. original data size. An estimated compression ratio larger
than this threshold indicates poor compression relative to the desired
level.
\item \code{compress_nmax} defines the maximum allowable size (in rows) of the
compressed dataset that can be serialized into R. Pays heed to the idea
that big data serialization can be costly (esp. for remote databases),
even if we have achieved good compression on top of the original dataset.
}

If both conditions are met, i.e. (1) estimated compression ratio <
\code{compress_ratio} and (2) estimated compressed data size < \code{compress_nmax},
then the \code{"compress"} strategy is used. Otherwise, either the \code{"mundlak"} or
\code{"moments"} strategy will be used, depending on the number of fixed effects.}

\item{query_only}{Logical indicating whether only the underlying compression
SQL query should be returned (i.e., no computation will be performed).
Default is \code{FALSE}.}

\item{data_only}{Logical indicating whether only the compressed dataset
should be returned (i.e., no regression is run). Default is \code{FALSE}.}

\item{verbose}{Logical. Print progress messages to the console? Defaults to
\code{TRUE}.}
}
\value{
A list of class "dbreg" containing various slots, including a table
of coefficients (which the associated print method will display).
}
\description{
Leverages the power of databases to run regressions on very large datasets,
which may not fit into R's memory. Various acceleration strategies allow for
highly efficient computation, while robust standard errors are computed from
sufficient statistics.
}
\section{Acceleration strategies}{


\code{dbreg} offers three primary acceleration (shortcut) strategies:
\enumerate{
\item \code{"compress"}: compress the size of data via a \verb{GROUP BY} operation (using regressors + fixed effects) and then run frequency-weighted least squares on the smaller dataset. This procedure follows the "optimal data compression" strategy proposed by Wang et. al. (2021).
\item \code{"moments"}: calculate sufficient statistics from global means (\eqn{X'X, X'y}), i.e. a single-row data frame computed on the database backend. Limited to cases without fixed effects.
\item \code{"mundlak"}: as per \code{"moments"}, but first subtract group-level means from the observations. Permits at most two fixed-effects (i.e., either demean or double-demean). This procedure follows the "generalized Mundlak estimator" proposed by Arkhangelsky & Imbens (2024).
}

The relative efficiency of each of these strategies depends on the size and
structure of the data, as well the number of unique regressors and
fixed-effects. While the compression approach can yield remarkable
performance gains for "standard" cases, it is less efficient for a true panel
(repeated cross-sections over time), where N >> T. In such cases, it is more
efficient to use a Mundlak-type representation that subtracts group means
first. (Reason: unit and time fixed-effects are typically high dimensional,
but covariate averages are not.)

If the user does not specify an explicit acceleration strategy, then
\code{dbreg} will invoke an \code{"auto"} heuristic behind the scenes. This requires
some additional overhead, but in most cases should be negligible next to the
overall time savings. The heuristic is as follows:
\itemize{
\item IF no fixed-effects AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN \code{"moments"}.
\item ELSE IF 1-2 fixed-effects AND (poor compression ratio OR too big compressed data) THEN \code{"mundlak"}.
\item ELSE THEN \code{"compress"}.
}
}

\examples{

# A not very compelling example using a small iin-memory dataset:
(mod = dbreg(Temp ~ Wind | Month, data = airquality))

# Same result as lm
summary(lm(Temp ~ Wind + factor(Month), data = airquality))

# Aside: dbreg's default print method hides the "nuisance" coefficients
# like the intercept and fixed effect(s). But we can grab them if we want.
print(mod, fes = TRUE)

# Note: for a more compelling and appropriate use-case, i.e. regression on a
# big (~180 million row) dataset of Hive-partioned parquet files, see the
# package website:
# https://github.com/grantmcdermott/dbreg?tab=readme-ov-file#quickstart
}
\references{
Arkhangelsky, D. & Imbens, G. (2024)
\cite{Fixed Effects and the Generalized Mundlak Estimator}.
The Review of Economic Studies, 91(5), pp. 2545â€“2571.
Available: https://doi.org/10.1093/restud/rdad089

Wong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021).
\cite{You Only Compress Once: Optimal Data Compression for Estimating Linear Models.}
arXiv preprint arXiv:2102.11297.
Available: https://doi.org/10.48550/arXiv.2102.11297
}
\seealso{
\code{\link[DBI]{dbConnect}} for creating database connections,
\code{\link[duckdb]{duckdb}} for DuckDB-specific connections
}

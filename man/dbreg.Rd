% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/dbreg.R
\name{dbreg}
\alias{dbreg}
\title{Run a regression on a database backend.}
\usage{
dbreg(
  fml,
  conn = NULL,
  table = NULL,
  data = NULL,
  path = NULL,
  vcov = c("iid", "hc1"),
  strategy = c("auto", "compress", "moments", "demean", "within", "mundlak"),
  compress_ratio = NULL,
  compress_nmax = 1e+06,
  query_only = FALSE,
  data_only = FALSE,
  drop_missings = TRUE,
  verbose = TRUE
)
}
\arguments{
\item{fml}{A \code{\link[stats]{formula}} representing the relation to be
estimated. Fixed effects should be included after a pipe, e.g
\code{fml = y ~ x1 + x2 | fe1 + f2}. Currently, only simple additive terms
are supported (i.e., no interaction terms, transformations or literals).}

\item{conn}{Database connection, e.g. created with
\code{\link[DBI]{dbConnect}}. Can be either persistent (disk-backed) or
ephemeral (in-memory). If no connection is provided, then an ephemeral
\code{\link[duckdb]{duckdb}} connection will be created automatically and
closed before the function exits. Note that a persistent (disk-backed)
database connection is required for larger-than-RAM datasets in order to take
advantage of out-of-core functionality like streaming (where supported).}

\item{table, data, path}{Mutually exclusive arguments for specifying the data
table (object) to be queried. In order of precedence:
\itemize{
\item \code{table}: Character string giving the name of the data table in an
existing (open) database connection.
\item \code{data}: R dataframe that can be copied over to \code{conn} as a temporary
table for querying via the DuckDB query engine. Ignored if \code{table} is
provided.
\item \code{path}: Character string giving a path to the data file(s) on disk, which
will be read into \code{conn}. Internally, this string is passed to the \code{FROM}
query statement, so could (should) include file globbing for
Hive-partitioned datasets, e.g. \code{"mydata/**/.*parquet"}. For more precision,
however, it is recommended to pass the desired database reader function as
part of this string, e.g. \code{"read_parquet('mydata/**/*.parquet')"} for DuckDB;
note the use of single quotes.
Ignored if either \code{table} or \code{data} is provided.
}}

\item{vcov}{Character string denoting the desired type of variance-
covariance correction / standard errors. At present, only \code{"iid"} (default)
or \code{"hc1"} (heteroskedasticity-consistent) are supported. Note that the
latter requires a second pass over the data unless \code{strategy = "compress"} to
construct the residuals.}

\item{strategy}{Character string indicating the preferred acceleration
strategy. The default \code{"auto"} will pick an optimal strategy based on
internal heuristics. Users can also override with one of the following
explicit strategies: \code{"compress"}, \code{"demean"} (alias: \code{"within"}),
\code{"mundlak"}, or \code{"moments"}. See the Acceleration Strategies section below
for details.}

\item{compress_ratio, compress_nmax}{Numeric(s). Parameters that help to
determine the acceleration \code{strategy} under the default \code{"auto"} option.
\itemize{
\item \code{compress_ratio} defines the compression ratio threshold, i.e. numeric
in the range \verb{[0,1]} defining the minimum acceptable compressed versus
the original data size. Default value of \code{NULL} means that the threshold
will be automatically determined based on some internal heuristic
(e.g., 0.01 for models without fixed effects).
\item \code{compress_nmax} defines the maximum allowable size (in rows) of the
compressed dataset that can be serialized into R. Pays heed to the idea
that big data serialization can be costly (esp. for remote databases),
even if we have achieved good compression on top of the original dataset.
Default value is 1e6 (i.e., a million rows).
}

See the Acceleration Strategies section below for further details.}

\item{query_only}{Logical indicating whether only the underlying compression
SQL query should be returned (i.e., no computation will be performed).
Default is \code{FALSE}.}

\item{data_only}{Logical indicating whether only the compressed dataset
should be returned (i.e., no regression is run). Default is \code{FALSE}.}

\item{drop_missings}{Logical indicating whether incomplete cases (i.e., rows
where any of the dependent, independent or FE variables are
missing) should be dropped. The default is \code{TRUE}, according with standard
regression software. It is \emph{strongly} recommended not to change this value
unless you are absolutely sure that your data have no missings and you wish
to skip some internal checks. (Even then, it probably isn't worth it.)}

\item{verbose}{Logical. Print auto strategy and progress messages to the
console? Defaults to \code{TRUE}.}
}
\value{
A list of class "dbreg" containing various slots, including a table
of coefficients (which the associated print method will display).
}
\description{
Leverages the power of databases to run regressions on very large datasets,
which may not fit into R's memory. Various acceleration strategies allow for
highly efficient computation, while robust standard errors are computed from
sufficient statistics.
}
\section{Acceleration Strategies}{


\code{dbreg} offers four primary acceleration strategies for estimating regression
results from simplified data representations. Below we use the shorthand
Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition
purposes:
\enumerate{
\item \code{"compress"}: compresses the data via a \verb{GROUP BY} operation (using X and
the FE as groups), before running weighted least squares on this much
smaller dataset:
\deqn{\hat{\beta} = (X_c' W X_c)^{-1} X_c' W Y_c}
where \eqn{W = \text{diag}(n_g)} are the group frequencies. This procedure
follows Wang et al. (2021).
\item \code{"moments"}: computes sufficient statistics (\eqn{X'X, X'y}) directly via
SQL aggregation, returning a single-row result. This solves the standard
OLS normal equations \eqn{\hat{\beta} = (X'X)^{-1}X'y}. Limited to cases
without FE.
\item \code{"demean"} (alias \code{"within"}): subtracts group-level means from both Y and
X before computing sufficient statistics (per the \code{"moments"} strategy).
For example, given unit \eqn{i} and time \eqn{t} FE, we apply double
demeaning:
\deqn{\ddot{Y}_{it} = \beta \ddot{X}_{it} + \varepsilon_{it}}
where \eqn{\ddot{X} = X - \bar{X}_i - \bar{X}_t + \bar{X}}. This
(single-pass) within transformation is algebraically equivalent to the
fixed effects projection---i.e., Frisch-Waugh-Lovell partialling out---in
the presence or a single FE. It is also identical for the two-way FE
(TWFE) case if your panel is balanced. For unbalanced two-way panels,
however, the double demeaning strategy is not algebraically equivalent to
the fixed effects projection and therefore does not recover the exact TWFE
coefficients. Moreover, note that this \code{"demean"} strategy permits at most
two FE.
\item \code{"mundlak"}: a generalized Mundlak (1978), or correlated random effects
(CRE) estimator that regresses Y on X plus group means of X:
\deqn{Y_{it} = \alpha + \beta X_{it} + \gamma \bar{X}_i + \varepsilon_{it} \quad \text{(one-way)}}
\deqn{Y_{it} = \alpha + \beta X_{it} + \gamma \bar{X}_{i} + \delta \bar{X}_{t} + \varepsilon_{it} \quad \text{(two-way, etc.)}}
Unlike \code{"demean"}, Y is not transformed, so predictions are on the
original scale. Supports any number of FE and works correctly for any
panel structure (balanced or unbalanced). However, note that CRE is a
\emph{different model} from FE: while coefficients are asymptotically
equivalent under certain assumptions, they will generally differ in
finite samples.
}

The relative efficiency of each of these strategies depends on the size and
structure of the data, as well the number of unique regressors and FE. For
(quote unquote) "standard" cases, the \code{"compress"} strategy can yield
remarkable performance gains and should justifiably be viewed as a good
default. However, the compression approach tends to be less efficient for
true panels (repeated cross-sections over time), where N >> T. In such
cases, it can be more efficient to use a demeaning strategy that first
controls for (e.g. subtracts) group means, before computing sufficient
statistics on the aggregated data. The reason for this is that time and unit
FE are typically high dimensional, but covariate averages are not; see
Arkhangelsky & Imbens (2024).

However, the demeaning approaches invite tradeoffs of their own. For example,
the double demeaning transformation of the \code{"demean"} strategy does not
obtain exact TWFE results in unbalanced panels, and it is also limited to at
most two FE. Conversely, the \code{"mundlak"} (CRE) strategy obtains consistent
coefficients regardless of panel structure and FE count, but at the "cost" of
recovering a different estimand. (It is a different model to TWFE, after
all.)

Users should weigh these tradeoffs when choosing their accleration strategy.
Summarising, we can provide a few guiding principles. \code{"compress"} is a good
default that guarantees the "exact" FE estimates and is usually very
efficient (barring data I/O costs and high FE dimensionality). \code{"mundlak"} is
another efficient alternative provided that the CRE estimand is acceptable
(don't be alarmed if your coefficients are not identical). Finally, the
\code{"demean"} and \code{"moments"} strategies are great for particular use cases
(i.e., balanced panels and cases without FE, respectively).

If this all sounds like too much to think about, don't fret. The good news
is that \code{dbreg} can do a lot (all?) of the deciding for you. Specifically, it
will invoke an \code{"auto"} heuristic behind the scenes if a user does not
provide an explicit acceleration strategy. Working through the heuristic
logic does impose some additional overhead, but this should be negligible in
most cases (certainly compared to the overall time savings). The \code{"auto"}
heuristic is as follows:
\itemize{
\item IF no FE AND (any continuous regressor OR poor compression ratio OR too big
compressed data) THEN \code{"moments"}.
\item ELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN
\code{"demean"}.
\item ELSE IF 2 FE AND (poor compression ratio OR too big compressed data):
\itemize{
\item IF balanced panel THEN \code{"demean"}.
\item ELSE error (exact TWFE infeasible; user must explicitly choose
\code{"compress"} or \code{"mundlak"}).
}
\item ELSE THEN \code{"compress"}.
}

\emph{Tip: set \code{dbreg(..., verbose = TRUE)} to print information about the auto
strategy decision criteria.}
}

\examples{

# A not very compelling example using a small in-memory dataset:
(mod = dbreg(Temp ~ Wind | Month, data = airquality))

# Same result as lm
summary(lm(Temp ~ Wind + factor(Month), data = airquality))

# Aside: dbreg's default print method hides the "nuisance" coefficients
# like the intercept and fixed effect(s). But we can grab them if we want.
print(mod, fes = TRUE)

# Note: for a more compelling and appropriate use-case, i.e. regression on a
# big (~180 million row) dataset of Hive-partioned parquet files, see the
# package website:
# https://github.com/grantmcdermott/dbreg?tab=readme-ov-file#quickstart
}
\references{
Arkhangelsky, D. & Imbens, G. (2024)
\cite{Fixed Effects and the Generalized Mundlak Estimator}.
The Review of Economic Studies, 91(5), pp. 2545–2571.
Available: https://doi.org/10.1093/restud/rdad089

Mundlak, Y. (1978)
\cite{On the Pooling of Time Series and Cross Section Data}.
Econometrica, 46(1), pp. 69–85.
Available: https://doi.org/10.2307/1913646

Wong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021).
\cite{You Only Compress Once: Optimal Data Compression for Estimating Linear Models.}
arXiv preprint arXiv:2102.11297.
Available: https://doi.org/10.48550/arXiv.2102.11297
}
\seealso{
\code{\link[DBI]{dbConnect}} for creating database connections,
\code{\link[duckdb]{duckdb}} for DuckDB-specific connections
}

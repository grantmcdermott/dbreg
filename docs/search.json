[
  {
    "objectID": "CITATION.html",
    "href": "CITATION.html",
    "title": "Citation",
    "section": "",
    "text": "Citation\nTo cite package ‚Äòdbreg‚Äô in publications use:\n\nMcDermott G, Brand J (2025). dbreg: Fast Regressions on Database Backends. R package version 0.0.2.99, https://grantmcdermott.com/dbreg/."
  },
  {
    "objectID": "NEWS.html",
    "href": "NEWS.html",
    "title": "News",
    "section": "",
    "text": "This NEWS file is best viewed on our website.\n\n\nWebsite\n\nWe finally have a package website: grantmcdermott.com/dbreg üéâ\n\nBreaking changes\n\nThe behaviour of the \"mundlak\" strategy has been changed. (#24)\n\nThe old \"mundlak\" strategy is remapped to the new \"demean\" (alias \"within\") strategy, to better reflect the fact that this strategy invokes a (double) demeaning transformation. Users who want the old behaviour should thus use \"demean\" instead of \"mundlak\".\nSimultaneously, we now provide a revised \"mundlak\" strategy that implements a ‚Äútrue‚Äù Mundlak/CRE estimator; see New features below.\n\nFor estimations with two fixed effects on unbalanced panels, strategy=\"auto\" now errors when the compression limits are exceeded. It does this to avoid silently selecting a different estimand (i.e., Mundlak/CRE instead of TWFE). For these ambiguous cases, users will now be prompted to explicitly choose \"compress\" (with higher limits) or \"mundlak\" (different model and thus potentially different coefficients). (#24)\nThe default verbose behaviour is changed to FALSE. Users can revert to the old behaviour for a single call (i.e., dbreg(..., verbose = TRUE)), or set it globally (i.e., options(dbreg.verbose = TRUE)). (#33)\nTechnically not a breaking change, since we currently support backwards compatibility, but several minor arguments have been renamed/superseded. (#34)\n\nquery_only -&gt; sql_only (in dbreg)\nfes -&gt; fe (in print.dbreg, coef.dbreg, confint.dbreg, etc.)\n\n\nMajor new features\n\nWe have added new/revised acceleration strategies. (#24)\n\nThe \"demean\" (alias \"within\") strategy implements a (double) demeaning transformation and is particularly suited to (balanced) panels with one or two fixed effects. Note the underlying query is the same as the old \"mundlak\" strategy, which was somewhat erroneously named. Speaking of which‚Ä¶\nThe revised \"mundlak\" strategy now implements a ‚Äútrue‚Äù Mundlak/CRE (correlated random effects) estimator by regressing Y on X plus group means of X. Unlike the \"demean\" strategy (above), this revised \"mundlak\" model obtains consistent coefficients regardless of panel structure (incl. unbalanced panels) and supports any number of fixed effects. However, users should note that Mundlak/CRE is a different model from ‚Äúvanilla‚Äù fixed effects‚Äîalbeit asymptotically equivalent under certain assumptions‚Äîand may obtain different coefficients as a result.\nPlease consult the expanded Acceleration Strategies section in the ?dbreg helpfile for technical details.\n\nAdd support for clustered standard errors. Follows the fixest API:\ndbreg(..., vcov = ~cluster_var)\nPlease note that these clustered SEs are computed analytically (not bootstrapped) and should thus add minimal overhead to your regressions. See the updated README for examples. (#29)\nAdded support for binscatter regressions on database backends via the dbbinsreg() function. This function attempts to mimic the main API of the binsreg package (modulo some simplifications), but should be much faster on big datasets. (#32)\nAdded support for interaction terms in formulas (e.g., y ~ x1 * x2 | fe). Interactions are expanded to SQL via the new (user-facing) sql_model_matrix() function and work with all acceleration strategies. Collinear terms (e.g., treatment main effects absorbed by unit FEs in DiD models) are automatically detected and dropped. (#48)\n\nOther new features\n\nThe \"auto\" strategy logic now considers a compress_nmax threshold, which governs the maximum allowable size of the compressed data object (default threshold = 1 million rows). This additional guardrail is intended to avoid cases where the \"compress\" strategy satisfies the compress_ratio threshold, but could still return a prohibitively large dataset. The most common example would be querying a massive dataset on a remote database, where network latency makes data I/O transfer expensive, even though we‚Äôve achieved good compression relative to the original data size. (#10)\n\nAside: Improved documentation and messaging (when verbose = TRUE) should also help users understand the \"auto\" strategy decision tree.\n\nEnabled weights for double demean (within) specification. (#13)\nEsimations now report some goodness-of-fit statistics like R2 and RMSE, powered by the (user-facing) gof() function. (#21)\nAdded support for various *.dbreg methods (#21, #30):\n\nFrom stats: coef(), confint(), predict(), and vcov().\nFrom broom/generics: tidy() and glance(). These also enable post-processing operations like exporting results to coefficient tables via modelsummary::msummary(). Thanks to @HariharanJayashankar for the request in #20.\n\nBetter documentation. (#28, #36, #43, and various other PRs)\n\nBug fixes\n\nAdded QR decomposition fallback for regression calculations, for cases where the default Cholesky solver fails. (#7)\nImproved integration for running regressions on AWS Athena datasets via the noctua package/driver. (#8)\nAutomatically drop incomplete cases (i.e., missing values) prior to any aggregation steps, avoiding mismatched matrices during estimation. (#19)\nUser-specified compress_ratio values should now bind in all cases. Previously, these could sometimes be silently ignored due to internal overrides. Also, clarify in the argument documentation that the default (automatic) compress_ratio threshold can vary based on heuristics related to model structure. (#25)\nCorrectly estimate HC1 standard errors for the \"moments\", \"demean\", and \"mundlak\" strategies. While the old analytic HC1 approach worked (and still does) for the \"compress\" case, it led to misleading SEs for these other strategies. The fix does impose some additional computational overhead, since it requires a second pass over the data to calculate the individual errors and ‚Äúmeat‚Äù of the sandwich matrix. But testing suggests that this leads to a &lt;2 increase in total estimation time, which seems a reasonable tradeoff for heteroskedastic-robust SEs. (#27)\nFixed a bug for in-memory dbreg(..., data = &lt;data&gt;) cases, where factor variables in X could cause the \"compress\" strategy to fail. (#41)\n\nInternals\n\nAdded unit testing framework using tinytest. (#16)\nAdded GitHub Actions CI. (#18)\n\n\n\n\nIMPORTANT BREAKING CHANGE:\nThe package has been renamed to dbreg to better reflect the fact that it supports multiple database backends. (#4)\nOther breaking changes\n\nThe default vcov is now ‚Äúiid‚Äù. (#2 @grantmcdermott)\n\nNew features\n\nThe new dbreg(..., strategy = &lt;strategy&gt;) argument allows users to choose between different acceleration strategies for efficient computation of the regression coefficients and standard errors. This includes \"compress\" (the old default), as well as \"mundlak\" or \"moments\". The latter two strategies are newly introduced in dbreg v0.0.2 and offer important advantages in the case of true panel data. If an explicit strategy is not provided by the user, then dbreg() will invoke some internal heuristics to determine the optimal strategy based on the size and structure of the data. (#2 @jamesbrandecon and @grantmcdermott)\n\nProject\n\n@jamesbrandecon has joined the project as a core contributor.\n\n\n\n\n\nInitial GitHub release."
  },
  {
    "objectID": "NEWS.html#dev-version",
    "href": "NEWS.html#dev-version",
    "title": "News",
    "section": "",
    "text": "Website\n\nWe finally have a package website: grantmcdermott.com/dbreg üéâ\n\nBreaking changes\n\nThe behaviour of the \"mundlak\" strategy has been changed. (#24)\n\nThe old \"mundlak\" strategy is remapped to the new \"demean\" (alias \"within\") strategy, to better reflect the fact that this strategy invokes a (double) demeaning transformation. Users who want the old behaviour should thus use \"demean\" instead of \"mundlak\".\nSimultaneously, we now provide a revised \"mundlak\" strategy that implements a ‚Äútrue‚Äù Mundlak/CRE estimator; see New features below.\n\nFor estimations with two fixed effects on unbalanced panels, strategy=\"auto\" now errors when the compression limits are exceeded. It does this to avoid silently selecting a different estimand (i.e., Mundlak/CRE instead of TWFE). For these ambiguous cases, users will now be prompted to explicitly choose \"compress\" (with higher limits) or \"mundlak\" (different model and thus potentially different coefficients). (#24)\nThe default verbose behaviour is changed to FALSE. Users can revert to the old behaviour for a single call (i.e., dbreg(..., verbose = TRUE)), or set it globally (i.e., options(dbreg.verbose = TRUE)). (#33)\nTechnically not a breaking change, since we currently support backwards compatibility, but several minor arguments have been renamed/superseded. (#34)\n\nquery_only -&gt; sql_only (in dbreg)\nfes -&gt; fe (in print.dbreg, coef.dbreg, confint.dbreg, etc.)\n\n\nMajor new features\n\nWe have added new/revised acceleration strategies. (#24)\n\nThe \"demean\" (alias \"within\") strategy implements a (double) demeaning transformation and is particularly suited to (balanced) panels with one or two fixed effects. Note the underlying query is the same as the old \"mundlak\" strategy, which was somewhat erroneously named. Speaking of which‚Ä¶\nThe revised \"mundlak\" strategy now implements a ‚Äútrue‚Äù Mundlak/CRE (correlated random effects) estimator by regressing Y on X plus group means of X. Unlike the \"demean\" strategy (above), this revised \"mundlak\" model obtains consistent coefficients regardless of panel structure (incl. unbalanced panels) and supports any number of fixed effects. However, users should note that Mundlak/CRE is a different model from ‚Äúvanilla‚Äù fixed effects‚Äîalbeit asymptotically equivalent under certain assumptions‚Äîand may obtain different coefficients as a result.\nPlease consult the expanded Acceleration Strategies section in the ?dbreg helpfile for technical details.\n\nAdd support for clustered standard errors. Follows the fixest API:\ndbreg(..., vcov = ~cluster_var)\nPlease note that these clustered SEs are computed analytically (not bootstrapped) and should thus add minimal overhead to your regressions. See the updated README for examples. (#29)\nAdded support for binscatter regressions on database backends via the dbbinsreg() function. This function attempts to mimic the main API of the binsreg package (modulo some simplifications), but should be much faster on big datasets. (#32)\nAdded support for interaction terms in formulas (e.g., y ~ x1 * x2 | fe). Interactions are expanded to SQL via the new (user-facing) sql_model_matrix() function and work with all acceleration strategies. Collinear terms (e.g., treatment main effects absorbed by unit FEs in DiD models) are automatically detected and dropped. (#48)\n\nOther new features\n\nThe \"auto\" strategy logic now considers a compress_nmax threshold, which governs the maximum allowable size of the compressed data object (default threshold = 1 million rows). This additional guardrail is intended to avoid cases where the \"compress\" strategy satisfies the compress_ratio threshold, but could still return a prohibitively large dataset. The most common example would be querying a massive dataset on a remote database, where network latency makes data I/O transfer expensive, even though we‚Äôve achieved good compression relative to the original data size. (#10)\n\nAside: Improved documentation and messaging (when verbose = TRUE) should also help users understand the \"auto\" strategy decision tree.\n\nEnabled weights for double demean (within) specification. (#13)\nEsimations now report some goodness-of-fit statistics like R2 and RMSE, powered by the (user-facing) gof() function. (#21)\nAdded support for various *.dbreg methods (#21, #30):\n\nFrom stats: coef(), confint(), predict(), and vcov().\nFrom broom/generics: tidy() and glance(). These also enable post-processing operations like exporting results to coefficient tables via modelsummary::msummary(). Thanks to @HariharanJayashankar for the request in #20.\n\nBetter documentation. (#28, #36, #43, and various other PRs)\n\nBug fixes\n\nAdded QR decomposition fallback for regression calculations, for cases where the default Cholesky solver fails. (#7)\nImproved integration for running regressions on AWS Athena datasets via the noctua package/driver. (#8)\nAutomatically drop incomplete cases (i.e., missing values) prior to any aggregation steps, avoiding mismatched matrices during estimation. (#19)\nUser-specified compress_ratio values should now bind in all cases. Previously, these could sometimes be silently ignored due to internal overrides. Also, clarify in the argument documentation that the default (automatic) compress_ratio threshold can vary based on heuristics related to model structure. (#25)\nCorrectly estimate HC1 standard errors for the \"moments\", \"demean\", and \"mundlak\" strategies. While the old analytic HC1 approach worked (and still does) for the \"compress\" case, it led to misleading SEs for these other strategies. The fix does impose some additional computational overhead, since it requires a second pass over the data to calculate the individual errors and ‚Äúmeat‚Äù of the sandwich matrix. But testing suggests that this leads to a &lt;2 increase in total estimation time, which seems a reasonable tradeoff for heteroskedastic-robust SEs. (#27)\nFixed a bug for in-memory dbreg(..., data = &lt;data&gt;) cases, where factor variables in X could cause the \"compress\" strategy to fail. (#41)\n\nInternals\n\nAdded unit testing framework using tinytest. (#16)\nAdded GitHub Actions CI. (#18)"
  },
  {
    "objectID": "NEWS.html#dbreg-0.0.2",
    "href": "NEWS.html#dbreg-0.0.2",
    "title": "News",
    "section": "",
    "text": "IMPORTANT BREAKING CHANGE:\nThe package has been renamed to dbreg to better reflect the fact that it supports multiple database backends. (#4)\nOther breaking changes\n\nThe default vcov is now ‚Äúiid‚Äù. (#2 @grantmcdermott)\n\nNew features\n\nThe new dbreg(..., strategy = &lt;strategy&gt;) argument allows users to choose between different acceleration strategies for efficient computation of the regression coefficients and standard errors. This includes \"compress\" (the old default), as well as \"mundlak\" or \"moments\". The latter two strategies are newly introduced in dbreg v0.0.2 and offer important advantages in the case of true panel data. If an explicit strategy is not provided by the user, then dbreg() will invoke some internal heuristics to determine the optimal strategy based on the size and structure of the data. (#2 @jamesbrandecon and @grantmcdermott)\n\nProject\n\n@jamesbrandecon has joined the project as a core contributor."
  },
  {
    "objectID": "NEWS.html#duckreg-0.0.1",
    "href": "NEWS.html#duckreg-0.0.1",
    "title": "News",
    "section": "",
    "text": "Initial GitHub release."
  },
  {
    "objectID": "vignettes/intro.html",
    "href": "vignettes/intro.html",
    "title": "Introduction to dbreg",
    "section": "",
    "text": "COMING SOON.\nIn the meantime, please take a look at the README examples. The ?dbreg helpfile also contains a lot of useful information, especially concerning the various acceleration strategies."
  },
  {
    "objectID": "man/predict.dbreg.html",
    "href": "man/predict.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Predict method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\npredict(\n  object,\n  newdata = NULL,\n  interval = c(\"none\", \"confidence\", \"prediction\"),\n  level = 0.95,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nnewdata\n\n\nData frame for predictions. Required for objects that were estimated using the ‚Äò\"mundlak\"‚Äô and ‚Äò\"moments\"‚Äô strategies, since ‚Äòdbreg‚Äô does not retain any data for these estimations.\n\n\n\n\ninterval\n\n\nType of interval to compute: ‚Äò\"none\"‚Äô (default), ‚Äò\"confidence\"‚Äô, or ‚Äò\"prediction\"‚Äô. Note that ‚Äò\"confidence\"‚Äô intervals reflect uncertainty in the estimated mean, while ‚Äò\"prediction\"‚Äô intervals additionally account for residual variance. See predict.lm for details.\n\n\n\n\nlevel\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\nPredicting on ‚Äòdbreg‚Äô objects should generally work as expected. However, predictions from ‚Äò\"demean\"‚Äô strategy models carry two important caveats:\n\nPredictions require group means to transform back to the original scale. If ‚Äònewdata‚Äô contains the outcome variable, group means are computed from ‚Äònewdata‚Äô and used to return level predictions. If the outcome is absent, within-group predictions (deviations from group means) are returned instead, with a message.\nConfidence/prediction intervals are not supported. A demeaned model cannot account for uncertainty in the fixed-effects (since these were absorbed at estimation time), which in turn would yield intervals that are too narrow. Requesting intervals for ‚Äò\"demean\"‚Äô strategy models will return point predictions with a message. Users should re-estimate with a different strategy if intervals are needed.\n\n\n\n\n[dbreg()] for examples.\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "predict.dbreg"
    ]
  },
  {
    "objectID": "man/predict.dbreg.html#predict-method-for-dbreg-objects",
    "href": "man/predict.dbreg.html#predict-method-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Predict method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\npredict(\n  object,\n  newdata = NULL,\n  interval = c(\"none\", \"confidence\", \"prediction\"),\n  level = 0.95,\n  ...\n)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nnewdata\n\n\nData frame for predictions. Required for objects that were estimated using the ‚Äò\"mundlak\"‚Äô and ‚Äò\"moments\"‚Äô strategies, since ‚Äòdbreg‚Äô does not retain any data for these estimations.\n\n\n\n\ninterval\n\n\nType of interval to compute: ‚Äò\"none\"‚Äô (default), ‚Äò\"confidence\"‚Äô, or ‚Äò\"prediction\"‚Äô. Note that ‚Äò\"confidence\"‚Äô intervals reflect uncertainty in the estimated mean, while ‚Äò\"prediction\"‚Äô intervals additionally account for residual variance. See predict.lm for details.\n\n\n\n\nlevel\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\nPredicting on ‚Äòdbreg‚Äô objects should generally work as expected. However, predictions from ‚Äò\"demean\"‚Äô strategy models carry two important caveats:\n\nPredictions require group means to transform back to the original scale. If ‚Äònewdata‚Äô contains the outcome variable, group means are computed from ‚Äònewdata‚Äô and used to return level predictions. If the outcome is absent, within-group predictions (deviations from group means) are returned instead, with a message.\nConfidence/prediction intervals are not supported. A demeaned model cannot account for uncertainty in the fixed-effects (since these were absorbed at estimation time), which in turn would yield intervals that are too narrow. Requesting intervals for ‚Äò\"demean\"‚Äô strategy models will return point predictions with a message. Users should re-estimate with a different strategy if intervals are needed.\n\n\n\n\n[dbreg()] for examples.\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "predict.dbreg"
    ]
  },
  {
    "objectID": "man/vcov.dbreg.html",
    "href": "man/vcov.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Variance-covariance matrix for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nvcov(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "vcov.dbreg"
    ]
  },
  {
    "objectID": "man/vcov.dbreg.html#variance-covariance-matrix-for-dbreg-objects",
    "href": "man/vcov.dbreg.html#variance-covariance-matrix-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Variance-covariance matrix for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nvcov(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused).\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "vcov.dbreg"
    ]
  },
  {
    "objectID": "man/coef.dbreg.html",
    "href": "man/coef.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Extract coefficients from dbreg objects\n\n\n\n## S3 method for class 'dbreg'\ncoef(object, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "coef.dbreg"
    ]
  },
  {
    "objectID": "man/coef.dbreg.html#extract-coefficients-from-dbreg-objects",
    "href": "man/coef.dbreg.html#extract-coefficients-from-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Extract coefficients from dbreg objects\n\n\n\n## S3 method for class 'dbreg'\ncoef(object, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "coef.dbreg"
    ]
  },
  {
    "objectID": "man/plot.dbbinsreg.html",
    "href": "man/plot.dbbinsreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Visualizes binned regression results from dbbinsreg. Plots dots at bin means with optional confidence intervals and/or confidence bands, and optionally overlays a smooth line if computed. Uses tinyplot for rendering but works with both plot() and tinyplot() generics.\n\n\n\n## S3 method for class 'dbbinsreg'\nplot(\n  x,\n  type = NULL,\n  ci = TRUE,\n  cb = TRUE,\n  line = TRUE,\n  lty = 1,\n  theme = \"basic\",\n  ...\n)\n\n## S3 method for class 'dbbinsreg'\ntinyplot(\n  x,\n  type = NULL,\n  ci = TRUE,\n  cb = TRUE,\n  line = TRUE,\n  lty = 1,\n  theme = \"basic\",\n  ...\n)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\ntype\n\n\nThe type of plot. If NULL (the default), then the type will be inferred based on the underlying object (e.g, ‚Äúpointrange‚Äù for points with confidence intervals).\n\n\n\n\nci\n\n\nLogical. Show confidence intervals for dots? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Show confidence bands as a ribbon? Default is TRUE if available in the object.\n\n\n\n\nline\n\n\nLogical. Show the line overlay if available? Default is TRUE.\n\n\n\n\nlty\n\n\nInteger or character string. Line type for line overlay.\n\n\n\n\ntheme\n\n\nCharacter string. One of the valid plot themes supported by tinytheme. The default ‚Äúbasic‚Äù theme is a light adaptation of the standard base graphics aesthetic, featuring filled points and a background grid. Various other themes are supported (e.g., ‚Äúclean‚Äù, ‚Äúminimal‚Äù, ‚Äúclassic‚Äù, etc.), while passing NULL switches the theme off entirely.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments passed to tinyplot, e.g.¬†main, sub, file, etc.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# You can pass additional plotting arguments via ... to (tiny)plot.dbbinsreg\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10,\n          main = \"A simple binscatter example\", theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Alternatively, save the object and plot later\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, plot = FALSE)\nplot(bs, main = \"Same example, different theme\", theme = \"classic\")\n\n\n\n\n\n\n\n# Piecewise linear (p = 1), no smoothness (s = 0)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 21 (compressed)\n\n# Piecewise linear (p = 1) with continuity (s = 1)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 (quantile-spaced)\nN = 578\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 (quantile-spaced)\nN = 578\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\n# (We'll also add a line and change the theme for a nicer plot)\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE,\n          line = c(1, 1), theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10,\n          theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)",
    "crumbs": [
      "Reference",
      "Base methods",
      "plot.dbbinsreg"
    ]
  },
  {
    "objectID": "man/plot.dbbinsreg.html#plot-method-for-dbbinsreg-objects",
    "href": "man/plot.dbbinsreg.html#plot-method-for-dbbinsreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Visualizes binned regression results from dbbinsreg. Plots dots at bin means with optional confidence intervals and/or confidence bands, and optionally overlays a smooth line if computed. Uses tinyplot for rendering but works with both plot() and tinyplot() generics.\n\n\n\n## S3 method for class 'dbbinsreg'\nplot(\n  x,\n  type = NULL,\n  ci = TRUE,\n  cb = TRUE,\n  line = TRUE,\n  lty = 1,\n  theme = \"basic\",\n  ...\n)\n\n## S3 method for class 'dbbinsreg'\ntinyplot(\n  x,\n  type = NULL,\n  ci = TRUE,\n  cb = TRUE,\n  line = TRUE,\n  lty = 1,\n  theme = \"basic\",\n  ...\n)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\ntype\n\n\nThe type of plot. If NULL (the default), then the type will be inferred based on the underlying object (e.g, ‚Äúpointrange‚Äù for points with confidence intervals).\n\n\n\n\nci\n\n\nLogical. Show confidence intervals for dots? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Show confidence bands as a ribbon? Default is TRUE if available in the object.\n\n\n\n\nline\n\n\nLogical. Show the line overlay if available? Default is TRUE.\n\n\n\n\nlty\n\n\nInteger or character string. Line type for line overlay.\n\n\n\n\ntheme\n\n\nCharacter string. One of the valid plot themes supported by tinytheme. The default ‚Äúbasic‚Äù theme is a light adaptation of the standard base graphics aesthetic, featuring filled points and a background grid. Various other themes are supported (e.g., ‚Äúclean‚Äù, ‚Äúminimal‚Äù, ‚Äúclassic‚Äù, etc.), while passing NULL switches the theme off entirely.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments passed to tinyplot, e.g.¬†main, sub, file, etc.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# You can pass additional plotting arguments via ... to (tiny)plot.dbbinsreg\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10,\n          main = \"A simple binscatter example\", theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Alternatively, save the object and plot later\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, plot = FALSE)\nplot(bs, main = \"Same example, different theme\", theme = \"classic\")\n\n\n\n\n\n\n\n# Piecewise linear (p = 1), no smoothness (s = 0)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 21 (compressed)\n\n# Piecewise linear (p = 1) with continuity (s = 1)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 (quantile-spaced)\nN = 578\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 (quantile-spaced)\nN = 578\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\n# (We'll also add a line and change the theme for a nicer plot)\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE,\n          line = c(1, 1), theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10,\n          theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)",
    "crumbs": [
      "Reference",
      "Base methods",
      "plot.dbbinsreg"
    ]
  },
  {
    "objectID": "man/print.dbbinsreg.html",
    "href": "man/print.dbbinsreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbbinsreg objects (binsreg-compatible format)\n\n\n\n## S3 method for class 'dbbinsreg'\nprint(x, ...)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments passed to print"
  },
  {
    "objectID": "man/print.dbbinsreg.html#print-method-for-dbbinsreg-objects-binsreg-compatible-format",
    "href": "man/print.dbbinsreg.html#print-method-for-dbbinsreg-objects-binsreg-compatible-format",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbbinsreg objects (binsreg-compatible format)\n\n\n\n## S3 method for class 'dbbinsreg'\nprint(x, ...)\n\n\n\n\n\n\n\nx\n\n\nA dbbinsreg object\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments passed to print"
  },
  {
    "objectID": "man/confint.dbreg.html",
    "href": "man/confint.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Confidence intervals for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nconfint(object, parm, level = 0.95, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nparm\n\n\na specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.\n\n\n\n\nlevel\n\n\nthe confidence level required. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "confint.dbreg"
    ]
  },
  {
    "objectID": "man/confint.dbreg.html#confidence-intervals-for-dbreg-objects",
    "href": "man/confint.dbreg.html#confidence-intervals-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Confidence intervals for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nconfint(object, parm, level = 0.95, fe = FALSE, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\nparm\n\n\na specification of which parameters are to be given confidence intervals, either a vector of numbers or a vector of names. If missing, all parameters are considered.\n\n\n\n\nlevel\n\n\nthe confidence level required. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be included? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n\n# coefficients\ncoef(mod)\n\n     Wind \n-0.743388 \n\ncoef(mod, fe = TRUE)  # include fixed effects\n\n(Intercept)        Wind      Month6      Month7      Month8      Month9 \n  74.188474   -0.743388   12.543643   16.362079   16.316286   10.279216 \n\n# confidence intervals\nconfint(mod)\n\n         2.5 %     97.5 %\nWind -1.037433 -0.4493427\n\n# variance-covariance matrix\nvcov(mod)\n\n            (Intercept)        Wind      Month6      Month7      Month8\n(Intercept)   4.2205624 -0.25730874 -1.57885933 -1.91972424 -1.95790554\nWind         -0.2573087  0.02213869  0.03001816  0.05934598  0.06263108\nMonth6       -1.5788593  0.03001816  2.54164270  1.31043885  1.31489316\nMonth7       -1.9197242  0.05934598  1.31043885  2.61902713  1.39786250\nMonth8       -1.9579055  0.06263108  1.31489316  1.39786250  2.63712696\nMonth9       -1.6011594  0.03193685  1.27327443  1.31558217  1.32032119\n                 Month9\n(Intercept) -1.60115942\nWind         0.03193685\nMonth6       1.27327443\nMonth7       1.31558217\nMonth8       1.32032119\nMonth9       2.54701213\nattr(,\"type\")\n[1] \"iid\"\nattr(,\"rss\")\n[1] 5604.977\nattr(,\"tss\")\n[1] 13617.88\n\n# predictions\nhead(predict(mod, newdata = airquality))\n\n[1] 68.68740 68.24137 64.82179 65.63951 63.55803 63.11199\n\nhead(predict(mod, newdata = airquality, interval = \"confidence\"))\n\n       fit      lwr      upr\n1 68.68740 66.16842 71.20639\n2 68.24137 65.80451 70.67823\n3 64.82179 62.61130 67.03227\n4 65.63951 63.44749 67.83153\n5 63.55803 61.22919 65.88686\n6 63.11199 60.71775 65.50623",
    "crumbs": [
      "Reference",
      "Base methods",
      "confint.dbreg"
    ]
  },
  {
    "objectID": "man/sql_model_matrix.html",
    "href": "man/sql_model_matrix.html",
    "title": "dbreg",
    "section": "",
    "text": "Expands formula terms into SQL SELECT expressions, handling factor one-hot encoding and interaction terms. Only the right-hand side of the formula is processed; the response variable (LHS) is ignored.\n\n\n\nsql_model_matrix(\n  formula,\n  conn,\n  table,\n  expand = c(\"all\", \"interactions\"),\n  sep = \"_x_\",\n  fe_vars = character()\n)\n\n\n\n\n\n\n\nformula\n\n\nA formula (or Formula) object. Only RHS terms are expanded; the LHS (response variable) is ignored.\n\n\n\n\nconn\n\n\nDatabase connection\n\n\n\n\ntable\n\n\nTable name or FROM clause\n\n\n\n\nexpand\n\n\nCharacter: ‚Äò\"all\"‚Äô expands factors and interactions, ‚Äò\"interactions\"‚Äô only expands interaction terms (factors in main effects kept as-is for grouping)\n\n\n\n\nsep\n\n\nCharacter separator for interaction term names. Default is ‚Äò\"x\"‚Äô. Use ‚Äò\":\"‚Äô for standard R naming convention.\n\n\n\n\nfe_vars\n\n\nCharacter vector of fixed effect variable names. These are treated as \"in the model\" for determining whether to drop reference levels in interactions.\n\n\n\n\n\n\nList with: - ‚Äòselect_exprs‚Äô: character vector of SQL expressions - ‚Äòcol_names‚Äô: corresponding column names - ‚Äòfactor_levels‚Äô: list of factor levels by variable (for reference)\n\n\n\n\nlibrary(\"dbreg\")\n\nlibrary(DBI)\nlibrary(duckdb)\ncon = dbConnect(duckdb())\nduckdb_register(con, \"test\", data.frame(x1 = 1:3, x2 = c(\"a\", \"b\", \"c\")))\nsql_model_matrix(~ x1 + x2, con, \"test\")\n\n$select_exprs\n[1] \"x1\"                                      \n[2] \"CASE WHEN x2 = 'b' THEN 1.0 ELSE 0.0 END\"\n[3] \"CASE WHEN x2 = 'c' THEN 1.0 ELSE 0.0 END\"\n\n$col_names\n[1] \"x1\"  \"x2b\" \"x2c\"\n\n$factor_levels\n$factor_levels$x2\n[1] \"a\" \"b\" \"c\"\n\nsql_model_matrix(~ x1:x2, con, \"test\")\n\n$select_exprs\n[1] \"(x1) * (CASE WHEN x2 = 'a' THEN 1.0 ELSE 0.0 END)\"\n[2] \"(x1) * (CASE WHEN x2 = 'b' THEN 1.0 ELSE 0.0 END)\"\n[3] \"(x1) * (CASE WHEN x2 = 'c' THEN 1.0 ELSE 0.0 END)\"\n\n$col_names\n[1] \"x1_x_x2a\" \"x1_x_x2b\" \"x1_x_x2c\"\n\n$factor_levels\n$factor_levels$x2\n[1] \"a\" \"b\" \"c\"\n\nsql_model_matrix(~ x1:x2, con, \"test\", sep = \":\")\n\n$select_exprs\n[1] \"(x1) * (CASE WHEN x2 = 'a' THEN 1.0 ELSE 0.0 END)\"\n[2] \"(x1) * (CASE WHEN x2 = 'b' THEN 1.0 ELSE 0.0 END)\"\n[3] \"(x1) * (CASE WHEN x2 = 'c' THEN 1.0 ELSE 0.0 END)\"\n\n$col_names\n[1] \"x1:x2a\" \"x1:x2b\" \"x1:x2c\"\n\n$factor_levels\n$factor_levels$x2\n[1] \"a\" \"b\" \"c\"\n\ndbDisconnect(con)",
    "crumbs": [
      "Reference",
      "utilities",
      "sql_model_matrix"
    ]
  },
  {
    "objectID": "man/sql_model_matrix.html#construct-sql-expressions-for-a-design-matrix",
    "href": "man/sql_model_matrix.html#construct-sql-expressions-for-a-design-matrix",
    "title": "dbreg",
    "section": "",
    "text": "Expands formula terms into SQL SELECT expressions, handling factor one-hot encoding and interaction terms. Only the right-hand side of the formula is processed; the response variable (LHS) is ignored.\n\n\n\nsql_model_matrix(\n  formula,\n  conn,\n  table,\n  expand = c(\"all\", \"interactions\"),\n  sep = \"_x_\",\n  fe_vars = character()\n)\n\n\n\n\n\n\n\nformula\n\n\nA formula (or Formula) object. Only RHS terms are expanded; the LHS (response variable) is ignored.\n\n\n\n\nconn\n\n\nDatabase connection\n\n\n\n\ntable\n\n\nTable name or FROM clause\n\n\n\n\nexpand\n\n\nCharacter: ‚Äò\"all\"‚Äô expands factors and interactions, ‚Äò\"interactions\"‚Äô only expands interaction terms (factors in main effects kept as-is for grouping)\n\n\n\n\nsep\n\n\nCharacter separator for interaction term names. Default is ‚Äò\"x\"‚Äô. Use ‚Äò\":\"‚Äô for standard R naming convention.\n\n\n\n\nfe_vars\n\n\nCharacter vector of fixed effect variable names. These are treated as \"in the model\" for determining whether to drop reference levels in interactions.\n\n\n\n\n\n\nList with: - ‚Äòselect_exprs‚Äô: character vector of SQL expressions - ‚Äòcol_names‚Äô: corresponding column names - ‚Äòfactor_levels‚Äô: list of factor levels by variable (for reference)\n\n\n\n\nlibrary(\"dbreg\")\n\nlibrary(DBI)\nlibrary(duckdb)\ncon = dbConnect(duckdb())\nduckdb_register(con, \"test\", data.frame(x1 = 1:3, x2 = c(\"a\", \"b\", \"c\")))\nsql_model_matrix(~ x1 + x2, con, \"test\")\n\n$select_exprs\n[1] \"x1\"                                      \n[2] \"CASE WHEN x2 = 'b' THEN 1.0 ELSE 0.0 END\"\n[3] \"CASE WHEN x2 = 'c' THEN 1.0 ELSE 0.0 END\"\n\n$col_names\n[1] \"x1\"  \"x2b\" \"x2c\"\n\n$factor_levels\n$factor_levels$x2\n[1] \"a\" \"b\" \"c\"\n\nsql_model_matrix(~ x1:x2, con, \"test\")\n\n$select_exprs\n[1] \"(x1) * (CASE WHEN x2 = 'a' THEN 1.0 ELSE 0.0 END)\"\n[2] \"(x1) * (CASE WHEN x2 = 'b' THEN 1.0 ELSE 0.0 END)\"\n[3] \"(x1) * (CASE WHEN x2 = 'c' THEN 1.0 ELSE 0.0 END)\"\n\n$col_names\n[1] \"x1_x_x2a\" \"x1_x_x2b\" \"x1_x_x2c\"\n\n$factor_levels\n$factor_levels$x2\n[1] \"a\" \"b\" \"c\"\n\nsql_model_matrix(~ x1:x2, con, \"test\", sep = \":\")\n\n$select_exprs\n[1] \"(x1) * (CASE WHEN x2 = 'a' THEN 1.0 ELSE 0.0 END)\"\n[2] \"(x1) * (CASE WHEN x2 = 'b' THEN 1.0 ELSE 0.0 END)\"\n[3] \"(x1) * (CASE WHEN x2 = 'c' THEN 1.0 ELSE 0.0 END)\"\n\n$col_names\n[1] \"x1:x2a\" \"x1:x2b\" \"x1:x2c\"\n\n$factor_levels\n$factor_levels$x2\n[1] \"a\" \"b\" \"c\"\n\ndbDisconnect(con)",
    "crumbs": [
      "Reference",
      "utilities",
      "sql_model_matrix"
    ]
  },
  {
    "objectID": "man/gof.html",
    "href": "man/gof.html",
    "title": "dbreg",
    "section": "",
    "text": "Calculate goodness-of-fit metrics for dbreg objects\n\n\n\ngof(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused)\n\n\n\n\n\n\nNamed vector with r2, adj_r2, and rmse\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ngof(mod)\n\n       r2    adj_r2      rmse \n0.5884105 0.5744109 6.0525892",
    "crumbs": [
      "Reference",
      "utilities",
      "gof"
    ]
  },
  {
    "objectID": "man/gof.html#calculate-goodness-of-fit-metrics-for-dbreg-objects",
    "href": "man/gof.html#calculate-goodness-of-fit-metrics-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Calculate goodness-of-fit metrics for dbreg objects\n\n\n\ngof(object, ...)\n\n\n\n\n\n\n\nobject\n\n\nA ‚Äòdbreg‚Äô object.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments (currently unused)\n\n\n\n\n\n\nNamed vector with r2, adj_r2, and rmse\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ngof(mod)\n\n       r2    adj_r2      rmse \n0.5884105 0.5744109 6.0525892",
    "crumbs": [
      "Reference",
      "utilities",
      "gof"
    ]
  },
  {
    "objectID": "man/print.dbreg.html",
    "href": "man/print.dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nprint(x, fe = FALSE, ...)\n\n\n\n\n\n\n\nx\n\n\n‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be displayed? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nOther arguments passed to print. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n# mod # same as below\nprint(mod)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\nprint(mod, fe = TRUE)  # include fixed effects\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411",
    "crumbs": [
      "Reference",
      "Base methods",
      "print.dbreg"
    ]
  },
  {
    "objectID": "man/print.dbreg.html#print-method-for-dbreg-objects",
    "href": "man/print.dbreg.html#print-method-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Print method for dbreg objects\n\n\n\n## S3 method for class 'dbreg'\nprint(x, fe = FALSE, ...)\n\n\n\n\n\n\n\nx\n\n\n‚Äòdbreg‚Äô object.\n\n\n\n\nfe\n\n\nShould the fixed effects be displayed? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nOther arguments passed to print. Currently unused, except to capture superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\n# mod # same as below\nprint(mod)\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n      Estimate Std. Error t value   Pr(&gt;|t|)    \nWind -0.743388   0.148791 -4.9962 1.6384e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411\n\nprint(mod, fe = TRUE)  # include fixed effects\n\nCompressed OLS estimation, Dep. Var.: Temp \nObservations.: 153 (original) | 88 (compressed) \nStandard Errors: IID \n             Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 74.188474   2.054401 36.11198  &lt; 2.2e-16 ***\nWind        -0.743388   0.148791 -4.99620 1.6384e-06 ***\nMonth6      12.543643   1.594253  7.86804 7.1848e-13 ***\nMonth7      16.362079   1.618341 10.11040  &lt; 2.2e-16 ***\nMonth8      16.316286   1.623923 10.04745  &lt; 2.2e-16 ***\nMonth9      10.279216   1.595936  6.44087 1.5903e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 6.1                 Adj. R2: 0.574411",
    "crumbs": [
      "Reference",
      "Base methods",
      "print.dbreg"
    ]
  },
  {
    "objectID": "man/dbbinsreg.html",
    "href": "man/dbbinsreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Performs binned regression entirely in SQL, returning plot-ready data with estimated bin means or piecewise polynomial fits. The API is designed to be compatible with the binsreg package by Cattaneo, Crump, Farrell, and Feng (2024). Supports unconditional and conditional models (with controls and/or fixed effects).\n\n\n\ndbbinsreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  points = c(0, 0),\n  line = NULL,\n  linegrid = 20,\n  nbins = 20,\n  binspos = \"qs\",\n  randcut = NULL,\n  sample_fit = NULL,\n  ci = TRUE,\n  cb = FALSE,\n  vcov = NULL,\n  level = 0.95,\n  nsims = 500,\n  strategy = c(\"auto\", \"compress\"),\n  plot = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  dots = NULL,\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the binscatter relation. The first variable on the RHS is the running variable; additional variables are controls. Fixed effects go after |. Examples:\n\n\ny ~ x: simple binscatter\n\n\ny ~ x + w1 + w2: binscatter with controls\n\n\ny ~ x | fe: binscatter with fixed effects\n\n\ny ~ x + w1 + w2 | fe: binscatter with controls and fixed effects\n\n\n\n\n\n\nconn\n\n\nDatabase connection, e.g.¬†created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g.¬†‚Äúmydata/**/.*parquet‚Äù. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g.¬†‚Äúread_parquet(‚Äômydata/**/*.parquet‚Äô)‚Äú for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\npoints\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the points (point estimates at bin means). Default is c(0, 0) for canonical binscatter (bin means). Set to NULL or FALSE to suppress points. The smoothness s must satisfy s &lt;= p.\n\n\n\n\nline\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the line (evaluated on a grid within bins). Default is NULL (no line). Set to TRUE for c(0, 0) or a vector like c(1, 1) for piecewise linear with continuity constraints. The smoothness \\(s\\) must satisfy s &lt;= p.\n\n\n\n\nlinegrid\n\n\nNumber of evaluation points per bin for the line. Default is 20.\n\n\n\n\nnbins\n\n\nInteger number of bins. Default is 20.\n\n\n\n\nbinspos\n\n\nBin positioning method. One of either ‚Äúqs‚Äù (quantile-spaced, equal-count bins, the default), ‚Äúes‚Äù (evenly-spaced, equal-width bins), or a numeric vector of knot positions for manual specification.\n\n\n\n\nrandcut\n\n\nNumeric in the range (0,1]. Controls the random sampling fraction for bin boundary computation on large datasets. If NULL (the default), then sampling is automatic: 0.01 (1%) for datasets exceeding 1 million rows and 1 (100%) otherwise. Note that sampling is only used for computing the bin boundaries, since this requires an expensive ranking operation. The subsequent, primary regression operations use all of the data (unless sample_fit is enabled).\n\n\n\n\nsample_fit\n\n\nLogical or NULL. Controls whether the spline regression (s &gt; 0) re-uses the same random sample (controlled by randcut) that is used for computing the bin boundaries. This trades off some precision for major speed gains on big datasets; see the Smoothness Constraints section below. If NULL (the default), sampling is enabled automatically when applicable, with a message. Explicitly set to TRUE to enable the same sampling behaviour, but without the message. Alternatively, set to FALSE to use the full dataset. Ignored when s = 0, since the ‚Äúcompress‚Äù strategy already handles these aggregation cases efficiently.\n\n\n\n\nci\n\n\nLogical. Calculate standard errors and confidence intervals for points? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Calculate simultaneous confidence bands using simulation? Default is FALSE.\n\n\n\n\nvcov\n\n\nCharacter string or formula for standard errors. Options are ‚ÄúHC1‚Äù (default, heteroskedasticity-robust, matches binsreg), ‚Äúiid‚Äù, or a one-sided formula for clustered standard errors (e.g., ~cluster_var).\n\n\n\n\nlevel\n\n\nNumeric in the range [0,1], giving the confidence level for the confidence levels and/or bands. Default is 0.95.\n\n\n\n\nnsims\n\n\nNumber of simulation draws for confidence band computation. Default is 500. Only used when cb = TRUE.\n\n\n\n\nstrategy\n\n\nAcceleration strategy passed to dbreg. Only ‚Äúcompress‚Äù is currently supported; ‚Äúauto‚Äù (the default) maps to ‚Äúcompress‚Äù. Included for API consistency with dbreg. Ignored when smoothness s &gt; 0, since spline basis construction requires row-level data (i.e., no pre-aggregation).\n\n\n\n\nplot\n\n\nLogical. If TRUE (default), a plot is produced as a side effect. Set to FALSE to suppress plotting.\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\ndots\n\n\nAlias for points for binsreg compatibility. If not NULL, overrides the points argument.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments passed to plot.dbbinsreg, which may in turn be passed to tinyplot.\n\n\n\n\n\n\nA list of class \"dbbinsreg\" containing:\n\n\npoints\n\n\nData frame of the point estimates (one row per bin): x (bin mean), bin, and fit (fitted value). If ci=TRUE in the original call, then also includes the columns: se, lwr, and upr. Similarly, if cb=TRUE, then includes the columns: cb_lwr and cb_upr.\n\n\nline\n\n\nData frame of the line estimates (multiple rows per bin): x, bin, fit. Only present if line is specified.\n\n\nbins\n\n\nData frame with bin boundaries: id (bin number), left (left endpoint), right (right endpoint).\n\n\nmodel\n\n\nThe fitted dbreg model object (for points).\n\n\nopt\n\n\nList of options used: points, line, nbins, binspos, etc.\n\n\nIf plot = TRUE (the default), a binscatter plot is also produced as a side effect. See plot.dbbinsreg for plot customization.\n\n\n\nThe dbbinsreg function is deeply inspired by the binsreg package (Cattaneo et. al., 2024). The main difference is that dbbinsreg performs most of its computation on a database backend, employing various acceration strategies, which makes it particularly suitable for large datasets (which may not fit in memory). At the same time, the database backend introduces its own set of tradeoffs. We cover the most important points of similarity and difference below.\n\nWe aim to mimic the binsreg API as much as possible. Key parameter mappings include:\n\n\npoints (alias dots): Point estimates at bin means\n\n\nc(0,0): Canonical binscatter (bin means)\n\n\nc(p,0): Piecewise polynomial of degree \\(p\\), no smoothness\n\n\nc(p,s): Piecewise polynomial with \\(s\\) smoothness constraints\n\n\n\n\nline: Same as points but evaluated on a finer grid for smooth visualization\n\n\nbinspos: Bin positioning\n\n\n‚Äúqs‚Äù: Quantile-spaced (equal count)\n\n\n‚Äúes‚Äù: Evenly-spaced (equal width)\n\n\n\n\nImportant: Unlike binsreg, dbbinsreg does not automatically select the IMSE-optimal number of bins. Rather, users must specify nbins manually (with a default of value of 20). For guidance on bin selection, see binsregselect or Cattaneo et al. (2024).\n\nWhen s &gt; 0, the function fits a regression spline using a truncated power basis. For degree \\(p\\) and smoothness \\(s\\), the basis includes global polynomial terms (\\(x, x^2, \\ldots, x^p\\)) plus truncated power terms \\((x - \\kappa_j)_+^r\\) at each interior knot \\(\\kappa_j\\) for \\(r = s, \\ldots, p\\). This enforces \\(C^{s-1}\\) continuity (continuous derivatives up to order \\(s-1\\)) at bin boundaries. For example, c(1,1) gives a piecewise linear fit that is continuous; c(2,2) gives a piecewise quadratic with continuous first derivatives.\nImportant: Unlike s = 0 (which uses the ‚Äúcompress‚Äù strategy for fast aggregation), s &gt; 0 requires row-level spline basis construction and can be very slow on large datasets. As a result, dbbinsreg re-uses the random sample (used to compute the bin boundaries) for estimating the spline fits in these cases, ensuring much faster computation at the cost of reduced precision. Users can override this behaviour by passing the sample_fit = FALSE argument to rather estimate the spline regressions on the full dataset.\n\nWhen ci = TRUE (default), pointwise confidence intervals (CIs) are computed at each bin mean using standard asymptotic theory. When cb = TRUE, simultaneous confidence bands (CBs) are computed using a simulation-based sup-\\(t\\) procedure:\n\n\nDraw nsims samples from the asymptotic distribution of the estimator\n\n\nCompute the supremum of the \\(t\\)-statistics across all bins for each draw\n\n\nUse the (\\(1-\\alpha\\)) quantile of these suprema as the critical value\n\n\nThe confidence band is wider than pointwise CIs and provides simultaneous coverage: with (\\(1-\\alpha\\)) probability, the entire true function lies within the band. This is useful for making statements about the overall shape of the relationship rather than individual point estimates.\nThere are two important caveats, regarding dbbinsreg‚Äôs CB support:\n\n\nUnlike binsreg, which evaluates CB on a fine grid within each bin, dbbinsreg computes CB only at bin means (same points as CI). This is much simpler for our backend SQL implementation and should be sufficient for most applications.\n\n\nCBs are currently only supported for unconstrained estimation (smoothness s = 0). When cb = TRUE with s &gt; 0, a warning is issued and CB is skipped.\n\n\n\nWhen using quantile-spaced bins (binspos = ‚Äúqs‚Äù), dbbinsreg uses SQL‚Äôs NTILE() window function, while binsreg uses R‚Äôs quantile with type = 2. These algorithms have slightly different tie-breaking behavior, which can cause small differences in bin assignments at boundaries. In practice, differences are typically &lt;1% and become negligible with larger datasets. To match binsreg exactly, compute quantile breaks on a subset of data in R and pass them via the binspos argument as a numeric vector.\n\n\n\nCattaneo, M. D., R. K. Crump, M. H. Farrell, and Y. Feng (2024). On Binscatter. American Economic Review, 114(5): 1488-1514.\n\n\n\nplot.dbbinsreg for plot customization, dbreg for the underlying regression engine, binsreg for the original implementation.\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# You can pass additional plotting arguments via ... to (tiny)plot.dbbinsreg\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10,\n          main = \"A simple binscatter example\", theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Alternatively, save the object and plot later\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, plot = FALSE)\nplot(bs, main = \"Same example, different theme\", theme = \"classic\")\n\n\n\n\n\n\n\n# Piecewise linear (p = 1), no smoothness (s = 0)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 21 (compressed)\n\n# Piecewise linear (p = 1) with continuity (s = 1)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 (quantile-spaced)\nN = 578\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 (quantile-spaced)\nN = 578\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\n# (We'll also add a line and change the theme for a nicer plot)\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE,\n          line = c(1, 1), theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10,\n          theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbbinsreg"
    ]
  },
  {
    "objectID": "man/dbbinsreg.html#run-a-binscatter-regression-on-a-database-backend-and-plot-the-result",
    "href": "man/dbbinsreg.html#run-a-binscatter-regression-on-a-database-backend-and-plot-the-result",
    "title": "dbreg",
    "section": "",
    "text": "Performs binned regression entirely in SQL, returning plot-ready data with estimated bin means or piecewise polynomial fits. The API is designed to be compatible with the binsreg package by Cattaneo, Crump, Farrell, and Feng (2024). Supports unconditional and conditional models (with controls and/or fixed effects).\n\n\n\ndbbinsreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  points = c(0, 0),\n  line = NULL,\n  linegrid = 20,\n  nbins = 20,\n  binspos = \"qs\",\n  randcut = NULL,\n  sample_fit = NULL,\n  ci = TRUE,\n  cb = FALSE,\n  vcov = NULL,\n  level = 0.95,\n  nsims = 500,\n  strategy = c(\"auto\", \"compress\"),\n  plot = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  dots = NULL,\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the binscatter relation. The first variable on the RHS is the running variable; additional variables are controls. Fixed effects go after |. Examples:\n\n\ny ~ x: simple binscatter\n\n\ny ~ x + w1 + w2: binscatter with controls\n\n\ny ~ x | fe: binscatter with fixed effects\n\n\ny ~ x + w1 + w2 | fe: binscatter with controls and fixed effects\n\n\n\n\n\n\nconn\n\n\nDatabase connection, e.g.¬†created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g.¬†‚Äúmydata/**/.*parquet‚Äù. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g.¬†‚Äúread_parquet(‚Äômydata/**/*.parquet‚Äô)‚Äú for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\npoints\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the points (point estimates at bin means). Default is c(0, 0) for canonical binscatter (bin means). Set to NULL or FALSE to suppress points. The smoothness s must satisfy s &lt;= p.\n\n\n\n\nline\n\n\nA vector c(p, s) specifying the polynomial degree \\(p\\) and smoothness \\(s\\) for the line (evaluated on a grid within bins). Default is NULL (no line). Set to TRUE for c(0, 0) or a vector like c(1, 1) for piecewise linear with continuity constraints. The smoothness \\(s\\) must satisfy s &lt;= p.\n\n\n\n\nlinegrid\n\n\nNumber of evaluation points per bin for the line. Default is 20.\n\n\n\n\nnbins\n\n\nInteger number of bins. Default is 20.\n\n\n\n\nbinspos\n\n\nBin positioning method. One of either ‚Äúqs‚Äù (quantile-spaced, equal-count bins, the default), ‚Äúes‚Äù (evenly-spaced, equal-width bins), or a numeric vector of knot positions for manual specification.\n\n\n\n\nrandcut\n\n\nNumeric in the range (0,1]. Controls the random sampling fraction for bin boundary computation on large datasets. If NULL (the default), then sampling is automatic: 0.01 (1%) for datasets exceeding 1 million rows and 1 (100%) otherwise. Note that sampling is only used for computing the bin boundaries, since this requires an expensive ranking operation. The subsequent, primary regression operations use all of the data (unless sample_fit is enabled).\n\n\n\n\nsample_fit\n\n\nLogical or NULL. Controls whether the spline regression (s &gt; 0) re-uses the same random sample (controlled by randcut) that is used for computing the bin boundaries. This trades off some precision for major speed gains on big datasets; see the Smoothness Constraints section below. If NULL (the default), sampling is enabled automatically when applicable, with a message. Explicitly set to TRUE to enable the same sampling behaviour, but without the message. Alternatively, set to FALSE to use the full dataset. Ignored when s = 0, since the ‚Äúcompress‚Äù strategy already handles these aggregation cases efficiently.\n\n\n\n\nci\n\n\nLogical. Calculate standard errors and confidence intervals for points? Default is TRUE.\n\n\n\n\ncb\n\n\nLogical. Calculate simultaneous confidence bands using simulation? Default is FALSE.\n\n\n\n\nvcov\n\n\nCharacter string or formula for standard errors. Options are ‚ÄúHC1‚Äù (default, heteroskedasticity-robust, matches binsreg), ‚Äúiid‚Äù, or a one-sided formula for clustered standard errors (e.g., ~cluster_var).\n\n\n\n\nlevel\n\n\nNumeric in the range [0,1], giving the confidence level for the confidence levels and/or bands. Default is 0.95.\n\n\n\n\nnsims\n\n\nNumber of simulation draws for confidence band computation. Default is 500. Only used when cb = TRUE.\n\n\n\n\nstrategy\n\n\nAcceleration strategy passed to dbreg. Only ‚Äúcompress‚Äù is currently supported; ‚Äúauto‚Äù (the default) maps to ‚Äúcompress‚Äù. Included for API consistency with dbreg. Ignored when smoothness s &gt; 0, since spline basis construction requires row-level data (i.e., no pre-aggregation).\n\n\n\n\nplot\n\n\nLogical. If TRUE (default), a plot is produced as a side effect. Set to FALSE to suppress plotting.\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\ndots\n\n\nAlias for points for binsreg compatibility. If not NULL, overrides the points argument.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments passed to plot.dbbinsreg, which may in turn be passed to tinyplot.\n\n\n\n\n\n\nA list of class \"dbbinsreg\" containing:\n\n\npoints\n\n\nData frame of the point estimates (one row per bin): x (bin mean), bin, and fit (fitted value). If ci=TRUE in the original call, then also includes the columns: se, lwr, and upr. Similarly, if cb=TRUE, then includes the columns: cb_lwr and cb_upr.\n\n\nline\n\n\nData frame of the line estimates (multiple rows per bin): x, bin, fit. Only present if line is specified.\n\n\nbins\n\n\nData frame with bin boundaries: id (bin number), left (left endpoint), right (right endpoint).\n\n\nmodel\n\n\nThe fitted dbreg model object (for points).\n\n\nopt\n\n\nList of options used: points, line, nbins, binspos, etc.\n\n\nIf plot = TRUE (the default), a binscatter plot is also produced as a side effect. See plot.dbbinsreg for plot customization.\n\n\n\nThe dbbinsreg function is deeply inspired by the binsreg package (Cattaneo et. al., 2024). The main difference is that dbbinsreg performs most of its computation on a database backend, employing various acceration strategies, which makes it particularly suitable for large datasets (which may not fit in memory). At the same time, the database backend introduces its own set of tradeoffs. We cover the most important points of similarity and difference below.\n\nWe aim to mimic the binsreg API as much as possible. Key parameter mappings include:\n\n\npoints (alias dots): Point estimates at bin means\n\n\nc(0,0): Canonical binscatter (bin means)\n\n\nc(p,0): Piecewise polynomial of degree \\(p\\), no smoothness\n\n\nc(p,s): Piecewise polynomial with \\(s\\) smoothness constraints\n\n\n\n\nline: Same as points but evaluated on a finer grid for smooth visualization\n\n\nbinspos: Bin positioning\n\n\n‚Äúqs‚Äù: Quantile-spaced (equal count)\n\n\n‚Äúes‚Äù: Evenly-spaced (equal width)\n\n\n\n\nImportant: Unlike binsreg, dbbinsreg does not automatically select the IMSE-optimal number of bins. Rather, users must specify nbins manually (with a default of value of 20). For guidance on bin selection, see binsregselect or Cattaneo et al. (2024).\n\nWhen s &gt; 0, the function fits a regression spline using a truncated power basis. For degree \\(p\\) and smoothness \\(s\\), the basis includes global polynomial terms (\\(x, x^2, \\ldots, x^p\\)) plus truncated power terms \\((x - \\kappa_j)_+^r\\) at each interior knot \\(\\kappa_j\\) for \\(r = s, \\ldots, p\\). This enforces \\(C^{s-1}\\) continuity (continuous derivatives up to order \\(s-1\\)) at bin boundaries. For example, c(1,1) gives a piecewise linear fit that is continuous; c(2,2) gives a piecewise quadratic with continuous first derivatives.\nImportant: Unlike s = 0 (which uses the ‚Äúcompress‚Äù strategy for fast aggregation), s &gt; 0 requires row-level spline basis construction and can be very slow on large datasets. As a result, dbbinsreg re-uses the random sample (used to compute the bin boundaries) for estimating the spline fits in these cases, ensuring much faster computation at the cost of reduced precision. Users can override this behaviour by passing the sample_fit = FALSE argument to rather estimate the spline regressions on the full dataset.\n\nWhen ci = TRUE (default), pointwise confidence intervals (CIs) are computed at each bin mean using standard asymptotic theory. When cb = TRUE, simultaneous confidence bands (CBs) are computed using a simulation-based sup-\\(t\\) procedure:\n\n\nDraw nsims samples from the asymptotic distribution of the estimator\n\n\nCompute the supremum of the \\(t\\)-statistics across all bins for each draw\n\n\nUse the (\\(1-\\alpha\\)) quantile of these suprema as the critical value\n\n\nThe confidence band is wider than pointwise CIs and provides simultaneous coverage: with (\\(1-\\alpha\\)) probability, the entire true function lies within the band. This is useful for making statements about the overall shape of the relationship rather than individual point estimates.\nThere are two important caveats, regarding dbbinsreg‚Äôs CB support:\n\n\nUnlike binsreg, which evaluates CB on a fine grid within each bin, dbbinsreg computes CB only at bin means (same points as CI). This is much simpler for our backend SQL implementation and should be sufficient for most applications.\n\n\nCBs are currently only supported for unconstrained estimation (smoothness s = 0). When cb = TRUE with s &gt; 0, a warning is issued and CB is skipped.\n\n\n\nWhen using quantile-spaced bins (binspos = ‚Äúqs‚Äù), dbbinsreg uses SQL‚Äôs NTILE() window function, while binsreg uses R‚Äôs quantile with type = 2. These algorithms have slightly different tie-breaking behavior, which can cause small differences in bin assignments at boundaries. In practice, differences are typically &lt;1% and become negligible with larger datasets. To match binsreg exactly, compute quantile breaks on a subset of data in R and pass them via the binspos argument as a numeric vector.\n\n\n\nCattaneo, M. D., R. K. Crump, M. H. Farrell, and Y. Feng (2024). On Binscatter. American Economic Review, 114(5): 1488-1514.\n\n\n\nplot.dbbinsreg for plot customization, dbreg for the underlying regression engine, binsreg for the original implementation.\n\n\n\n\nlibrary(\"dbreg\")\n\n#\n## In-memory data ----\n\n# Like `dbreg`, we can pass in-memory R data frames to an ephemeral DuckDB\n# connection via the `data` argument. \n\n# Canonical binscatter: bin means (default)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# You can pass additional plotting arguments via ... to (tiny)plot.dbbinsreg\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10,\n          main = \"A simple binscatter example\", theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Alternatively, save the object and plot later\nbs = dbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, plot = FALSE)\nplot(bs, main = \"Same example, different theme\", theme = \"classic\")\n\n\n\n\n\n\n\n# Piecewise linear (p = 1), no smoothness (s = 0)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 0))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 21 (compressed)\n\n# Piecewise linear (p = 1) with continuity (s = 1)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = NULL | nbins = 10 (quantile-spaced)\nN = 578\n\n# With line overlay for smooth visualization\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(1, 1), line = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(1,1) | line = c(1,1) | nbins = 10 (quantile-spaced)\nN = 578\n\n# Different line smoothness to points\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, points = c(0, 0), line = c(1, 1))\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# With uniform confidence bands (much greater uncertainty)\nset.seed(99)\ndbbinsreg(weight ~ Time, data = ChickWeight, nbins = 10, cb = TRUE)\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 10 (compressed)\n\n# Accounting for Diet \"fixed effects\" helps to resolve the situation\n# (We'll also add a line and change the theme for a nicer plot)\ndbbinsreg(weight ~ Time | Diet, data = ChickWeight, nbins = 10, cb = TRUE,\n          line = c(1, 1), theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = c(1,1) | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n#\n## DBI connection ----\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbbinsreg(weight ~ Time | Diet, conn = con, table = \"cw\", nbins = 10,\n          theme = \"clean\")\n\n\n\n\n\n\n\n\nBinscatter Plot\nFormula: weight ~ Time | Diet \npoints = c(0,0) | line = NULL | nbins = 10 (quantile-spaced)\nObservations: 578 (original) | 40 (compressed)\n\n# etc.\n\n# See ?dbreg for more connection examples\n\n# Clean up\ndbDisconnect(con)",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbbinsreg"
    ]
  },
  {
    "objectID": "man/dbreg.html",
    "href": "man/dbreg.html",
    "title": "dbreg",
    "section": "",
    "text": "Leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics.\n\n\n\ndbreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  vcov = c(\"iid\", \"hc1\"),\n  strategy = c(\"auto\", \"compress\", \"moments\", \"demean\", \"within\", \"mundlak\"),\n  compress_ratio = NULL,\n  compress_nmax = 1e+06,\n  cluster = NULL,\n  ssc = c(\"full\", \"nested\"),\n  sql_only = FALSE,\n  data_only = FALSE,\n  drop_missings = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the relation to be estimated. Fixed effects should be included after a pipe, e.g fml = y ~ x1 + x2 | fe1 + f2. Interaction terms are supported using standard R syntax (: for interactions, * for main effects plus interaction). Transformations and literals are not yet supported.\n\n\n\n\nconn\n\n\nDatabase connection, e.g.¬†created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g.¬†‚Äúmydata/**/.*parquet‚Äù. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g.¬†‚Äúread_parquet(‚Äômydata/**/*.parquet‚Äô)‚Äú for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\nvcov\n\n\nCharacter string or formula denoting the desired type of variance- covariance correction / standard errors. Options are ‚Äúiid‚Äù (default), ‚Äúhc1‚Äù (heteroskedasticity-consistent), or a one-sided formula like ~cluster_var for cluster-robust standard errors. Note that ‚Äúhc1‚Äù and clustered SEs require a second pass over the data unless strategy = ‚Äúcompress‚Äù to construct the residuals.\n\n\n\n\nstrategy\n\n\nCharacter string indicating the preferred acceleration strategy. The default ‚Äúauto‚Äù will pick an optimal strategy based on internal heuristics. Users can also override with one of the following explicit strategies: ‚Äúcompress‚Äù, ‚Äúdemean‚Äù (alias: ‚Äúwithin‚Äù), ‚Äúmundlak‚Äù, or ‚Äúmoments‚Äù. See the Acceleration Strategies section below for details.\n\n\n\n\ncompress_ratio, compress_nmax\n\n\nNumeric(s). Parameters that help to determine the acceleration strategy under the default ‚Äúauto‚Äù option.\n\n\ncompress_ratio defines the compression ratio threshold, i.e.¬†numeric in the range [0,1] defining the minimum acceptable compressed versus the original data size. Default value of NULL means that the threshold will be automatically determined based on some internal heuristic (e.g., 0.01 for models without fixed effects).\n\n\ncompress_nmax defines the maximum allowable size (in rows) of the compressed dataset that can be serialized into R. Pays heed to the idea that big data serialization can be costly (esp.¬†for remote databases), even if we have achieved good compression on top of the original dataset. Default value is 1e6 (i.e., a million rows).\n\n\nSee the Acceleration Strategies section below for further details.\n\n\n\n\ncluster\n\n\nOptional. Provides an alternative way to specify cluster-robust standard errors (i.e., instead of vcov = ~cluster_var). Either a one-sided formula (e.g., ~firm) or character string giving the variable name. Only single-variable clustering is currently supported.\n\n\n\n\nssc\n\n\nCharacter string controlling the small-sample correction for clustered standard errors. Options are ‚Äúfull‚Äù (default) or ‚Äúnested‚Äù. With ‚Äúfull‚Äù, all parameters (including fixed effect dummies) are counted in K for the CR1 correction. With ‚Äúnested‚Äù, fixed effects that are nested within the cluster variable are excluded from K, matching the default behavior of fixest::feols. Only applies to ‚Äúcompress‚Äù and ‚Äúdemean‚Äù strategies (Mundlak uses explicit group mean regressors, not FE dummies). This distinction only matters for small samples. For large datasets (dbreg‚Äôs target use case), the difference is negligible and hence we default to the simple ‚Äúfull‚Äù option.\n\n\n\n\nsql_only\n\n\nLogical indicating whether only the underlying compression SQL query should be returned (i.e., no computation will be performed). Default is FALSE.\n\n\n\n\ndata_only\n\n\nLogical indicating whether only the compressed dataset should be returned (i.e., no regression is run). Default is FALSE.\n\n\n\n\ndrop_missings\n\n\nLogical indicating whether incomplete cases (i.e., rows where any of the dependent, independent or FE variables are missing) should be dropped. The default is TRUE, according with standard regression software. It is strongly recommended not to change this value unless you are absolutely sure that your data have no missings and you wish to skip some internal checks. (Even then, it probably isn‚Äôt worth it.)\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently ignored, except to handle superseded arguments for backwards compatibility.\n\n\n\n\n\n\nA list of class \"dbreg\" containing various slots, including a table of coefficients (which the associated print method will display).\n\n\n\ndbreg offers four primary acceleration strategies for estimating regression results from simplified data representations. Below we use the shorthand Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition purposes:\n\n\n‚Äúcompress‚Äù: compresses the data via a GROUP BY operation (using X and the FE as groups), before running weighted least squares on this much smaller dataset:\n\n\\(\\hat{\\beta} = (X_c' W X_c)^{-1} X_c' W Y_c\\)\nwhere \\(W = \\text{diag}(n_g)\\) are the group frequencies. This procedure follows Wang et al.¬†(2021).\n\n\n‚Äúmoments‚Äù: computes sufficient statistics (\\(X'X, X'y\\)) directly via SQL aggregation, returning a single-row result. This solves the standard OLS normal equations \\(\\hat{\\beta} = (X'X)^{-1}X'y\\). Limited to cases without FE.\n\n\n‚Äúdemean‚Äù (alias ‚Äúwithin‚Äù): subtracts group-level means from both Y and X before computing sufficient statistics (per the ‚Äúmoments‚Äù strategy). For example, given unit \\(i\\) and time \\(t\\) FE, we apply double demeaning:\n\n\\(\\ddot{Y}_{it} = \\beta \\ddot{X}_{it} + \\varepsilon_{it}\\)\nwhere \\(\\ddot{X} = X - \\bar{X}_i - \\bar{X}_t + \\bar{X}\\). This (single-pass) within transformation is algebraically equivalent to the fixed effects projection‚Äîi.e., Frisch-Waugh-Lovell partialling out‚Äîin the presence of a single FE. It is also identical for the two-way FE (TWFE) case if your panel is balanced. For unbalanced two-way panels, however, the double demeaning strategy is not algebraically equivalent to the fixed effects projection and therefore does not recover the exact TWFE coefficients. Moreover, note that this ‚Äúdemean‚Äù strategy permits at most two FE.\n\n\n‚Äúmundlak‚Äù: a generalized Mundlak (1978), or correlated random effects (CRE) estimator that regresses Y on X plus group means of X:\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_i + \\varepsilon_{it} \\quad \\text{(one-way)}\\)\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_{i} + \\delta \\bar{X}_{t} + \\varepsilon_{it} \\quad \\text{(two-way, etc.)}\\)\nUnlike ‚Äúdemean‚Äù, Y is not transformed, so predictions are on the original scale. Supports any number of FE and works correctly for any panel structure (balanced or unbalanced). However, note that CRE is a different model from FE: while coefficients are asymptotically equivalent under certain assumptions, they will generally differ in finite samples.\n\n\nThe relative efficiency of each of these strategies depends on the size and structure of the data, as well the number of unique regressors and FE. For (quote unquote) \"standard\" cases, the ‚Äúcompress‚Äù strategy can yield remarkable performance gains and should justifiably be viewed as a good default. However, the compression approach tends to be less efficient for true panels (repeated cross-sections over time), where N &gt;&gt; T. In such cases, it can be more efficient to use a demeaning strategy that first controls for (e.g.¬†subtracts) group means, before computing sufficient statistics on the aggregated data. The reason for this is that time and unit FE are typically high dimensional, but covariate averages are not; see Arkhangelsky & Imbens (2024).\nHowever, the demeaning approaches invite tradeoffs of their own. For example, the double demeaning transformation of the ‚Äúdemean‚Äù strategy does not obtain exact TWFE results in unbalanced panels, and it is also limited to at most two FE. Conversely, the ‚Äúmundlak‚Äù (CRE) strategy obtains consistent coefficients regardless of panel structure and FE count, but at the \"cost\" of recovering a different estimand. (It is a different model to TWFE, after all.) See Wooldridge (2025) for an extended discussion of these issues.\nUsers should weigh these tradeoffs when choosing their acceleration strategy. Summarising, we can provide a few guiding principles. ‚Äúcompress‚Äù is a good default that guarantees the \"exact\" FE estimates and is usually very efficient (barring data I/O costs and high FE dimensionality). ‚Äúmundlak‚Äù is another efficient alternative provided that the CRE estimand is acceptable (don‚Äôt be alarmed if your coefficients are not identical). Finally, the ‚Äúdemean‚Äù and ‚Äúmoments‚Äù strategies are great for particular use cases (i.e., balanced panels and cases without FE, respectively).\nIf this all sounds like too much to think about, don‚Äôt fret. The good news is that dbreg can do a lot (all?) of the deciding for you. Specifically, it will invoke an ‚Äúauto‚Äù heuristic behind the scenes if a user does not provide an explicit acceleration strategy. Working through the heuristic logic does impose some additional overhead, but this should be negligible in most cases (certainly compared to the overall time savings). The ‚Äúauto‚Äù heuristic is as follows:\n\n\nIF no FE AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN ‚Äúmoments‚Äù.\n\n\nELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN ‚Äúdemean‚Äù.\n\n\nELSE IF 2 FE AND (poor compression ratio OR too big compressed data):\n\n\nIF balanced panel THEN ‚Äúdemean‚Äù.\n\n\nELSE error (exact TWFE infeasible; user must explicitly choose ‚Äúcompress‚Äù or ‚Äúmundlak‚Äù).\n\n\n\n\nELSE THEN ‚Äúcompress‚Äù.\n\n\nTip: set dbreg(‚Ä¶, verbose = TRUE) to print information about the auto strategy decision criteria.\n\n\n\nArkhangelsky, D. & Imbens, G. (2024) Fixed Effects and the Generalized Mundlak Estimator. The Review of Economic Studies, 91(5), pp.¬†2545‚Äì2571. Available: https://doi.org/10.1093/restud/rdad089\nMundlak, Y. (1978) On the Pooling of Time Series and Cross Section Data. Econometrica, 46(1), pp.¬†69‚Äì85. Available: https://doi.org/10.2307/1913646\nWong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021). You Only Compress Once: Optimal Data Compression for Estimating Linear Models. arXiv preprint arXiv:2102.11297. Available: https://doi.org/10.48550/arXiv.2102.11297\nWooldridge, J.M. (2025) Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators. Empirical Economics, 69, pp.¬†2545‚Äì2587. Available: https://doi.org/10.1007/s00181-025-02807-z\n\n\n\ndbConnect for creating database connections, duckdb for DuckDB-specific connections\n\n\n\n\nlibrary(\"dbreg\")\n\n## In-memory data ----\n\n# We can pass in-memory R data frames to an ephemeral DuckDB connection via\n# the `data` argument. This is convenient for small(er) datasets and demos.\n\n# Default \"compress\" strategy reduces the data to 4 rows before running OLS\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Compare with lm\nsummary(lm(weight ~ Diet, data = ChickWeight))$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 102.64545   4.673954 21.961161 4.712762e-78\nDiet2        19.97121   7.867437  2.538465 1.139708e-02\nDiet3        40.30455   7.867437  5.122958 4.113938e-07\nDiet4        32.61726   7.910461  4.123307 4.286352e-05\n\n# Add \"fixed effects\" after a `|` \ndbreg(weight ~ Time | Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# \"robust\" SEs can also be computed using a sufficient statistics approach\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = \"hc1\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Heteroskedasticity-robust \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.261676 33.4402 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = ~Chick)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Clustered (50 clusters) \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.527463 16.5898 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Different acceleration strategies + specifications\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"demean\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.729032\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"mundlak\")\n\nOne-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049    0.22765 38.4383 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729827\n\ndbreg(weight ~ Time | Diet + Chick, data = ChickWeight, strategy = \"mundlak\") # two-way Mundlak\n\nTwo-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.71519   0.229744 37.9344 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729953\n\ndbreg(weight ~ Time, data = ChickWeight, strategy = \"moments\") # no FEs\n\nMoments-based OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 27.46743   3.036464  9.04586 &lt; 2.2e-16 ***\nTime         8.80304   0.239700 36.72524 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 38.8                Adj. R2: 0.700220\n\n# etc.\n\n# Interactions: does the effect of Time vary by Diet?\n# (Diet main effects are collinear with Chick FE, so these drop out)\ndbreg(weight ~ Time * Diet | Chick, data = ChickWeight)\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n           Estimate Std. Error  t value   Pr(&gt;|t|)    \nTime        6.69062   0.259809 25.75212  &lt; 2.2e-16 ***\nTime:Diet2  1.91851   0.429357  4.46834 9.6656e-06 ***\nTime:Diet3  4.73225   0.429357 11.02170  &lt; 2.2e-16 ***\nTime:Diet4  2.96531   0.435005  6.81674 2.5702e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 24.2                Adj. R2: 0.844227\n3 variables were removed because of collinearity (Diet2, Diet3, Diet4)\n\n#\n## DBI connection ----\n\n# For persistent databases or more control, use the `conn` + `table` args.\n# Again, we use DuckDB below but any other DBI-supported backend should work\n# too (e.g., odbc, bigrquery, noctua (AWS Athena),  etc.) See:\n# https://r-dbi.org/backends/\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbreg(weight ~ Time | Diet, conn = con, table = \"cw\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Tip: Rather than creating or writing (temp) tables, use CREATE VIEW to\n# define subsets or computed columns without materializing data. This is more\n# efficient and especially useful for filtering or adding variables.\ndbExecute(\n  con,\n  \"\n  CREATE VIEW cw1 AS\n  SELECT *\n  FROM cw\n  WHERE Diet = 1\n  \"\n)\n\n[1] 0\n\ndbreg(weight ~ Time | Chick, conn = con, table = \"cw1\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 220 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  6.69062   0.246299 27.1646 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 22.9                Adj. R2: 0.766255\n\n#\n## Path to file ----\n#\n# For file-based data (e.g., parquet), use the path argument.\n\ntmp = tempfile(fileext = \".parquet\")\ndbExecute(con, sprintf(\"COPY cw TO '%s' (FORMAT PARQUET)\", tmp))\n\n[1] 578\n\ndbreg(weight ~ Time | Diet, path = tmp)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Cleanup\ndbDisconnect(con)\nunlink(tmp)\n\n#\n## Big dataset ----\n\n# For a more compelling and appropriate dbreg use-case, i.e. regression on a\n# big (~180 million row) dataset of Hive-partioned parquet files, see the\n# package website:\n# https://grantmcdermott.com/dbreg/",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbreg"
    ]
  },
  {
    "objectID": "man/dbreg.html#run-a-regression-on-a-database-backend",
    "href": "man/dbreg.html#run-a-regression-on-a-database-backend",
    "title": "dbreg",
    "section": "",
    "text": "Leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics.\n\n\n\ndbreg(\n  fml,\n  conn = NULL,\n  table = NULL,\n  data = NULL,\n  path = NULL,\n  vcov = c(\"iid\", \"hc1\"),\n  strategy = c(\"auto\", \"compress\", \"moments\", \"demean\", \"within\", \"mundlak\"),\n  compress_ratio = NULL,\n  compress_nmax = 1e+06,\n  cluster = NULL,\n  ssc = c(\"full\", \"nested\"),\n  sql_only = FALSE,\n  data_only = FALSE,\n  drop_missings = TRUE,\n  verbose = getOption(\"dbreg.verbose\", FALSE),\n  ...\n)\n\n\n\n\n\n\n\nfml\n\n\nA formula representing the relation to be estimated. Fixed effects should be included after a pipe, e.g fml = y ~ x1 + x2 | fe1 + f2. Interaction terms are supported using standard R syntax (: for interactions, * for main effects plus interaction). Transformations and literals are not yet supported.\n\n\n\n\nconn\n\n\nDatabase connection, e.g.¬†created with dbConnect. Can be either persistent (disk-backed) or ephemeral (in-memory). If no connection is provided, then an ephemeral duckdb connection will be created automatically and closed before the function exits. Note that a persistent (disk-backed) database connection is required for larger-than-RAM datasets in order to take advantage of out-of-core functionality like streaming (where supported).\n\n\n\n\ntable, data, path\n\n\nMutually exclusive arguments for specifying the data table (object) to be queried. In order of precedence:\n\n\ntable: Character string giving the name of the data table in an existing (open) database connection.\n\n\ndata: R dataframe that can be copied over to conn as a temporary table for querying via the DuckDB query engine. Ignored if table is provided.\n\n\npath: Character string giving a path to the data file(s) on disk, which will be read into conn. Internally, this string is passed to the FROM query statement, so could (should) include file globbing for Hive-partitioned datasets, e.g.¬†‚Äúmydata/**/.*parquet‚Äù. For more precision, however, it is recommended to pass the desired database reader function as part of this string, e.g.¬†‚Äúread_parquet(‚Äômydata/**/*.parquet‚Äô)‚Äú for DuckDB; note the use of single quotes. Ignored if either table or data is provided.\n\n\n\n\n\n\nvcov\n\n\nCharacter string or formula denoting the desired type of variance- covariance correction / standard errors. Options are ‚Äúiid‚Äù (default), ‚Äúhc1‚Äù (heteroskedasticity-consistent), or a one-sided formula like ~cluster_var for cluster-robust standard errors. Note that ‚Äúhc1‚Äù and clustered SEs require a second pass over the data unless strategy = ‚Äúcompress‚Äù to construct the residuals.\n\n\n\n\nstrategy\n\n\nCharacter string indicating the preferred acceleration strategy. The default ‚Äúauto‚Äù will pick an optimal strategy based on internal heuristics. Users can also override with one of the following explicit strategies: ‚Äúcompress‚Äù, ‚Äúdemean‚Äù (alias: ‚Äúwithin‚Äù), ‚Äúmundlak‚Äù, or ‚Äúmoments‚Äù. See the Acceleration Strategies section below for details.\n\n\n\n\ncompress_ratio, compress_nmax\n\n\nNumeric(s). Parameters that help to determine the acceleration strategy under the default ‚Äúauto‚Äù option.\n\n\ncompress_ratio defines the compression ratio threshold, i.e.¬†numeric in the range [0,1] defining the minimum acceptable compressed versus the original data size. Default value of NULL means that the threshold will be automatically determined based on some internal heuristic (e.g., 0.01 for models without fixed effects).\n\n\ncompress_nmax defines the maximum allowable size (in rows) of the compressed dataset that can be serialized into R. Pays heed to the idea that big data serialization can be costly (esp.¬†for remote databases), even if we have achieved good compression on top of the original dataset. Default value is 1e6 (i.e., a million rows).\n\n\nSee the Acceleration Strategies section below for further details.\n\n\n\n\ncluster\n\n\nOptional. Provides an alternative way to specify cluster-robust standard errors (i.e., instead of vcov = ~cluster_var). Either a one-sided formula (e.g., ~firm) or character string giving the variable name. Only single-variable clustering is currently supported.\n\n\n\n\nssc\n\n\nCharacter string controlling the small-sample correction for clustered standard errors. Options are ‚Äúfull‚Äù (default) or ‚Äúnested‚Äù. With ‚Äúfull‚Äù, all parameters (including fixed effect dummies) are counted in K for the CR1 correction. With ‚Äúnested‚Äù, fixed effects that are nested within the cluster variable are excluded from K, matching the default behavior of fixest::feols. Only applies to ‚Äúcompress‚Äù and ‚Äúdemean‚Äù strategies (Mundlak uses explicit group mean regressors, not FE dummies). This distinction only matters for small samples. For large datasets (dbreg‚Äôs target use case), the difference is negligible and hence we default to the simple ‚Äúfull‚Äù option.\n\n\n\n\nsql_only\n\n\nLogical indicating whether only the underlying compression SQL query should be returned (i.e., no computation will be performed). Default is FALSE.\n\n\n\n\ndata_only\n\n\nLogical indicating whether only the compressed dataset should be returned (i.e., no regression is run). Default is FALSE.\n\n\n\n\ndrop_missings\n\n\nLogical indicating whether incomplete cases (i.e., rows where any of the dependent, independent or FE variables are missing) should be dropped. The default is TRUE, according with standard regression software. It is strongly recommended not to change this value unless you are absolutely sure that your data have no missings and you wish to skip some internal checks. (Even then, it probably isn‚Äôt worth it.)\n\n\n\n\nverbose\n\n\nLogical. Print auto strategy and progress messages to the console? Defaults to FALSE. This can be overridden for a single call by supplying verbose = TRUE, or set globally via options(dbreg.verbose = TRUE).\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments. Currently ignored, except to handle superseded arguments for backwards compatibility.\n\n\n\n\n\n\nA list of class \"dbreg\" containing various slots, including a table of coefficients (which the associated print method will display).\n\n\n\ndbreg offers four primary acceleration strategies for estimating regression results from simplified data representations. Below we use the shorthand Y (outcome), X (explanatory variables), and FE (fixed effects) for exposition purposes:\n\n\n‚Äúcompress‚Äù: compresses the data via a GROUP BY operation (using X and the FE as groups), before running weighted least squares on this much smaller dataset:\n\n\\(\\hat{\\beta} = (X_c' W X_c)^{-1} X_c' W Y_c\\)\nwhere \\(W = \\text{diag}(n_g)\\) are the group frequencies. This procedure follows Wang et al.¬†(2021).\n\n\n‚Äúmoments‚Äù: computes sufficient statistics (\\(X'X, X'y\\)) directly via SQL aggregation, returning a single-row result. This solves the standard OLS normal equations \\(\\hat{\\beta} = (X'X)^{-1}X'y\\). Limited to cases without FE.\n\n\n‚Äúdemean‚Äù (alias ‚Äúwithin‚Äù): subtracts group-level means from both Y and X before computing sufficient statistics (per the ‚Äúmoments‚Äù strategy). For example, given unit \\(i\\) and time \\(t\\) FE, we apply double demeaning:\n\n\\(\\ddot{Y}_{it} = \\beta \\ddot{X}_{it} + \\varepsilon_{it}\\)\nwhere \\(\\ddot{X} = X - \\bar{X}_i - \\bar{X}_t + \\bar{X}\\). This (single-pass) within transformation is algebraically equivalent to the fixed effects projection‚Äîi.e., Frisch-Waugh-Lovell partialling out‚Äîin the presence of a single FE. It is also identical for the two-way FE (TWFE) case if your panel is balanced. For unbalanced two-way panels, however, the double demeaning strategy is not algebraically equivalent to the fixed effects projection and therefore does not recover the exact TWFE coefficients. Moreover, note that this ‚Äúdemean‚Äù strategy permits at most two FE.\n\n\n‚Äúmundlak‚Äù: a generalized Mundlak (1978), or correlated random effects (CRE) estimator that regresses Y on X plus group means of X:\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_i + \\varepsilon_{it} \\quad \\text{(one-way)}\\)\n\n\\(Y_{it} = \\alpha + \\beta X_{it} + \\gamma \\bar{X}_{i} + \\delta \\bar{X}_{t} + \\varepsilon_{it} \\quad \\text{(two-way, etc.)}\\)\nUnlike ‚Äúdemean‚Äù, Y is not transformed, so predictions are on the original scale. Supports any number of FE and works correctly for any panel structure (balanced or unbalanced). However, note that CRE is a different model from FE: while coefficients are asymptotically equivalent under certain assumptions, they will generally differ in finite samples.\n\n\nThe relative efficiency of each of these strategies depends on the size and structure of the data, as well the number of unique regressors and FE. For (quote unquote) \"standard\" cases, the ‚Äúcompress‚Äù strategy can yield remarkable performance gains and should justifiably be viewed as a good default. However, the compression approach tends to be less efficient for true panels (repeated cross-sections over time), where N &gt;&gt; T. In such cases, it can be more efficient to use a demeaning strategy that first controls for (e.g.¬†subtracts) group means, before computing sufficient statistics on the aggregated data. The reason for this is that time and unit FE are typically high dimensional, but covariate averages are not; see Arkhangelsky & Imbens (2024).\nHowever, the demeaning approaches invite tradeoffs of their own. For example, the double demeaning transformation of the ‚Äúdemean‚Äù strategy does not obtain exact TWFE results in unbalanced panels, and it is also limited to at most two FE. Conversely, the ‚Äúmundlak‚Äù (CRE) strategy obtains consistent coefficients regardless of panel structure and FE count, but at the \"cost\" of recovering a different estimand. (It is a different model to TWFE, after all.) See Wooldridge (2025) for an extended discussion of these issues.\nUsers should weigh these tradeoffs when choosing their acceleration strategy. Summarising, we can provide a few guiding principles. ‚Äúcompress‚Äù is a good default that guarantees the \"exact\" FE estimates and is usually very efficient (barring data I/O costs and high FE dimensionality). ‚Äúmundlak‚Äù is another efficient alternative provided that the CRE estimand is acceptable (don‚Äôt be alarmed if your coefficients are not identical). Finally, the ‚Äúdemean‚Äù and ‚Äúmoments‚Äù strategies are great for particular use cases (i.e., balanced panels and cases without FE, respectively).\nIf this all sounds like too much to think about, don‚Äôt fret. The good news is that dbreg can do a lot (all?) of the deciding for you. Specifically, it will invoke an ‚Äúauto‚Äù heuristic behind the scenes if a user does not provide an explicit acceleration strategy. Working through the heuristic logic does impose some additional overhead, but this should be negligible in most cases (certainly compared to the overall time savings). The ‚Äúauto‚Äù heuristic is as follows:\n\n\nIF no FE AND (any continuous regressor OR poor compression ratio OR too big compressed data) THEN ‚Äúmoments‚Äù.\n\n\nELSE IF 1 FE AND (poor compression ratio OR too big compressed data) THEN ‚Äúdemean‚Äù.\n\n\nELSE IF 2 FE AND (poor compression ratio OR too big compressed data):\n\n\nIF balanced panel THEN ‚Äúdemean‚Äù.\n\n\nELSE error (exact TWFE infeasible; user must explicitly choose ‚Äúcompress‚Äù or ‚Äúmundlak‚Äù).\n\n\n\n\nELSE THEN ‚Äúcompress‚Äù.\n\n\nTip: set dbreg(‚Ä¶, verbose = TRUE) to print information about the auto strategy decision criteria.\n\n\n\nArkhangelsky, D. & Imbens, G. (2024) Fixed Effects and the Generalized Mundlak Estimator. The Review of Economic Studies, 91(5), pp.¬†2545‚Äì2571. Available: https://doi.org/10.1093/restud/rdad089\nMundlak, Y. (1978) On the Pooling of Time Series and Cross Section Data. Econometrica, 46(1), pp.¬†69‚Äì85. Available: https://doi.org/10.2307/1913646\nWong, J., Forsell, E., Lewis, R., Mao, T., & Wardrop, M. (2021). You Only Compress Once: Optimal Data Compression for Estimating Linear Models. arXiv preprint arXiv:2102.11297. Available: https://doi.org/10.48550/arXiv.2102.11297\nWooldridge, J.M. (2025) Two-way fixed effects, the two-way mundlak regression, and difference-in-differences estimators. Empirical Economics, 69, pp.¬†2545‚Äì2587. Available: https://doi.org/10.1007/s00181-025-02807-z\n\n\n\ndbConnect for creating database connections, duckdb for DuckDB-specific connections\n\n\n\n\nlibrary(\"dbreg\")\n\n## In-memory data ----\n\n# We can pass in-memory R data frames to an ephemeral DuckDB connection via\n# the `data` argument. This is convenient for small(er) datasets and demos.\n\n# Default \"compress\" strategy reduces the data to 4 rows before running OLS\ndbreg(weight ~ Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 4 (compressed) \nStandard Errors: IID \n            Estimate Std. Error  t value   Pr(&gt;|t|)    \n(Intercept) 102.6455    4.67395 21.96116  &lt; 2.2e-16 ***\nDiet2        19.9712    7.86744  2.53846 1.1397e-02 *  \nDiet3        40.3045    7.86744  5.12296 4.1139e-07 ***\nDiet4        32.6173    7.91046  4.12331 4.2864e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 69.1                Adj. R2: 0.048530\n\n# Compare with lm\nsummary(lm(weight ~ Diet, data = ChickWeight))$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 102.64545   4.673954 21.961161 4.712762e-78\nDiet2        19.97121   7.867437  2.538465 1.139708e-02\nDiet3        40.30455   7.867437  5.122958 4.113938e-07\nDiet4        32.61726   7.910461  4.123307 4.286352e-05\n\n# Add \"fixed effects\" after a `|` \ndbreg(weight ~ Time | Diet, data = ChickWeight)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# \"robust\" SEs can also be computed using a sufficient statistics approach\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = \"hc1\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Heteroskedasticity-robust \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.261676 33.4402 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, vcov = ~Chick)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: Clustered (50 clusters) \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.527463 16.5898 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Different acceleration strategies + specifications\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"demean\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.729032\n\ndbreg(weight ~ Time | Diet, data = ChickWeight, strategy = \"mundlak\")\n\nOne-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049    0.22765 38.4383 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729827\n\ndbreg(weight ~ Time | Diet + Chick, data = ChickWeight, strategy = \"mundlak\") # two-way Mundlak\n\nTwo-way Mundlak OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.71519   0.229744 37.9344 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 36.8                Adj. R2: 0.729953\n\ndbreg(weight ~ Time, data = ChickWeight, strategy = \"moments\") # no FEs\n\nMoments-based OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \n(Intercept) 27.46743   3.036464  9.04586 &lt; 2.2e-16 ***\nTime         8.80304   0.239700 36.72524 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 38.8                Adj. R2: 0.700220\n\n# etc.\n\n# Interactions: does the effect of Time vary by Diet?\n# (Diet main effects are collinear with Chick FE, so these drop out)\ndbreg(weight ~ Time * Diet | Chick, data = ChickWeight)\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 578 \nStandard Errors: IID \n           Estimate Std. Error  t value   Pr(&gt;|t|)    \nTime        6.69062   0.259809 25.75212  &lt; 2.2e-16 ***\nTime:Diet2  1.91851   0.429357  4.46834 9.6656e-06 ***\nTime:Diet3  4.73225   0.429357 11.02170  &lt; 2.2e-16 ***\nTime:Diet4  2.96531   0.435005  6.81674 2.5702e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 24.2                Adj. R2: 0.844227\n3 variables were removed because of collinearity (Diet2, Diet3, Diet4)\n\n#\n## DBI connection ----\n\n# For persistent databases or more control, use the `conn` + `table` args.\n# Again, we use DuckDB below but any other DBI-supported backend should work\n# too (e.g., odbc, bigrquery, noctua (AWS Athena),  etc.) See:\n# https://r-dbi.org/backends/\n\nlibrary(DBI)\ncon = dbConnect(duckdb::duckdb())\ndbWriteTable(con, \"cw\", as.data.frame(ChickWeight))\n\ndbreg(weight ~ Time | Diet, conn = con, table = \"cw\")\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Tip: Rather than creating or writing (temp) tables, use CREATE VIEW to\n# define subsets or computed columns without materializing data. This is more\n# efficient and especially useful for filtering or adding variables.\ndbExecute(\n  con,\n  \"\n  CREATE VIEW cw1 AS\n  SELECT *\n  FROM cw\n  WHERE Diet = 1\n  \"\n)\n\n[1] 0\n\ndbreg(weight ~ Time | Chick, conn = con, table = \"cw1\")\n\nDemeaned OLS estimation, Dep. Var.: weight \nObservations.: 220 \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  6.69062   0.246299 27.1646 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 22.9                Adj. R2: 0.766255\n\n#\n## Path to file ----\n#\n# For file-based data (e.g., parquet), use the path argument.\n\ntmp = tempfile(fileext = \".parquet\")\ndbExecute(con, sprintf(\"COPY cw TO '%s' (FORMAT PARQUET)\", tmp))\n\n[1] 578\n\ndbreg(weight ~ Time | Diet, path = tmp)\n\nCompressed OLS estimation, Dep. Var.: weight \nObservations.: 578 (original) | 48 (compressed) \nStandard Errors: IID \n     Estimate Std. Error t value  Pr(&gt;|t|)    \nTime  8.75049   0.221805 39.4512 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 35.8                Adj. R2: 0.743522\n\n# Cleanup\ndbDisconnect(con)\nunlink(tmp)\n\n#\n## Big dataset ----\n\n# For a more compelling and appropriate dbreg use-case, i.e. regression on a\n# big (~180 million row) dataset of Hive-partioned parquet files, see the\n# package website:\n# https://grantmcdermott.com/dbreg/",
    "crumbs": [
      "Reference",
      "Main functions",
      "dbreg"
    ]
  },
  {
    "objectID": "man/tidiers.html",
    "href": "man/tidiers.html",
    "title": "dbreg",
    "section": "",
    "text": "Provides broom::tidy and broom::glance methods for \"dbreg\" objects.\n\n\n\n## S3 method for class 'dbreg'\ntidy(x, conf.int = FALSE, conf.level = 0.95, fe = FALSE, ...)\n\n## S3 method for class 'dbreg'\nglance(x, ...)\n\n\n\n\n\n\n\nx\n\n\na model of class ‚Äòdbreg‚Äô produced by the dbreg function.\n\n\n\n\nconf.int\n\n\nLogical indicating whether to include confidence intervals. Default is ‚ÄòFALSE‚Äô.\n\n\n\n\nconf.level\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be tidied too? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments to tidying method. Currently unused except to handle superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ntidy(mod, conf.int = TRUE)\n\n  term  estimate std.error statistic     p.values  conf.low  conf.high\n1 Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n\ntidy(mod, conf.int = TRUE, fe = TRUE)\n\n         term  estimate std.error statistic     p.values  conf.low  conf.high\n1 (Intercept) 74.188474 2.0544007 36.111978 5.675829e-75 70.128499 78.2484490\n2        Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n3      Month6 12.543643 1.5942530  7.868038 7.184769e-13  9.393027 15.6942587\n4      Month7 16.362079 1.6183409 10.110404 1.431916e-18 13.163860 19.5602984\n5      Month8 16.316286 1.6239233 10.047449 2.091354e-18 13.107035 19.5255376\n6      Month9 10.279216 1.5959361  6.440869 1.590257e-09  7.125274 13.4331579\n\nglance(mod)\n\n  r.squared adj.r.squared     rmse nobs df.residual\n1 0.5884105     0.5744109 6.052589  153         147",
    "crumbs": [
      "Reference",
      "Other methods",
      "glance.dbreg"
    ]
  },
  {
    "objectID": "man/tidiers.html#tidiers-for-dbreg-objects",
    "href": "man/tidiers.html#tidiers-for-dbreg-objects",
    "title": "dbreg",
    "section": "",
    "text": "Provides broom::tidy and broom::glance methods for \"dbreg\" objects.\n\n\n\n## S3 method for class 'dbreg'\ntidy(x, conf.int = FALSE, conf.level = 0.95, fe = FALSE, ...)\n\n## S3 method for class 'dbreg'\nglance(x, ...)\n\n\n\n\n\n\n\nx\n\n\na model of class ‚Äòdbreg‚Äô produced by the dbreg function.\n\n\n\n\nconf.int\n\n\nLogical indicating whether to include confidence intervals. Default is ‚ÄòFALSE‚Äô.\n\n\n\n\nconf.level\n\n\nConfidence level for intervals. Default is 0.95.\n\n\n\n\nfe\n\n\nShould the fixed effects be tidied too? Default is ‚ÄòFALSE‚Äô.\n\n\n\n\n‚Ä¶\n\n\nAdditional arguments to tidying method. Currently unused except to handle superseded arguments.\n\n\n\n\n\n\n\nlibrary(\"dbreg\")\n\nmod = dbreg(Temp ~ Wind | Month, data = airquality)\ntidy(mod, conf.int = TRUE)\n\n  term  estimate std.error statistic     p.values  conf.low  conf.high\n1 Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n\ntidy(mod, conf.int = TRUE, fe = TRUE)\n\n         term  estimate std.error statistic     p.values  conf.low  conf.high\n1 (Intercept) 74.188474 2.0544007 36.111978 5.675829e-75 70.128499 78.2484490\n2        Wind -0.743388 0.1487908 -4.996197 1.638368e-06 -1.037433 -0.4493427\n3      Month6 12.543643 1.5942530  7.868038 7.184769e-13  9.393027 15.6942587\n4      Month7 16.362079 1.6183409 10.110404 1.431916e-18 13.163860 19.5602984\n5      Month8 16.316286 1.6239233 10.047449 2.091354e-18 13.107035 19.5255376\n6      Month9 10.279216 1.5959361  6.440869 1.590257e-09  7.125274 13.4331579\n\nglance(mod)\n\n  r.squared adj.r.squared     rmse nobs df.residual\n1 0.5884105     0.5744109 6.052589  153         147",
    "crumbs": [
      "Reference",
      "Other methods",
      "glance.dbreg"
    ]
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "MIT License",
    "section": "",
    "text": "MIT License\nCopyright (c) 2024 dbreg authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the ‚ÄúSoftware‚Äù), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED ‚ÄúAS IS‚Äù, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "dbreg",
    "section": "",
    "text": "Fast regressions on database backends.\n\n\ndbreg is an R package that leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics. Our default DuckDB backend provides a powerful, embedded analytics engine to get users up and running with minimal effort. Users can also specify alternative database backends, depending on their computing needs and setup.\nThe dbreg R package is inspired by, and has similar aims to, the duckreg Python package. This implementation offers some idiomatic, R-focused features like a formula interface and ‚Äúpretty‚Äù print methods. But our long-term goal is that these two packages should be aligned in terms of core feature parity.\n\n\n\ndbreg can be installed from R-universe.\ninstall.packages(\n   \"dbreg\",\n   repos = c(\"https://grantmcdermott.r-universe.dev\", getOption(\"repos\"))\n)\n\n\n\n\n\nTo get ourselves situated, we‚Äôll first demonstrate by using an in-memory R dataset.\nlibrary(dbreg)\nlibrary(fixest) # for data and comparison\n\ndata(\"trade\", package = \"fixest\")\n\ndbreg(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; Compressed OLS estimation, Dep. Var.: Euros \n#&gt; Observations.: 38,325 (original) | 210 (compressed) \n#&gt; Standard Errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3         Adj. R2: 0.215289\nBehind the scenes, dbreg has compressed the original dataset down from nearly 40,000 observations to only 210, before running the final (weighted) regression on this much smaller data object. This compression procedure trick follows Wang _et. al.¬†(2021) and effectively allows us to compute on a much lighter object, saving time and memory. We can confirm that it still gives the same result as running fixest::feols on the full dataset:\nfeols(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; OLS estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325\n#&gt; Fixed-effects: Destination: 15,  Origin: 15\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3     Adj. R2: 0.215289\n#&gt;                       Within R2: 0.025914\n\n\n\nFor a more appropriate dbreg use-case, let‚Äôs run a regression on some NYC taxi data. (Download instructions here.) The dataset that we‚Äôre working with here is about 180 million rows deep and takes up 8.5 GB on disk.1 dbreg offers two basic ways to analyse and interact with data of this size.\n\n\nUse the path argument to read the data directly from disk and perform the compression computation in an ephemeral DuckDB connection. This requires that the data are small enough to fit into RAM‚Ä¶ but please note that ‚Äúsmall enough‚Äù is a relative concept. Thanks to DuckDB‚Äôs incredible efficiency, your RAM should be able to handle very large datasets that would otherwise crash your R session, and require only a fraction of the computation time. Note that we also invoke the (optional) verbose  = TRUE argument to print additional information about the estimation strategy.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\", ## path to hive-partitioned dataset\n   vcov = \"hc1\",\n   verbose = TRUE ## optional (print info about the estimation strategy)\n)\n#&gt; [dbreg] Auto strategy:\n#&gt;         - data has 178,544,324 rows with 2 FE (24 unique groups)\n#&gt;         - compression ratio (0.00) satisfies threshold (0.6)\n#&gt;         - decision: compress\n#&gt; [dbreg] Executing compress strategy SQL\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Heteroskedasticity-robust \n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000068 1564.742 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.000106 -273.866 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nNote the size of the original dataset, which is nearly 180 million rows, versus the compressed dataset, which is down to only 70k. On my laptop (M4 MacBook Pro) this regression completes in under 3 seconds‚Ä¶ and that includes the time it took to determine an optimal estimation strategy, as well as read the data from disk!2\nIn case you were wondering, obtaining clustered standard errors is just as easy; simply pass the relevant cluster variable as a formula to the vcov argument. Since we know that the optimal acceleration strategy is \"compress\", we‚Äôll also go ahead a specify this explicitly to skip the auto strategy overhead.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path     = \"read_parquet('nyc-taxi/**/*.parquet')\",\n   vcov     = ~month,    # clustered SEs\n   strategy = \"compress\" # skip auto strategy overhead\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters) \n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\n\n\n\nWhile querying on-the-fly with our default DuckDB backend is both convenient and extremely performant, you can also run regressions against existing tables in a persistent database connection. This could be DuckDB, but it could also be any other supported backend. All you need to do is specify the appropriate conn and table arguments.\n# load the DBI package to connect to a persistent database\nlibrary(DBI)\n\n# create connection to persistent DuckDB database (could be any supported backend)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"nyc.db\")\n\n# create a 'taxi' table in our new nyc.db database from our parquet dataset\ndbExecute(\n   con,\n   \"\n   CREATE TABLE taxi AS\n      FROM read_parquet('nyc-taxi/**/*.parquet')\n      SELECT tip_amount, fare_amount, passenger_count, month, vendor_name\n   \"\n)\n#&gt; [1] 178544324\n\n# now run our regression against this conn+table combo\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   conn = con,     # database connection,\n   table = \"taxi\", # table name\n   vcov = ~month,\n   strategy = \"compress\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters) \n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nResult: we get the same coefficient and standard error estimates as earlier.\n\n[!TIP] If you don‚Äôt want to create a persistent database (and materialize data), a nice alternative is CREATE VIEW. This lets you define subsets or computed columns on-the-fly. For example, to regress on Q1 2012 data with a day-of-week fixed effect:\ndbExecute(con, \"\n   CREATE VIEW nyc_subset AS\n   SELECT\n      tip_amount, trip_distance, passenger_count,\n      vendor_name, month,\n      dayofweek(dropoff_datetime) AS dofw\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   WHERE year = 2012 AND month &lt;= 3\n\")\n\ndbreg(\n   tip_amount ~ trip_distance + passenger_count | month + dofw + vendor_name,\n   conn = con,\n   table = \"nyc_subset\",\n   vcov = ~dofw\n)\n\nWe‚Äôll close by doing some (optional) clean up.\ndbRemoveTable(con, \"taxi\")\ndbDisconnect(con)\nunlink(\"nyc.db\") # remove from disk\n\n\n\n\n\nAll of the examples in this README have made use of the \"compress\" strategy. But the compression trick is not the only game in town and dbreg supports several other acceleration strategies: \"moments\", \"demean\", and \"mundlak\". Depending on your data and regression requirements, one of these other strategies may better suit your problem. The good news is that (the default) strategy = \"auto\" option uses some intelligent heuristics to determine which strategy is (probably) optimal for each case. You can set the verbose = TRUE argument to get real-time feedback about the decision criteria being used. The Acceleration Strategies section of the ?dbreg helpfile contains a lot detail about the different options and tradeoffs involved, so please do consult the documentation.\n\n\n\nThe companion dbbinsreg() function provides a quick way to visualize relationships in large datasets. Here we plot average tips by month:\ndbbinsreg(\n   tip_amount ~ month, nbins = 12,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\"\n)\n\n#&gt; Binscatter Plot\n#&gt; Formula: tip_amount ~ month \n#&gt; points = c(0,0) | line = NULL | nbins = 11 (quantile-spaced)\n#&gt; Observations: 178,544,324 (original) | 11 (compressed)\nThe plot reveals a striking jump in tips starting in September. We can investigate whether this jump differed by taxi vendor using an interaction model. First, create a view with a post-September indicator:\n# re-establish a (new) duckdb connection\ncon = dbConnect(duckdb::duckdb(), shutdown = TRUE)\n\n# create a temporary view with a post-September dummy\ndbExecute(\n   con,\n   \"\n   CREATE VIEW nyc_post AS\n   SELECT \n      tip_amount, vendor_name, month,\n      CAST(month &gt;= 9 AS INTEGER) AS post\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   \"\n)\n#&gt; [1] 0\nThen run a DiD-style regression:\ndbreg(\n   tip_amount ~ post * vendor_name | month,\n   conn = con,\n   table = \"nyc_post\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 24 (compressed) \n#&gt; Standard Errors: IID \n#&gt;                      Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; post                 0.226457   0.000768 295.0536 &lt; 2.2e-16 ***\n#&gt; vendor_nameVTS      -0.009126   0.000349 -26.1327 &lt; 2.2e-16 ***\n#&gt; post:vendor_nameVTS  0.006936   0.000615  11.2757 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.9                 Adj. R2: 0.002497\n#&gt; 1 variable was removed because of collinearity (month12)\nThe results confirm a $0.23 jump in tips post-September, with a small additional increase for VTS taxis.\n\n\n\ndbreg is a maturing package and there are a number of features that we still plan to add before submitting it to CRAN. (See our TO-DO list.) At the same time, the core dbreg() routine has been tested pretty thoroughly and should work in standard cases. Please help us by kicking the tyres and creating GitHub issues for both bug reports and feature requests."
  },
  {
    "objectID": "index.html#what",
    "href": "index.html#what",
    "title": "dbreg",
    "section": "",
    "text": "dbreg is an R package that leverages the power of databases to run regressions on very large datasets, which may not fit into R‚Äôs memory. Various acceleration strategies allow for highly efficient computation, while robust standard errors are computed from sufficient statistics. Our default DuckDB backend provides a powerful, embedded analytics engine to get users up and running with minimal effort. Users can also specify alternative database backends, depending on their computing needs and setup.\nThe dbreg R package is inspired by, and has similar aims to, the duckreg Python package. This implementation offers some idiomatic, R-focused features like a formula interface and ‚Äúpretty‚Äù print methods. But our long-term goal is that these two packages should be aligned in terms of core feature parity."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "dbreg",
    "section": "",
    "text": "dbreg can be installed from R-universe.\ninstall.packages(\n   \"dbreg\",\n   repos = c(\"https://grantmcdermott.r-universe.dev\", getOption(\"repos\"))\n)"
  },
  {
    "objectID": "index.html#quickstart",
    "href": "index.html#quickstart",
    "title": "dbreg",
    "section": "",
    "text": "To get ourselves situated, we‚Äôll first demonstrate by using an in-memory R dataset.\nlibrary(dbreg)\nlibrary(fixest) # for data and comparison\n\ndata(\"trade\", package = \"fixest\")\n\ndbreg(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; Compressed OLS estimation, Dep. Var.: Euros \n#&gt; Observations.: 38,325 (original) | 210 (compressed) \n#&gt; Standard Errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3         Adj. R2: 0.215289\nBehind the scenes, dbreg has compressed the original dataset down from nearly 40,000 observations to only 210, before running the final (weighted) regression on this much smaller data object. This compression procedure trick follows Wang _et. al.¬†(2021) and effectively allows us to compute on a much lighter object, saving time and memory. We can confirm that it still gives the same result as running fixest::feols on the full dataset:\nfeols(Euros ~ dist_km | Destination + Origin, data = trade, vcov = 'hc1')\n#&gt; OLS estimation, Dep. Var.: Euros\n#&gt; Observations: 38,325\n#&gt; Fixed-effects: Destination: 15,  Origin: 15\n#&gt; Standard-errors: Heteroskedasticity-robust \n#&gt;         Estimate Std. Error t value  Pr(&gt;|t|)    \n#&gt; dist_km -45709.8    1195.84 -38.224 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 124,221,786.3     Adj. R2: 0.215289\n#&gt;                       Within R2: 0.025914\n\n\n\nFor a more appropriate dbreg use-case, let‚Äôs run a regression on some NYC taxi data. (Download instructions here.) The dataset that we‚Äôre working with here is about 180 million rows deep and takes up 8.5 GB on disk.1 dbreg offers two basic ways to analyse and interact with data of this size.\n\n\nUse the path argument to read the data directly from disk and perform the compression computation in an ephemeral DuckDB connection. This requires that the data are small enough to fit into RAM‚Ä¶ but please note that ‚Äúsmall enough‚Äù is a relative concept. Thanks to DuckDB‚Äôs incredible efficiency, your RAM should be able to handle very large datasets that would otherwise crash your R session, and require only a fraction of the computation time. Note that we also invoke the (optional) verbose  = TRUE argument to print additional information about the estimation strategy.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\", ## path to hive-partitioned dataset\n   vcov = \"hc1\",\n   verbose = TRUE ## optional (print info about the estimation strategy)\n)\n#&gt; [dbreg] Auto strategy:\n#&gt;         - data has 178,544,324 rows with 2 FE (24 unique groups)\n#&gt;         - compression ratio (0.00) satisfies threshold (0.6)\n#&gt;         - decision: compress\n#&gt; [dbreg] Executing compress strategy SQL\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Heteroskedasticity-robust \n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000068 1564.742 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.000106 -273.866 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nNote the size of the original dataset, which is nearly 180 million rows, versus the compressed dataset, which is down to only 70k. On my laptop (M4 MacBook Pro) this regression completes in under 3 seconds‚Ä¶ and that includes the time it took to determine an optimal estimation strategy, as well as read the data from disk!2\nIn case you were wondering, obtaining clustered standard errors is just as easy; simply pass the relevant cluster variable as a formula to the vcov argument. Since we know that the optimal acceleration strategy is \"compress\", we‚Äôll also go ahead a specify this explicitly to skip the auto strategy overhead.\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   path     = \"read_parquet('nyc-taxi/**/*.parquet')\",\n   vcov     = ~month,    # clustered SEs\n   strategy = \"compress\" # skip auto strategy overhead\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters) \n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\n\n\n\nWhile querying on-the-fly with our default DuckDB backend is both convenient and extremely performant, you can also run regressions against existing tables in a persistent database connection. This could be DuckDB, but it could also be any other supported backend. All you need to do is specify the appropriate conn and table arguments.\n# load the DBI package to connect to a persistent database\nlibrary(DBI)\n\n# create connection to persistent DuckDB database (could be any supported backend)\ncon = dbConnect(duckdb::duckdb(), dbdir = \"nyc.db\")\n\n# create a 'taxi' table in our new nyc.db database from our parquet dataset\ndbExecute(\n   con,\n   \"\n   CREATE TABLE taxi AS\n      FROM read_parquet('nyc-taxi/**/*.parquet')\n      SELECT tip_amount, fare_amount, passenger_count, month, vendor_name\n   \"\n)\n#&gt; [1] 178544324\n\n# now run our regression against this conn+table combo\ndbreg(\n   tip_amount ~ fare_amount + passenger_count | month + vendor_name,\n   conn = con,     # database connection,\n   table = \"taxi\", # table name\n   vcov = ~month,\n   strategy = \"compress\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 70,782 (compressed) \n#&gt; Standard Errors: Clustered (12 clusters) \n#&gt;                  Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; fare_amount      0.106744   0.000657 162.4934 &lt; 2.2e-16 ***\n#&gt; passenger_count -0.029086   0.001030 -28.2278 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.7                 Adj. R2: 0.243549\nResult: we get the same coefficient and standard error estimates as earlier.\n\n[!TIP] If you don‚Äôt want to create a persistent database (and materialize data), a nice alternative is CREATE VIEW. This lets you define subsets or computed columns on-the-fly. For example, to regress on Q1 2012 data with a day-of-week fixed effect:\ndbExecute(con, \"\n   CREATE VIEW nyc_subset AS\n   SELECT\n      tip_amount, trip_distance, passenger_count,\n      vendor_name, month,\n      dayofweek(dropoff_datetime) AS dofw\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   WHERE year = 2012 AND month &lt;= 3\n\")\n\ndbreg(\n   tip_amount ~ trip_distance + passenger_count | month + dofw + vendor_name,\n   conn = con,\n   table = \"nyc_subset\",\n   vcov = ~dofw\n)\n\nWe‚Äôll close by doing some (optional) clean up.\ndbRemoveTable(con, \"taxi\")\ndbDisconnect(con)\nunlink(\"nyc.db\") # remove from disk"
  },
  {
    "objectID": "index.html#acceleration-strategies",
    "href": "index.html#acceleration-strategies",
    "title": "dbreg",
    "section": "",
    "text": "All of the examples in this README have made use of the \"compress\" strategy. But the compression trick is not the only game in town and dbreg supports several other acceleration strategies: \"moments\", \"demean\", and \"mundlak\". Depending on your data and regression requirements, one of these other strategies may better suit your problem. The good news is that (the default) strategy = \"auto\" option uses some intelligent heuristics to determine which strategy is (probably) optimal for each case. You can set the verbose = TRUE argument to get real-time feedback about the decision criteria being used. The Acceleration Strategies section of the ?dbreg helpfile contains a lot detail about the different options and tradeoffs involved, so please do consult the documentation."
  },
  {
    "objectID": "index.html#bonus-binscatter-interactions",
    "href": "index.html#bonus-binscatter-interactions",
    "title": "dbreg",
    "section": "",
    "text": "The companion dbbinsreg() function provides a quick way to visualize relationships in large datasets. Here we plot average tips by month:\ndbbinsreg(\n   tip_amount ~ month, nbins = 12,\n   path = \"read_parquet('nyc-taxi/**/*.parquet')\"\n)\n\n#&gt; Binscatter Plot\n#&gt; Formula: tip_amount ~ month \n#&gt; points = c(0,0) | line = NULL | nbins = 11 (quantile-spaced)\n#&gt; Observations: 178,544,324 (original) | 11 (compressed)\nThe plot reveals a striking jump in tips starting in September. We can investigate whether this jump differed by taxi vendor using an interaction model. First, create a view with a post-September indicator:\n# re-establish a (new) duckdb connection\ncon = dbConnect(duckdb::duckdb(), shutdown = TRUE)\n\n# create a temporary view with a post-September dummy\ndbExecute(\n   con,\n   \"\n   CREATE VIEW nyc_post AS\n   SELECT \n      tip_amount, vendor_name, month,\n      CAST(month &gt;= 9 AS INTEGER) AS post\n   FROM read_parquet('nyc-taxi/**/*.parquet')\n   \"\n)\n#&gt; [1] 0\nThen run a DiD-style regression:\ndbreg(\n   tip_amount ~ post * vendor_name | month,\n   conn = con,\n   table = \"nyc_post\"\n)\n#&gt; Compressed OLS estimation, Dep. Var.: tip_amount \n#&gt; Observations.: 178,544,324 (original) | 24 (compressed) \n#&gt; Standard Errors: IID \n#&gt;                      Estimate Std. Error  t value  Pr(&gt;|t|)    \n#&gt; post                 0.226457   0.000768 295.0536 &lt; 2.2e-16 ***\n#&gt; vendor_nameVTS      -0.009126   0.000349 -26.1327 &lt; 2.2e-16 ***\n#&gt; post:vendor_nameVTS  0.006936   0.000615  11.2757 &lt; 2.2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; RMSE: 1.9                 Adj. R2: 0.002497\n#&gt; 1 variable was removed because of collinearity (month12)\nThe results confirm a $0.23 jump in tips post-September, with a small additional increase for VTS taxis."
  },
  {
    "objectID": "index.html#limitations",
    "href": "index.html#limitations",
    "title": "dbreg",
    "section": "",
    "text": "dbreg is a maturing package and there are a number of features that we still plan to add before submitting it to CRAN. (See our TO-DO list.) At the same time, the core dbreg() routine has been tested pretty thoroughly and should work in standard cases. Please help us by kicking the tyres and creating GitHub issues for both bug reports and feature requests."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "dbreg",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo be clear, this dataset would occupy significantly more RAM than 8.5 GB if we loaded it into R‚Äôs memory, due to data serialization and the switch to richer representation formats (e.g., ordered factors require more memory). So there‚Äôs a good chance that just trying to load this raw dataset into R would cause your whole system to crash‚Ä¶ never mind doing any statistical analysis on it.‚Ü©Ô∏é\nIf we provided an explicit dbreg(..., strategy = \"compress\") argument (thus skipping the automatic strategy determination), then the total computation time drops to less than 1 second‚Ä¶‚Ü©Ô∏é"
  }
]